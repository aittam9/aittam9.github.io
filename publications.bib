
@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {1877--1901},
}

@article{noauthor_notitle_nodate,
}

@article{hebart_things_2019,
	title = {{THINGS}: {A} database of 1,854 object concepts and more than 26,000 naturalistic object images},
	volume = {14},
	url = {https://doi.org/10.1371/journal.pone.0223792},
	doi = {10.1371/journal.pone.0223792},
	abstract = {In recent years, the use of a large number of object concepts and naturalistic object images has been growing strongly in cognitive neuroscience research. Classical databases of object concepts are based mostly on a manually curated set of concepts. Further, databases of naturalistic object images typically consist of single images of objects cropped from their background, or a large number of naturalistic images of varying quality, requiring elaborate manual image curation. Here we provide a set of 1,854 diverse object concepts sampled systematically from concrete picturable and nameable nouns in the American English language. Using these object concepts, we conducted a large-scale web image search to compile a database of 26,107 high-quality naturalistic images of those objects, with 12 or more object images per concept and all images cropped to square size. Using crowdsourcing, we provide higher-level category membership for the 27 most common categories and validate them by relating them to representations in a semantic embedding derived from large text corpora. Finally, by feeding images through a deep convolutional neural network, we demonstrate that they exhibit high selectivity for different object concepts, while at the same time preserving variability of different object images within each concept. Together, the THINGS database provides a rich resource of object concepts and object images and offers a tool for both systematic and large-scale naturalistic research in the fields of psychology, neuroscience, and computer science.},
	number = {10},
	journal = {PLOS ONE},
	author = {Hebart, Martin N. and Dickter, Adam H. and Kidder, Alexis and Kwok, Wan Y. and Corriveau, Anna and Van Wicklin, Caitlin and Baker, Chris I.},
	month = oct,
	year = {2019},
	note = {Publisher: Public Library of Science},
	pages = {1--24},
}

@article{noauthor_notitle_nodate-1,
	file = {Causality_ models, reasoning, and inference[Judea_Pearl].pdf:C\:\\Users\\user\\Zotero\\storage\\73MPK9DB\\Causality_ models, reasoning, and inference[Judea_Pearl].pdf:application/pdf},
}

@misc{musker_semantic_2024,
	title = {Semantic {Structure}-{Mapping} in {LLM} and {Human} {Analogical} {Reasoning}},
	url = {http://arxiv.org/abs/2406.13803},
	doi = {10.48550/arXiv.2406.13803},
	abstract = {Analogical reasoning is considered core to human learning and cognition. Recent studies have compared the analogical reasoning abilities of human subjects and Large Language Models (LLMs) on abstract symbol manipulation tasks, such as letter string analogies. However, these studies largely neglect analogical reasoning over semantically meaningful symbols, such as natural language words. This ability to draw analogies that link language to non-linguistic domains, which we term semantic structure-mapping, is thought to play a crucial role in language acquisition and broader cognitive development. We test human subjects and LLMs on analogical reasoning tasks that require the transfer of semantic structure and content from one domain to another. Advanced LLMs match human performance across many task variations. However, humans and LLMs respond differently to certain task variations and semantic distractors. Overall, our data suggest that LLMs are approaching human-level performance on these important cognitive tasks, but are not yet entirely human like.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Musker, Sam and Duchnowski, Alex and Millière, Raphaël and Pavlick, Ellie},
	month = jun,
	year = {2024},
	note = {arXiv:2406.13803 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\PCQ38D9B\\Musker et al. - 2024 - Semantic Structure-Mapping in LLM and Human Analog.pdf:application/pdf},
}

@article{musker_testing_2024,
	title = {Testing {Causal} {Models} of {Word} {Meaning} in {LLMs}},
	volume = {46},
	url = {https://escholarship.org/uc/item/0wc4315w},
	abstract = {Large Language Models (LLMs) have driven extraordinary improvements in NLP. However, it is unclear how such models represent lexical concepts-i.e., the meanings of the words they use. We evaluate the lexical representations of GPT-4, GPT-3, and Falcon-40B through the lens of HIPE theory, a concept representation theory focused on words describing artifacts (such as ‚Äúmop‚Äù, ‚Äúpencil‚Äù, and ‚Äúwhistle‚Äù). The theory posits a causal graph relating the meanings of such words to the form, use, and history of the referred objects. We test LLMs with the stimuli used by Chaigneau et al. (2004) on human subjects, and consider a variety of prompt designs. Our experiments concern judgements about causal outcomes, object function, and object naming. We do not find clear evidence that GPT-3 or Falcon-40B encode HIPE's causal structure, but find evidence that GPT-4 does. The results contribute to a growing body of research characterizing the representational capacity of LLMs.},
	language = {en},
	number = {0},
	urldate = {2024-08-01},
	journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
	author = {Musker, Sam and Pavlick, Ellie},
	year = {2024},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\INBU2LQB\\Musker e Pavlick - 2024 - Testing Causal Models of Word Meaning in LLMs.pdf:application/pdf},
}

@misc{merullo_talking_2024,
	title = {Talking {Heads}: {Understanding} {Inter}-layer {Communication} in {Transformer} {Language} {Models}},
	shorttitle = {Talking {Heads}},
	url = {http://arxiv.org/abs/2406.09519},
	doi = {10.48550/arXiv.2406.09519},
	abstract = {Although it is known that transformer language models (LMs) pass features from early layers to later layers, it is not well understood how this information is represented and routed by the model. By analyzing particular mechanism LMs use to accomplish this, we find that it is also used to recall items from a list, and show that this mechanism can explain an otherwise arbitrary-seeming sensitivity of the model to the order of items in the prompt. Specifically, we find that models write into low-rank subspaces of the residual stream to represent features which are then read out by specific later layers, forming low-rank communication channels between layers. By decomposing attention head weight matrices with the Singular Value Decomposition (SVD), we find that previously described interactions between heads separated by one or more layers can be predicted via analysis of their weight matrices. We show that it is possible to manipulate the internal model representations as well as edit model weights based on the mechanism we discover in order to significantly improve performance on our synthetic Laundry List task, which requires recall from a list, often improving task accuracy by over 20\%. Our analysis reveals a surprisingly intricate interpretable structure learned from language model pretraining, and helps us understand why sophisticated LMs sometimes fail in simple domains, facilitating future analysis of more complex behaviors.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09519 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\CD2X22UD\\Merullo et al. - 2024 - Talking Heads Understanding Inter-layer Communica.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\JXYWJKAC\\2406.html:text/html},
}

@article{machamer_thinking_2000,
	title = {Thinking about {Mechanisms}},
	volume = {67},
	issn = {0031-8248},
	url = {https://www.jstor.org/stable/188611},
	abstract = {The concept of mechanism is analyzed in terms of entities and activities, organized such that they are productive of regular changes. Examples show how mechanisms work in neurobiology and molecular biology. Thinking in terms of mechanisms provides a new framework for addressing many traditional philosophical issues: causality, laws, explanation, reduction, and scientific change.},
	number = {1},
	urldate = {2024-08-01},
	journal = {Philosophy of Science},
	author = {Machamer, Peter and Darden, Lindley and Craver, Carl F.},
	year = {2000},
	note = {Publisher: [The University of Chicago Press, Philosophy of Science Association]},
	pages = {1--25},
	file = {JSTOR Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\3JP2ITTN\\Machamer et al. - 2000 - Thinking about Mechanisms.pdf:application/pdf},
}

@article{levesque_our_2014,
	title = {On our best behaviour},
	volume = {212},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370214000356},
	doi = {10.1016/j.artint.2014.03.007},
	abstract = {The science of AI is concerned with the study of intelligent forms of behaviour in computational terms. But what does it tell us when a good semblance of a behaviour can be achieved using cheap tricks that seem to have little to do with what we intuitively imagine intelligence to be? Are these intuitions wrong, and is intelligence really just a bag of tricks? Or are the philosophers right, and is a behavioural understanding of intelligence simply too weak? I think both of these are wrong. I suggest in the context of question-answering that what matters when it comes to the science of AI is not a good semblance of intelligent behaviour at all, but the behaviour itself, what it depends on, and how it can be achieved. I go on to discuss two major hurdles that I believe will need to be cleared.},
	language = {en},
	urldate = {2024-08-01},
	journal = {Artificial Intelligence},
	author = {Levesque, Hector J.},
	month = jul,
	year = {2014},
	pages = {27--35},
	file = {Levesque - 2014 - On our best behaviour.pdf:C\:\\Users\\user\\Zotero\\storage\\B7NSIZDE\\Levesque - 2014 - On our best behaviour.pdf:application/pdf},
}

@inproceedings{borzunov_petals_2023,
	address = {Toronto, Canada},
	title = {Petals: {Collaborative} {Inference} and {Fine}-tuning of {Large} {Models}},
	shorttitle = {Petals},
	url = {https://aclanthology.org/2023.acl-demo.54},
	doi = {10.18653/v1/2023.acl-demo.54},
	abstract = {Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM offloading or hosted APIs. However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits. In this work, we propose Petals - a system for inference and fine-tuning of large models collaboratively by joining the resources of multiple parties. We demonstrate that this strategy outperforms offloading for very large models, running inference of BLOOM-176B on consumer GPUs with {\textbackslash}mbox{\textbackslash}approx1 step per second, which is enough for many interactive LLM applications. Unlike most inference APIs, Petals also natively exposes hidden states of served models, allowing to train and share custom model extensions based on efficient fine-tuning methods. The system, its source code, and documentation are available at https://petals.mlVideo (2 min): https://youtu.be/F4muLI-0hTE},
	urldate = {2024-08-01},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 3: {System} {Demonstrations})},
	publisher = {Association for Computational Linguistics},
	author = {Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and Riabinin, Maksim and Belkada, Younes and Chumachenko, Artem and Samygin, Pavel and Raffel, Colin},
	editor = {Bollegala, Danushka and Huang, Ruihong and Ritter, Alan},
	month = jul,
	year = {2023},
	pages = {558--568},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\92X3SXPK\\Borzunov et al. - 2023 - Petals Collaborative Inference and Fine-tuning of.pdf:application/pdf},
}

@inproceedings{sarti_inseq_2023,
	address = {Toronto, Canada},
	title = {Inseq: {An} {Interpretability} {Toolkit} for {Sequence} {Generation} {Models}},
	shorttitle = {Inseq},
	url = {https://aclanthology.org/2023.acl-demo.40},
	doi = {10.18653/v1/2023.acl-demo.40},
	abstract = {Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.},
	urldate = {2024-08-01},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 3: {System} {Demonstrations})},
	publisher = {Association for Computational Linguistics},
	author = {Sarti, Gabriele and Feldhus, Nils and Sickert, Ludwig and van der Wal, Oskar},
	editor = {Bollegala, Danushka and Huang, Ruihong and Ritter, Alan},
	month = jul,
	year = {2023},
	pages = {421--435},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\F3EE2UDS\\Sarti et al. - 2023 - Inseq An Interpretability Toolkit for Sequence Ge.pdf:application/pdf},
}

@misc{yin_interpreting_2022,
	title = {Interpreting {Language} {Models} with {Contrastive} {Explanations}},
	url = {http://arxiv.org/abs/2202.10419},
	doi = {10.48550/arXiv.2202.10419},
	abstract = {Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics. Existing explanation methods conflate evidence for all these features into a single explanation, which is less interpretable for human understanding. To disentangle the different decisions in language modeling, we focus on explaining language models contrastively: we look for salient input tokens that explain why the model predicted one token instead of another. We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers. We also identify groups of contrastive decisions where the model uses similar evidence, and we are able to characterize what input tokens models use during various language generation decisions.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Yin, Kayo and Neubig, Graham},
	month = may,
	year = {2022},
	note = {arXiv:2202.10419 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\UULF5U48\\Yin e Neubig - 2022 - Interpreting Language Models with Contrastive Expl.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\VCPMVK6I\\2202.html:text/html},
}

@misc{cunningham_sparse_2023,
	title = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.08600},
	doi = {10.48550/arXiv.2309.08600},
	abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	month = oct,
	year = {2023},
	note = {arXiv:2309.08600 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 20 pages, 18 figures, 2 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\4XMQSQXT\\Cunningham et al. - 2023 - Sparse Autoencoders Find Highly Interpretable Feat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\9IBVC4TW\\2309.html:text/html},
}

@article{gao_scaling_nodate,
	title = {Scaling and evaluating sparse autoencoders},
	language = {en},
	author = {Gao, Leo and Goh, Gabriel and Sutskever, Ilya},
	file = {Gao et al. - Scaling and evaluating sparse autoencoders.pdf:C\:\\Users\\user\\Zotero\\storage\\SMRHXQCN\\Gao et al. - Scaling and evaluating sparse autoencoders.pdf:application/pdf},
}

@misc{bereska_mechanistic_2024,
	title = {Mechanistic {Interpretability} for {AI} {Safety} -- {A} {Review}},
	url = {http://arxiv.org/abs/2404.14082},
	doi = {10.48550/arXiv.2404.14082},
	abstract = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Bereska, Leonard and Gavves, Efstratios},
	month = jul,
	year = {2024},
	note = {arXiv:2404.14082 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: HTML version: https://leonardbereska.github.io/blog/2024/mechinterpreview/},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\J4GWZVJZ\\Bereska e Gavves - 2024 - Mechanistic Interpretability for AI Safety -- A Re.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\KNVECWFR\\2404.html:text/html},
}

@inproceedings{sarti_quantifying_2023,
	title = {Quantifying the {Plausibility} of {Context} {Reliance} in {Neural} {Machine} {Translation}},
	url = {https://openreview.net/forum?id=XTHfNGI3zT},
	abstract = {Establishing whether language models can use contextual information in a human-plausible way is important to ensure their safe adoption in real-world settings. However, the questions of \${\textbackslash}textit\{when\}\$ and \${\textbackslash}textit\{which parts\}\$ of the context affect model generations are typically tackled separately, and current plausibility evaluations are practically limited to a handful of artificial benchmarks. To address this, we introduce \${\textbackslash}textbf\{P\}\$lausibility \${\textbackslash}textbf\{E\}\$valuation of \${\textbackslash}textbf\{Co\}\$ntext \${\textbackslash}textbf\{Re\}\$liance (PECoRe), an end-to-end interpretability framework designed to quantify context usage in language models' generations. Our approach leverages model internals to (i) contrastively identify context-sensitive target tokens in generated texts and (ii) link them to contextual cues justifying their prediction. We use PECoRe to quantify the plausibility of context-aware machine translation models, comparing model rationales with human annotations across several discourse-level phenomena. Finally, we apply our method to unannotated model translations to identify context-mediated predictions and highlight instances of (im)plausible context usage throughout generation.},
	language = {en},
	urldate = {2024-08-01},
	author = {Sarti, Gabriele and Chrupała, Grzegorz and Nissim, Malvina and Bisazza, Arianna},
	month = oct,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\2R75VR43\\Sarti et al. - 2023 - Quantifying the Plausibility of Context Reliance i.pdf:application/pdf},
}

@misc{ferrando_primer_2024,
	title = {A {Primer} on the {Inner} {Workings} of {Transformer}-based {Language} {Models}},
	url = {http://arxiv.org/abs/2405.00208},
	doi = {10.48550/arXiv.2405.00208},
	abstract = {The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture. We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-jussà, Marta R.},
	month = may,
	year = {2024},
	note = {arXiv:2405.00208 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\TN7M5XAA\\Ferrando et al. - 2024 - A Primer on the Inner Workings of Transformer-base.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\AHTBMD9J\\2405.html:text/html},
}

@misc{park_linear_2024,
	title = {The {Linear} {Representation} {Hypothesis} and the {Geometry} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.03658},
	doi = {10.48550/arXiv.2311.03658},
	abstract = {Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Park, Kiho and Choe, Yo Joong and Veitch, Victor},
	month = jul,
	year = {2024},
	note = {arXiv:2311.03658 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted for a presentation at ICML 2024 and an oral presentation at NeurIPS 2023 Workshop on Causal Representation Learning. Code is available at https://github.com/KihoPark/linear\_rep\_geometry},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\JP63KIT9\\Park et al. - 2024 - The Linear Representation Hypothesis and the Geome.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\4DTFE6U4\\2311.html:text/html},
}

@misc{park_geometry_2024,
	title = {The {Geometry} of {Categorical} and {Hierarchical} {Concepts} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2406.01506},
	doi = {10.48550/arXiv.2406.01506},
	abstract = {Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability. In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as \{'mammal', 'bird', 'reptile', 'fish'\}, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that 'dog' is a kind of 'mammal' encoded? We show how to extend the linear representation hypothesis to answer these questions. We find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure. We validate these theoretical results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from WordNet.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Park, Kiho and Choe, Yo Joong and Jiang, Yibo and Veitch, Victor},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01506 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Code is available at https://github.com/KihoPark/LLM\_Categorical\_Hierarchical\_Representations},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\RHX7ZF9C\\Park et al. - 2024 - The Geometry of Categorical and Hierarchical Conce.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\CWGKML72\\2406.html:text/html},
}

@misc{jiang_origins_2024,
	title = {On the {Origins} of {Linear} {Representations} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2403.03867},
	doi = {10.48550/arXiv.2403.03867},
	abstract = {Recent works have argued that high-level semantic concepts are encoded "linearly" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon and Veitch, Victor},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03867 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\KB36DZF7\\Jiang et al. - 2024 - On the Origins of Linear Representations in Large .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\2FIEXNH6\\2403.html:text/html},
}

@article{madsen_post-hoc_2023,
	title = {Post-hoc {Interpretability} for {Neural} {NLP}: {A} {Survey}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Post-hoc {Interpretability} for {Neural} {NLP}},
	url = {http://arxiv.org/abs/2108.04840},
	doi = {10.1145/3546577},
	abstract = {Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.},
	number = {8},
	urldate = {2024-08-01},
	journal = {ACM Computing Surveys},
	author = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
	month = aug,
	year = {2023},
	note = {arXiv:2108.04840 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {1--42},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\GH5J9SDD\\Madsen et al. - 2023 - Post-hoc Interpretability for Neural NLP A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\VZ6M8XT8\\2108.html:text/html},
}

@misc{wang_interpretability_2022,
	title = {Interpretability in the {Wild}: a {Circuit} for {Indirect} {Object} {Identification} in {GPT}-2 small},
	shorttitle = {Interpretability in the {Wild}},
	url = {http://arxiv.org/abs/2211.00593},
	doi = {10.48550/arXiv.2211.00593},
	abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	month = nov,
	year = {2022},
	note = {arXiv:2211.00593 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\7ZP8NVYB\\Wang et al. - 2022 - Interpretability in the Wild a Circuit for Indire.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\EKYFL8BR\\2211.html:text/html},
}

@misc{marks_geometry_2023,
	title = {The {Geometry} of {Truth}: {Emergent} {Linear} {Structure} in {Large} {Language} {Model} {Representations} of {True}/{False} {Datasets}},
	shorttitle = {The {Geometry} of {Truth}},
	url = {http://arxiv.org/abs/2310.06824},
	abstract = {Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM’s internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we curate high-quality datasets of true/false statements and use them to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM’s forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that language models linearly represent the truth or falsehood of factual statements. We also introduce a novel technique, mass-mean probing, which generalizes better and is more causally implicated in model outputs than other probing techniques.},
	language = {en},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Marks, Samuel and Tegmark, Max},
	month = dec,
	year = {2023},
	note = {arXiv:2310.06824 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Marks e Tegmark - 2023 - The Geometry of Truth Emergent Linear Structure i.pdf:C\:\\Users\\user\\Zotero\\storage\\GGHIJ56V\\Marks e Tegmark - 2023 - The Geometry of Truth Emergent Linear Structure i.pdf:application/pdf},
}

@inproceedings{proietti_does_2022,
	address = {Gyeongju, Republic of Korea},
	title = {Does {BERT} {Recognize} an {Agent}? {Modeling} {Dowty}'s {Proto}-{Roles} with {Contextual} {Embeddings}},
	copyright = {All rights reserved},
	shorttitle = {Does {BERT} {Recognize} an {Agent}?},
	url = {https://aclanthology.org/2022.coling-1.360},
	abstract = {Contextual embeddings build multidimensional representations of word tokens based on their context of occurrence. Such models have been shown to achieve a state-of-the-art performance on a wide variety of tasks. Yet, the community struggles in understanding what kind of semantic knowledge these representations encode. We report a series of experiments aimed at investigating to what extent one of such models, BERT, is able to infer the semantic relations that, according to Dowty's Proto-Roles theory, a verbal argument receives by virtue of its role in the event described by the verb. This hypothesis were put to test by learning a linear mapping from the BERT's verb embeddings to an interpretable space of semantic properties built from the linguistic dataset by White et al. (2016). In a first experiment we tested whether the semantic properties inferred from a typed version of the BERT embeddings would be more linguistically plausible than those produced by relying on static embeddings. We then move to evaluate the semantic properties inferred from the contextual embeddings both against those available in the original dataset, as well as by assessing their ability to model the semantic properties possessed by the agent of the verbs participating in the so-called causative alternation.},
	urldate = {2024-08-01},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Proietti, Mattia and Lebani, Gianluca and Lenci, Alessandro},
	editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
	month = oct,
	year = {2022},
	pages = {4101--4112},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\B29CJSAF\\Proietti et al. - 2022 - Does BERT Recognize an Agent Modeling Dowty's Pro.pdf:application/pdf},
}

@article{proietti_proto-role_2024,
	title = {On the proto-role properties inferred by transformer language models},
	copyright = {All rights reserved},
	issn = {1720-9331},
	doi = {10.1418/113930},
	number = {1/2024},
	journal = {Lingue e linguaggio},
	author = {Proietti, Mattia and Lebani, Gianluca E. and Lenci, Alessandro},
	year = {2024},
	file = {Rivisteweb\: Mattia Proietti, Gianluca E. Lebani, Alessandro Lenci, On the proto-role properties inferred by transformer language models:C\:\\Users\\user\\Zotero\\storage\\7LDK87TI\\113930.html:text/html},
}

@misc{tenney_bert_2019,
	title = {{BERT} {Rediscovers} the {Classical} {NLP} {Pipeline}},
	url = {http://arxiv.org/abs/1905.05950},
	abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We ﬁnd that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations.},
	language = {en},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
	month = aug,
	year = {2019},
	note = {arXiv:1905.05950 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Presented at ACL 2019},
}

@misc{mccoy_right_2019,
	title = {Right for the {Wrong} {Reasons}: {Diagnosing} {Syntactic} {Heuristics} in {Natural} {Language} {Inference}},
	shorttitle = {Right for the {Wrong} {Reasons}},
	url = {http://arxiv.org/abs/1902.01007},
	doi = {10.48550/arXiv.1902.01007},
	abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {McCoy, R. Thomas and Pavlick, Ellie and Linzen, Tal},
	month = jun,
	year = {2019},
	note = {arXiv:1902.01007 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\JABLYEJ2\\McCoy et al. - 2019 - Right for the Wrong Reasons Diagnosing Syntactic .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\YEK3QIHJ\\1902.html:text/html},
}

@article{levesque_is_nodate,
	title = {Is it {Enough} to {Get} the {Behaviour} {Right}?},
	abstract = {This paper deals with the relationship between intelligent behaviour, on the one hand, and the mental qualities needed to produce it, on the other. We consider two well-known opposing positions on this issue: one due to Alan Turing and one due to John Searle (via the Chinese Room). In particular, we argue against Searle, showing that his answer to the so-called System Reply does not work. The argument takes a novel form: we shift the debate to a different and more plausible room where the required conversational behaviour is much easier to characterize and to analyze. Despite being much simpler than the Chinese Room, we show that the behaviour there is still complex enough that it cannot be produced without appropriate mental qualities.},
	language = {en},
	author = {Levesque, Hector J},
	file = {Levesque - Is it Enough to Get the Behaviour Right.pdf:C\:\\Users\\user\\Zotero\\storage\\68SDMZ6F\\Levesque - Is it Enough to Get the Behaviour Right.pdf:application/pdf},
}

@misc{kauf_comparing_2024,
	title = {Comparing {Plausibility} {Estimates} in {Base} and {Instruction}-{Tuned} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2403.14859},
	abstract = {Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood (\${\textbackslash}textit\{LL\}\$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) \${\textbackslash}textit\{LL\}\$-based performance is still inferior to human performance; (iii) instruction-tuned models have worse \${\textbackslash}textit\{LL\}\$-based performance than base models. In Experiment 2, we show that \${\textbackslash}textit\{LL\}\$ scores across models are modulated by context in the expected way, showing high performance on three metrics of context-sensitive plausibility and providing a direct match to explicit human plausibility judgments. Overall, \${\textbackslash}textit\{LL\}\$ estimates remain a more reliable measure of plausibility in LLMs than direct prompting.},
	language = {en},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Kauf, Carina and Chersoni, Emmanuele and Lenci, Alessandro and Fedorenko, Evelina and Ivanova, Anna A.},
	month = mar,
	year = {2024},
	note = {arXiv:2403.14859 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Kauf et al. - 2024 - Comparing Plausibility Estimates in Base and Instr.pdf:C\:\\Users\\user\\Zotero\\storage\\G6NWCZSX\\Kauf et al. - 2024 - Comparing Plausibility Estimates in Base and Instr.pdf:application/pdf},
}

@misc{hosseini_artificial_2023,
	title = {Artificial neural network language models predict human brain responses to language even after a developmentally realistic amount of training},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.10.04.510681v2},
	doi = {10.1101/2022.10.04.510681},
	abstract = {Artificial neural networks have emerged as computationally plausible models of human language processing. A major criticism of these models is that the amount of training data they receive far exceeds that of humans during language learning. Here, we use two complementary approaches to ask how the models’ ability to capture human fMRI responses to sentences is affected by the amount of training data. First, we evaluate GPT-2 models trained on 1 million, 10 million, 100 million, or 1 billion words against an fMRI benchmark. We consider the 100-million-word model to be developmentally plausible in terms of the amount of training data given that this amount is similar to what children are estimated to be exposed to during the first 10 years of life. Second, we test the performance of a GPT-2 model trained on a 9-billion-token dataset to reach state-of-the-art next-word prediction performance on the human benchmark at different stages during training. Across both approaches, we find that (i) the models trained on a developmentally plausible amount of data already achieve near-maximal performance in capturing fMRI responses to sentences. Further, (ii) lower perplexity—a measure of next-word prediction performance—is associated with stronger alignment with human data, suggesting that models that have received enough training to achieve sufficiently high next-word prediction performance also acquire representations of sentences that are predictive of human fMRI responses. In tandem, these findings establish that although some training is necessary for the models’ predictive ability, a developmentally realistic amount of training (∼100 million words) may suffice.},
	language = {en},
	urldate = {2024-08-01},
	publisher = {bioRxiv},
	author = {Hosseini, Eghbal A. and Schrimpf, Martin and Zhang, Yian and Bowman, Samuel and Zaslavsky, Noga and Fedorenko, Evelina},
	month = sep,
	year = {2023},
	note = {Pages: 2022.10.04.510681
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\W5SL65B6\\Hosseini et al. - 2023 - Artificial neural network language models predict .pdf:application/pdf},
}

@article{fedorenko_language_2024,
	title = {Language is primarily a tool for communication rather than thought},
	volume = {630},
	copyright = {2024 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07522-w},
	doi = {10.1038/s41586-024-07522-w},
	abstract = {Language is a defining characteristic of our species, but the function, or functions, that it serves has been debated for centuries. Here we bring recent evidence from neuroscience and allied disciplines to argue that in modern humans, language is a tool for communication, contrary to a prominent view that we use language for thinking. We begin by introducing the brain network that supports linguistic ability in humans. We then review evidence for a double dissociation between language and thought, and discuss several properties of language that suggest that it is optimized for communication. We conclude that although the emergence of language has unquestionably transformed human culture, language does not appear to be a prerequisite for complex thought, including symbolic thought. Instead, language is a powerful tool for the transmission of cultural knowledge; it plausibly co-evolved with our thinking and reasoning capacities, and only reflects, rather than gives rise to, the signature sophistication of human cognition.},
	language = {en},
	number = {8017},
	urldate = {2024-08-01},
	journal = {Nature},
	author = {Fedorenko, Evelina and Piantadosi, Steven T. and Gibson, Edward A. F.},
	month = jun,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Language},
	pages = {575--586},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\L9YHD8EI\\Fedorenko et al. - 2024 - Language is primarily a tool for communication rat.pdf:application/pdf},
}

@article{fedorenko_language_2024-1,
	title = {The language network as a natural kind within the broader landscape of the human brain},
	volume = {25},
	copyright = {2024 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-024-00802-4},
	doi = {10.1038/s41583-024-00802-4},
	abstract = {Language behaviour is complex, but neuroscientific evidence disentangles it into distinct components supported by dedicated brain areas or networks. In this Review, we describe the ‘core’ language network, which includes left-hemisphere frontal and temporal areas, and show that it is strongly interconnected, independent of input and output modalities, causally important for language and language-selective. We discuss evidence that this language network plausibly stores language knowledge and supports core linguistic computations related to accessing words and constructions from memory and combining them to interpret (decode) or generate (encode) linguistic messages. We emphasize that the language network works closely with, but is distinct from, both lower-level — perceptual and motor — mechanisms and higher-level systems of knowledge and reasoning. The perceptual and motor mechanisms process linguistic signals, but, in contrast to the language network, are sensitive only to these signals’ surface properties, not their meanings; the systems of knowledge and reasoning (such as the system that supports social reasoning) are sometimes engaged during language use but are not language-selective. This Review lays a foundation both for in-depth investigations of these different components of the language processing pipeline and for probing inter-component interactions.},
	language = {en},
	number = {5},
	urldate = {2024-08-01},
	journal = {Nature Reviews Neuroscience},
	author = {Fedorenko, Evelina and Ivanova, Anna A. and Regev, Tamar I.},
	month = may,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Language},
	pages = {289--312},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\YE9K8N2X\\Fedorenko et al. - 2024 - The language network as a natural kind within the .pdf:application/pdf},
}

@misc{hu_prompting_2023,
	title = {Prompting is not a substitute for probability measurements in large language models},
	url = {http://arxiv.org/abs/2305.13264},
	doi = {10.48550/arXiv.2305.13264},
	abstract = {Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models' probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models' linguistic knowledge. Broadly, we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Hu, Jennifer and Levy, Roger},
	month = oct,
	year = {2023},
	note = {arXiv:2305.13264 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Fundamental questions:
1- how should we interpret models’ responses to metalinguistic prompts?
2- How do these responses correspond to models’ internal representations?
3- When should we use metalinguistic prompts as opposed to direct measurements?” 

Goal: “evaluate the validity of metalinguistic prompting as a way of measuring LLMs’ internal knowledge.” 
On the distinction between competence and perfomarmnce in LLMs. The authors identify the former with “information encoded in a model’s isolated-sentence string probability distribution” and the second with “ the model’s behavioral responses to prompts.” While the latter si straightforward, the second may be questioned. Why not the internal representations learned durin training? (weights, biases and all parameters, emb, pos emb, unemb, attn, mlp)

},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\YGIEBIWV\\Hu e Levy - 2023 - Prompting is not a substitute for probability meas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\66JCXHRM\\2305.html:text/html},
}

@article{hu_precision_2023,
	title = {Precision {fMRI} reveals that the language-selective network supports both phrase-structure building and lexical access during language production},
	volume = {33},
	issn = {1047-3211},
	url = {https://doi.org/10.1093/cercor/bhac350},
	doi = {10.1093/cercor/bhac350},
	abstract = {A fronto-temporal brain network has long been implicated in language comprehension. However, this network’s role in language production remains debated. In particular, it remains unclear whether all or only some language regions contribute to production, and which aspects of production these regions support. Across 3 functional magnetic resonance imaging experiments that rely on robust individual-subject analyses, we characterize the language network’s response to high-level production demands. We report 3 novel results. First, sentence production, spoken or typed, elicits a strong response throughout the language network. Second, the language network responds to both phrase-structure building and lexical access demands, although the response to phrase-structure building is stronger and more spatially extensive, present in every language region. Finally, contra some proposals, we find no evidence of brain regions—within or outside the language network—that selectively support phrase-structure building in production relative to comprehension. Instead, all language regions respond more strongly during production than comprehension, suggesting that production incurs a greater cost for the language network. Together, these results align with the idea that language comprehension and production draw on the same knowledge representations, which are stored in a distributed manner within the language-selective network and are used to both interpret and generate linguistic utterances.},
	number = {8},
	urldate = {2024-08-01},
	journal = {Cerebral Cortex},
	author = {Hu, Jennifer and Small, Hannah and Kean, Hope and Takahashi, Atsushi and Zekelman, Leo and Kleinman, Daniel and Ryan, Elizabeth and Nieto-Castañón, Alfonso and Ferreira, Victor and Fedorenko, Evelina},
	month = apr,
	year = {2023},
	pages = {4384--4404},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\9MC3ZPLR\\Hu et al. - 2023 - Precision fMRI reveals that the language-selective.pdf:application/pdf},
}

@article{searle_minds_1980,
	title = {Minds, brains, and programs},
	volume = {3},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X00005756/type/journal_article},
	doi = {10.1017/S0140525X00005756},
	abstract = {This article can be viewed as an attempt to explore the consequences of two propositions. (1) Intentionality in human beings (and animals) is a product of causal features of the brain. I assume this is an empirical fact about the actual causal relations between mental processes and brains. It says simply that certain brain processes are sufficient for intentionality. (2) Instantiating a computer program is never by itself a sufficient condition of intentionality. The main argument of this paper is directed at establishing this claim. The form of the argument is to show how a human agent could instantiate the program and still not have the relevant intentionality. These two propositions have the following consequences: (3) The explanation of how the brain produces intentionality cannot be that it does it by instantiating a computer program. This is a strict logical consequence of 1 and 2. (4) Any mechanism capable of producing intentionality must have causal powers equal to those of the brain. This is meant to be a trivial consequence of 1. (5) Any attempt literally to create intentionality artificially (strong AI) could not succeed just by designing programs but would have to duplicate the causal powers of the human brain. This follows from 2 and 4. "Could a machine think?" On the argument advanced here only a machine could think, and only very special kinds of machines, namely brains and machines with internal causal powers equivalent to those of brains. And that is why strong AI has little to tell us about thinking, since it is not about machines but about programs, and no program by itself is sufficient for thinking.},
	language = {en},
	number = {3},
	urldate = {2024-08-01},
	journal = {Behavioral and Brain Sciences},
	author = {Searle, John R.},
	month = sep,
	year = {1980},
	pages = {417--424},
	file = {Searle - 1980 - Minds, brains, and programs.pdf:C\:\\Users\\user\\Zotero\\storage\\9YIYABCX\\Searle - 1980 - Minds, brains, and programs.pdf:application/pdf},
}

@inproceedings{gladkova_analogy-based_2016,
	address = {San Diego, California},
	title = {Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't.},
	shorttitle = {Analogy-based detection of morphological and semantic relations with word embeddings},
	url = {https://aclanthology.org/N16-2002},
	doi = {10.18653/v1/N16-2002},
	urldate = {2024-08-01},
	booktitle = {Proceedings of the {NAACL} {Student} {Research} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Gladkova, Anna and Drozd, Aleksandr and Matsuoka, Satoshi},
	editor = {Andreas, Jacob and Choi, Eunsol and Lazaridou, Angeliki},
	month = jun,
	year = {2016},
	pages = {8--15},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\X2DWRUTU\\Gladkova et al. - 2016 - Analogy-based detection of morphological and seman.pdf:application/pdf},
}

@inproceedings{mikolov_linguistic_2013,
	address = {Atlanta, Georgia},
	title = {Linguistic {Regularities} in {Continuous} {Space} {Word} {Representations}},
	url = {https://aclanthology.org/N13-1090},
	urldate = {2024-08-01},
	booktitle = {Proceedings of the 2013 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	editor = {Vanderwende, Lucy and Daumé III, Hal and Kirchhoff, Katrin},
	month = jun,
	year = {2013},
	pages = {746--751},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\R5QMSURH\\Mikolov et al. - 2013 - Linguistic Regularities in Continuous Space Word R.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\JX8FUJSV\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\HA58EZ6Q\\1706.html:text/html},
}

@misc{amir_deep_2022,
	title = {Deep {ViT} {Features} as {Dense} {Visual} {Descriptors}},
	url = {http://arxiv.org/abs/2112.05814},
	doi = {10.48550/arXiv.2112.05814},
	abstract = {We study the use of deep features extracted from a pretrained Vision Transformer (ViT) as dense visual descriptors. We observe and empirically demonstrate that such features, when extractedfrom a self-supervised ViT model (DINO-ViT), exhibit several striking properties, including: (i) the features encode powerful, well-localized semantic information, at high spatial granularity, such as object parts; (ii) the encoded semantic information is shared across related, yet different object categories, and (iii) positional bias changes gradually throughout the layers. These properties allow us to design simple methods for a variety of applications, including co-segmentation, part co-segmentation and semantic correspondences. To distill the power of ViT features from convoluted design choices, we restrict ourselves to lightweight zero-shot methodologies (e.g., binning and clustering) applied directly to the features. Since our methods require no additional training nor data, they are readily applicable across a variety of domains. We show by extensive qualitative and quantitative evaluation that our simple methodologies achieve competitive results with recent state-of-the-art supervised methods, and outperform previous unsupervised methods by a large margin. Code is available in dino-vit-features.github.io.},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Amir, Shir and Gandelsman, Yossi and Bagon, Shai and Dekel, Tali},
	month = oct,
	year = {2022},
	note = {arXiv:2112.05814 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\VURSBNEY\\Amir et al. - 2022 - Deep ViT Features as Dense Visual Descriptors.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\NPNYVN8K\\2112.html:text/html},
}

@misc{aw_instruction-tuning_2023,
	title = {Instruction-tuning {Aligns} {LLMs} to the {Human} {Brain}},
	url = {http://arxiv.org/abs/2312.00575},
	doi = {10.48550/arXiv.2312.00575},
	abstract = {Instruction-tuning is a widely adopted method of finetuning that enables large language models (LLMs) to generate output that more closely resembles human responses to natural language queries, in many cases leading to human-level performance on diverse testbeds. However, it remains unclear whether instruction-tuning truly makes LLMs more similar to how humans process language. We investigate the effect of instruction-tuning on LLM-human similarity in two ways: (1) brain alignment, the similarity of LLM internal representations to neural activity in the human language system, and (2) behavioral alignment, the similarity of LLM and human behavior on a reading task. We assess 25 vanilla and instruction-tuned LLMs across three datasets involving humans reading naturalistic stories and sentences. We discover that instruction-tuning generally enhances brain alignment by an average of 6\%, but does not have a similar effect on behavioral alignment. To identify the factors underlying LLM-brain alignment, we compute correlations between the brain alignment of LLMs and various model properties, such as model size, various problem-solving abilities, and performance on tasks requiring world knowledge spanning various domains. Notably, we find a strong positive correlation between brain alignment and model size (r = 0.95), as well as performance on tasks requiring world knowledge (r = 0.81). Our results demonstrate that instruction-tuning LLMs improves both world knowledge representations and brain alignment, suggesting that mechanisms that encode world knowledge in LLMs also improve representational alignment to the human brain.},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Aw, Khai Loong and Montariol, Syrielle and AlKhamissi, Badr and Schrimpf, Martin and Bosselut, Antoine},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00575 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\EX7CVEW6\\Aw et al. - 2023 - Instruction-tuning Aligns LLMs to the Human Brain.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\TYS8TAIA\\2312.html:text/html},
}

@article{evans_myth_2009,
	title = {The myth of language universals: {Language} diversity and its importance for cognitive science},
	volume = {32},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0140-525X, 1469-1825},
	shorttitle = {The myth of language universals},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X0999094X/type/journal_article},
	doi = {10.1017/S0140525X0999094X},
	abstract = {Talk of linguistic universals has given cognitive scientists the impression that languages are all built to a common pattern. In fact, there are vanishingly few universals of language in the direct sense that all languages exhibit them. Instead, diversity can be found at almost every level of linguistic organization. This fundamentally changes the object of enquiry from a cognitive science perspective. This target article summarizes decades of cross-linguistic work by typologists and descriptive linguists, showing just how few and unprofound the universal characteristics of language are, once we honestly confront the diversity offered to us by the world’s 6,000 to 8,000 languages. After surveying the various uses of “universal,” we illustrate the ways languages vary radically in sound, meaning, and syntactic organization, and then we examine in more detail the core grammatical machinery of recursion, constituency, and grammatical relations. Although there are signiﬁcant recurrent patterns in organization, these are better explained as stable engineering solutions satisfying multiple design constraints, reﬂecting both cultural-historical factors and the constraints of human cognition.},
	language = {en},
	number = {5},
	urldate = {2024-08-05},
	journal = {Behavioral and Brain Sciences},
	author = {Evans, Nicholas and Levinson, Stephen C.},
	month = oct,
	year = {2009},
	pages = {429--448},
	file = {Evans e Levinson - 2009 - The myth of language universals Language diversit.pdf:C\:\\Users\\user\\Zotero\\storage\\YZBV58W9\\Evans e Levinson - 2009 - The myth of language universals Language diversit.pdf:application/pdf},
}

@misc{weiss_thinking_2021,
	title = {Thinking {Like} {Transformers}},
	url = {http://arxiv.org/abs/2106.06981},
	doi = {10.48550/arXiv.2106.06981},
	abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
	month = jul,
	year = {2021},
	note = {arXiv:2106.06981 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\4BVT75AY\\Weiss et al. - 2021 - Thinking Like Transformers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\QPMAZNXM\\2106.html:text/html},
}

@misc{misra_experimental_2024,
	title = {Experimental {Contexts} {Can} {Facilitate} {Robust} {Semantic} {Property} {Inference} in {Language} {Models}, but {Inconsistently}},
	url = {http://arxiv.org/abs/2401.06640},
	abstract = {Recent zero-shot evaluations have highlighted important limitations in the abilities of language models (LMs) to perform meaning extraction. However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions. How well does this translate to previously studied meaning-sensitive tasks? We present a casestudy on the extent to which experimental contexts can improve LMs’ robustness in performing property inheritance—predicting semantic properties of novel concepts, a task that they have been previously shown to fail on. Upon carefully controlling the nature of the in-context examples and the instructions, our work reveals that they can indeed lead to nontrivial property inheritance behavior in LMs. However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computational principles of semantic property inference are yet to be mastered by LMs.},
	language = {en},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Misra, Kanishka and Ettinger, Allyson and Mahowald, Kyle},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06640 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Misra et al. - 2024 - Experimental Contexts Can Facilitate Robust Semant.pdf:C\:\\Users\\user\\Zotero\\storage\\DBUAN2TY\\Misra et al. - 2024 - Experimental Contexts Can Facilitate Robust Semant.pdf:application/pdf},
}

@misc{gu_language_2023,
	title = {Do language models have coherent mental models of everyday things?},
	url = {http://arxiv.org/abs/2212.10029},
	abstract = {When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that "the yolk surrounds the shell" is a false statement. Do language models similarly have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts, expressed as 11,720 "X relation Y?" true/false questions. Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent "parts mental models" (54-59\% accurate, 19-43\% conditional constraint violation). We propose an extension where we add a constraint satisfaction layer on top of the LM's raw predictions to apply commonsense constraints. As well as removing inconsistencies, we find that this also significantly improves accuracy (by 16-20\%), suggesting how the incoherence of the LM's pictures of everyday things can be significantly reduced.},
	language = {en},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Gu, Yuling and Mishra, Bhavana Dalvi and Clark, Peter},
	month = jun,
	year = {2023},
	note = {arXiv:2212.10029 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Gu et al. - 2023 - Do language models have coherent mental models of .pdf:C\:\\Users\\user\\Zotero\\storage\\THYV4JRX\\Gu et al. - 2023 - Do language models have coherent mental models of .pdf:application/pdf},
}

@inproceedings{kassner_language_2023,
	address = {Singapore},
	title = {Language {Models} with {Rationality}},
	url = {https://aclanthology.org/2023.emnlp-main.877},
	doi = {10.18653/v1/2023.emnlp-main.877},
	abstract = {While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent “beliefs”. This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a **rational, self-reflecting layer** on top of the LLM. First, given a question, we construct a **belief graph** using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8\%-11\% absolute) without harming overall answer accuracy, resulting in answers supported by faithful chains of reasoning drawn from a more consistent belief system. This suggests a new style of system architecture in which an LLM extended with a rational layer can provide an interpretable window into system beliefs, add a systematic reasoning capability, and repair latent inconsistencies present in the LLM.},
	urldate = {2024-08-07},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kassner, Nora and Tafjord, Oyvind and Sabharwal, Ashish and Richardson, Kyle and Schuetze, Hinrich and Clark, Peter},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {14190--14201},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\RLNEKSQV\\Kassner et al. - 2023 - Language Models with Rationality.pdf:application/pdf},
}

@article{dentella_testing_nodate,
	title = {Testing {AI} on language comprehension tasks reveals insensitivity to underlying meaning},
	abstract = {Large Language Models (LLMs) are recruited in applications that span from clinical assistance and legal support to question answering and education. Their success in specialized tasks has led to the claim that they possess human-like linguistic capabilities related to compositional understanding and reasoning. Yet, reverse-engineering is bound by Moravec’s Paradox, according to which easy skills are hard. We systematically assess 7 state-of-the-art models on a novel benchmark. Models answered a series of comprehension questions, each prompted multiple times in two settings, permitting one-word or open-length replies. Each question targets a short text featuring high-frequency linguistic constructions. To establish a baseline for achieving human-like performance, we tested 400 humans on the same prompts. Based on a dataset of n=26,680 datapoints, we discovered that LLMs perform at chance accuracy and waver considerably in their answers. Quantitatively, the tested models are outperformed by humans, and qualitatively their answers showcase distinctly non-human errors in language understanding. We interpret this evidence as suggesting that, despite their usefulness in various tasks, current AI models fall short of understanding language in a way that matches humans, and we argue that this may be due to their lack of a compositional operator for regulating grammatical and semantic information.},
	language = {en},
	author = {Dentella, Vittoria and Günther, Fritz and Murphy, Elliot and Marcus, Gary and Leivada, Evelina},
	file = {Dentella et al. - Testing AI on language comprehension tasks reveals.pdf:C\:\\Users\\user\\Zotero\\storage\\USP5WXUW\\Dentella et al. - Testing AI on language comprehension tasks reveals.pdf:application/pdf},
}

@article{katzir_why_2023,
	title = {Why {Large} {Language} {Models} {Are} {Poor} {Theories} of {Human} {Linguistic} {Cognition}: {A} {Reply} to {Piantadosi}},
	volume = {17},
	copyright = {Copyright (c) 2023 Roni Katzir},
	issn = {1450-3417},
	shorttitle = {Why {Large} {Language} {Models} {Are} {Poor} {Theories} of {Human} {Linguistic} {Cognition}},
	url = {https://bioling.psychopen.eu/index.php/bioling/article/view/13153},
	doi = {10.5964/bioling.13153},
	abstract = {In a recent manuscript entitled “Modern language models refute Chomsky’s approach to language”, Steven Piantadosi proposes that large language models such as GPT-3 can serve as serious theories of human linguistic cognition. In fact, he maintains that these models are significantly better linguistic theories than proposals emerging from within generative linguistics. The present note explains why this claim is wrong.},
	language = {en},
	urldate = {2024-08-08},
	journal = {Biolinguistics},
	author = {Katzir, Roni},
	month = dec,
	year = {2023},
	keywords = {competence, generalization, generative linguistics, large language models, learning, performance, typology},
	pages = {1--12},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\4L648GBH\\Katzir - 2023 - Why Large Language Models Are Poor Theories of Hum.pdf:application/pdf},
}

@inproceedings{lu_neurologic_2021,
	address = {Online},
	title = {{NeuroLogic} {Decoding}: ({Un})supervised {Neural} {Text} {Generation} with {Predicate} {Logic} {Constraints}},
	shorttitle = {{NeuroLogic} {Decoding}},
	url = {https://aclanthology.org/2021.naacl-main.339},
	doi = {10.18653/v1/2021.naacl-main.339},
	abstract = {Conditional text generation often requires lexical constraints, i.e., which words should or shouldn't be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models – supervised or not – to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.},
	urldate = {2024-08-08},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Lu, Ximing and West, Peter and Zellers, Rowan and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {4288--4299},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\D68TMVSQ\\Lu et al. - 2021 - NeuroLogic Decoding (Un)supervised Neural Text Ge.pdf:application/pdf},
}

@misc{lu_neurologic_2021-1,
	title = {{NeuroLogic} {A}*esque {Decoding}: {Constrained} {Text} {Generation} with {Lookahead} {Heuristics}},
	shorttitle = {{NeuroLogic} {A}*esque {Decoding}},
	url = {http://arxiv.org/abs/2112.08726},
	abstract = {The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A* search algorithm, we propose NEUROLOGIC A esque,1 a decoding algorithm that incorporates heuristic estimates of future cost. We develop efﬁcient lookahead heuristics that are efﬁcient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NEUROLOGIC decoding (Lu et al., 2021), combining its ﬂexibility in incorporating logical constraints with A esque estimates of future constraint satisfaction.},
	language = {en},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Lu, Ximing and Welleck, Sean and West, Peter and Jiang, Liwei and Kasai, Jungo and Khashabi, Daniel and Bras, Ronan Le and Qin, Lianhui and Yu, Youngjae and Zellers, Rowan and Smith, Noah A. and Choi, Yejin},
	month = dec,
	year = {2021},
	note = {arXiv:2112.08726 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Lu et al. - 2021 - NeuroLogic Aesque Decoding Constrained Text Gene.pdf:C\:\\Users\\user\\Zotero\\storage\\Q63RH4NW\\Lu et al. - 2021 - NeuroLogic Aesque Decoding Constrained Text Gene.pdf:application/pdf},
}

@article{maslova_dynamic_2000,
	title = {A dynamic approach to the verification of distributional universals},
	volume = {4},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1613-415X},
	url = {https://www.degruyter.com/document/doi/10.1515/lity.2000.4.3.307/html?lang=en},
	doi = {10.1515/lity.2000.4.3.307},
	abstract = {Article A dynamic approach to the verification of distributional universals was published on January 1, 2000 in the journal Linguistic Typology  (volume 4, issue 3).},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	author = {Maslova, Elena},
	month = jan,
	year = {2000},
	note = {Publisher: De Gruyter Mouton
Section: Linguistic Typology},
	pages = {307--333},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\UYNLUFQ6\\Maslova - 2000 - A dynamic approach to the verification of distribu.pdf:application/pdf},
}

@article{pavlick_semantic_2022,
	title = {Semantic {Structure} in {Deep} {Learning}},
	volume = {8},
	issn = {2333-9683, 2333-9691},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-linguistics-031120-122924},
	doi = {10.1146/annurev-linguistics-031120-122924},
	abstract = {Deep learning has recently come to dominate computational linguistics, leading to claims of human-level performance in a range of language processing tasks. Like much previous computational work, deep learning–based linguistic representations adhere to the distributional meaning-in-use hypothesis, deriving semantic representations from word co-occurrence statistics. However, current deep learning methods entail fundamentally new models of lexical and compositional meaning that are ripe for theoretical analysis. Whereas traditional distributional semantics models take a bottom-up approach in which sentence meaning is characterized by explicit composition functions applied to word meanings, new approaches take a top-down approach in which sentence representations are treated as primary and representations of words and syntax are viewed as emergent. This article summarizes our current understanding of how well such representations capture lexical semantics, world knowledge, and composition. The goal is to foster increased collaboration on testing the implications of such representations as general-purpose models of semantics.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Annual Review of Linguistics},
	author = {Pavlick, Ellie},
	month = jan,
	year = {2022},
	pages = {447--471},
	file = {Pavlick - 2022 - Semantic Structure in Deep Learning.pdf:C\:\\Users\\user\\Zotero\\storage\\GZ5BUPG5\\Pavlick - 2022 - Semantic Structure in Deep Learning.pdf:application/pdf},
}

@misc{baroni_proper_2022,
	title = {On the proper role of linguistically-oriented deep net analysis in linguistic theorizing},
	url = {http://arxiv.org/abs/2106.08694},
	abstract = {A lively research ﬁeld has recently emerged that uses experimental methods to probe the linguistic behavior of modern deep networks. While work in this tradition often reports intriguing results about the grammatical skills of deep nets, it is not clear what their implications for linguistic theorizing should be. As a consequence, linguistically-oriented deep net analysis has had very little impact on linguistics at large. In this chapter, I suggest that deep networks should be treated as theories making explicit predictions about the acceptability of linguistic utterances. I argue that, if we overcome some obstacles standing in the way of seriously pursuing this idea, we will gain a powerful new theoretical tool, complementary to mainstream algebraic approaches.},
	language = {en},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Baroni, Marco},
	month = mar,
	year = {2022},
	note = {arXiv:2106.08694 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Baroni - 2022 - On the proper role of linguistically-oriented deep.pdf:C\:\\Users\\user\\Zotero\\storage\\4CZ8SGJQ\\Baroni - 2022 - On the proper role of linguistically-oriented deep.pdf:application/pdf},
}

@misc{noauthor_large_nodate,
	title = {Large {Linguistic} {Models}: {Analyzing} theoretical linguistic abilities of {LLMs} - lingbuzz/007269},
	url = {https://ling.auf.net/lingbuzz/007269},
	urldate = {2024-08-09},
	file = {Large Linguistic Models\: Analyzing theoretical linguistic abilities of LLMs - lingbuzz/007269:C\:\\Users\\user\\Zotero\\storage\\GNFTS782\\007269.html:text/html},
}

@article{begusa_large_nodate,
	title = {Large {Linguistic} {Models}: {Analyzing} theoretical linguistic abilities of {LLMs}},
	abstract = {The performance of large language models (LLMs) has recently improved to the point where the models can generate valid and coherent meta-linguistic analyses of data. This paper illustrates a vast potential for analyses of the meta-linguistic abilities of large language models. LLMs are primarily trained on language data in the form of text; analyzing their meta-linguistic abilities is informative both for our understanding of the general capabilities of LLMs as well as for models of linguistics. In this paper, we propose several types of experiments and prompt designs that allow us to analyze the ability of GPT-4 to generate meta-linguistic analyses. We focus on three linguistics subfields with formalisms that allow for a detailed analysis of GPT-4’s theoretical capabilities: theoretical syntax, phonology, and semantics. We identify types of experiments, provide general guidelines, discuss limitations, and offer future directions for this research program.},
	language = {en},
	author = {Beguša, Gašper and Dąbkowskia, Maksymilian and Rhodesb, Ryan},
	file = {Beguša et al. - Large Linguistic Models Analyzing theoretical lin.pdf:C\:\\Users\\user\\Zotero\\storage\\5SPPEYH8\\Beguša et al. - Large Linguistic Models Analyzing theoretical lin.pdf:application/pdf},
}

@misc{geiger_causal_2021,
	title = {Causal {Abstractions} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/2106.02997},
	abstract = {Structural analysis methods (e.g., probing and feature attribution) are increasingly important tools for neural network analysis. We propose a new structural analysis method grounded in a formal theory of causal abstraction that provides rich characterizations of model-internal representations and their roles in input/output behavior. In this method, neural representations are aligned with variables in interpretable causal models, and then interchange interventions are used to experimentally verify that the neural representations have the causal properties of their aligned variables. We apply this method in a case study to analyze neural models trained on Multiply Quantiﬁed Natural Language Inference (MQNLI) corpus, a highly complex NLI dataset that was constructed with a tree-structured natural logic causal model. We discover that a BERT-based model with state-of-the-art performance successfully realizes parts of the natural logic model’s causal structure, whereas a simpler baseline model fails to show any such structure, demonstrating that BERT representations encode the compositional structure of MQNLI.},
	language = {en},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Geiger, Atticus and Lu, Hanson and Icard, Thomas and Potts, Christopher},
	month = oct,
	year = {2021},
	note = {arXiv:2106.02997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2021},
	file = {Geiger et al. - 2021 - Causal Abstractions of Neural Networks.pdf:C\:\\Users\\user\\Zotero\\storage\\ZIJ5IEIV\\Geiger et al. - 2021 - Causal Abstractions of Neural Networks.pdf:application/pdf},
}

@article{schrimpf_neural_2021,
	title = {The neural architecture of language: {Integrative} modeling converges on predictive processing},
	volume = {118},
	shorttitle = {The neural architecture of language},
	url = {https://www.pnas.org/doi/10.1073/pnas.2105646118},
	doi = {10.1073/pnas.2105646118},
	abstract = {The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species’ signature cognitive skill. We find that the most powerful “transformer” models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models’ neural fits (“brain score”) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.},
	number = {45},
	urldate = {2024-08-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = nov,
	year = {2021},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2105646118},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\PYXFN6E9\\Schrimpf et al. - 2021 - The neural architecture of language Integrative m.pdf:application/pdf},
}

@inproceedings{geva_transformer_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://aclanthology.org/2022.emnlp-main.3},
	doi = {10.18653/v1/2022.emnlp-main.3},
	abstract = {Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50\%, and for improving computation efficiency with a simple early exit rule, saving 20\% of computation on average.},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {30--45},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\DPMB56JN\\Geva et al. - 2022 - Transformer Feed-Forward Layers Build Predictions .pdf:application/pdf},
}

@misc{geiger_finding_2024,
	title = {Finding {Alignments} {Between} {Interpretable} {Causal} {Variables} and {Distributed} {Neural} {Representations}},
	url = {http://arxiv.org/abs/2303.02536},
	abstract = {Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases—distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS removes previous obstacles to uncovering conceptual structure in trained neural nets.},
	language = {en},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Geiger, Atticus and Wu, Zhengxuan and Potts, Christopher and Icard, Thomas and Goodman, Noah D.},
	month = feb,
	year = {2024},
	note = {arXiv:2303.02536 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {2303.pdf:C\:\\Users\\user\\Zotero\\storage\\NXMAPGBX\\2303.pdf:application/pdf},
}

@article{feder_causalm_2021,
	title = {{CausaLM}: {Causal} {Model} {Explanation} {Through} {Counterfactual} {Language} {Models}},
	issn = {0891-2017, 1530-9312},
	shorttitle = {{CausaLM}},
	url = {https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00404/98518/CausaLM-Causal-Model-Explanation-Through},
	doi = {10.1162/coli_a_00404},
	abstract = {Abstract
            Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning–based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.},
	language = {en},
	urldate = {2024-08-09},
	journal = {Computational Linguistics},
	author = {Feder, Amir and Oved, Nadav and Shalit, Uri and Reichart, Roi},
	month = may,
	year = {2021},
	pages = {1--54},
	file = {Feder et al. - 2021 - CausaLM Causal Model Explanation Through Counterf.pdf:C\:\\Users\\user\\Zotero\\storage\\PQPPGVT4\\Feder et al. - 2021 - CausaLM Causal Model Explanation Through Counterf.pdf:application/pdf},
}

@inproceedings{geiger_inducing_2022,
	title = {Inducing {Causal} {Structure} for {Interpretable} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v162/geiger22a.html},
	abstract = {In many areas, we have well-founded insights about causal structure that would be useful to bring into our trained models while still allowing them to learn in a data-driven fashion. To achieve this, we present the new method of interchange intervention training (IIT). In IIT, we (1) align variables in a causal model (e.g., a deterministic program or Bayesian network) with representations in a neural model and (2) train the neural model to match the counterfactual behavior of the causal model on a base input when aligned representations in both models are set to be the value they would be for a source input. IIT is fully differentiable, flexibly combines with other objectives, and guarantees that the target causal model is a causal abstraction of the neural model when its loss is zero. We evaluate IIT on a structural vision task (MNIST-PVR), a navigational language task (ReaSCAN), and a natural language inference task (MQNLI). We compare IIT against multi-task training objectives and data augmentation. In all our experiments, IIT achieves the best results and produces neural models that are more interpretable in the sense that they more successfully realize the target causal model.},
	language = {en},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Geiger, Atticus and Wu, Zhengxuan and Lu, Hanson and Rozner, Josh and Kreiss, Elisa and Icard, Thomas and Goodman, Noah and Potts, Christopher},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {7324--7338},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\CJA5HJRL\\Geiger et al. - 2022 - Inducing Causal Structure for Interpretable Neural.pdf:application/pdf},
}

@article{tuckute_language_2024,
	title = {Language in {Brains}, {Minds}, and {Machines}},
	volume = {47},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-neuro-120623-101142},
	doi = {10.1146/annurev-neuro-120623-101142},
	abstract = {It has long been argued that only humans could produce and understand language. But now, for the first time, artificial language models (LMs) achieve this feat. Here we survey the new purchase LMs are providing on the question of how language is implemented in the brain. We discuss why, a priori, LMs might be expected to share similarities with the human language system. We then summarize evidence that LMs represent linguistic information similarly enough to humans to enable relatively accurate brain encoding and decoding during language processing. Finally, we examine which LM properties—their architecture, task performance, or training—are critical for capturing human neural responses to language and review studies using LMs as in silico model organisms for testing hypotheses about language. These ongoing investigations bring us closer to understanding the representations and processes that underlie our ability to comprehend sentences and express thoughts in language.},
	language = {en},
	number = {1},
	urldate = {2024-08-12},
	journal = {Annual Review of Neuroscience},
	author = {Tuckute, Greta and Kanwisher, Nancy and Fedorenko, Evelina},
	month = aug,
	year = {2024},
	pages = {277--301},
	file = {Tuckute et al. - 2024 - Language in Brains, Minds, and Machines.pdf:C\:\\Users\\user\\Zotero\\storage\\3EW2SHTF\\Tuckute et al. - 2024 - Language in Brains, Minds, and Machines.pdf:application/pdf},
}

@misc{noauthor_neural_nodate,
	title = {The neural architecture of language: {Integrative} modeling converges on predictive processing},
	shorttitle = {The neural architecture of language},
	url = {https://www.pnas.org/doi/epub/10.1073/pnas.2105646118},
	language = {en},
	urldate = {2024-08-12},
	doi = {10.1073/pnas.2105646118},
	file = {Full text:C\:\\Users\\user\\Zotero\\storage\\GY282B4T\\The neural architecture of language Integrative m.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\MEXCLWG8\\pnas.html:text/html},
}

@misc{tuckute_35-minute-long_2024,
	title = {A 3.5-minute-long reading-based {fMRI} localizer for the language network},
	url = {http://biorxiv.org/lookup/doi/10.1101/2024.07.02.601683},
	doi = {10.1101/2024.07.02.601683},
	abstract = {The field of human cognitive neuroscience is increasingly acknowledging inter-individual differences in the precise locations of functional areas and the corresponding need for individuallevel analyses in fMRI studies. One approach to identifying functional areas and networks within individual brains is based on robust and extensively validated ‘localizer’ paradigms—contrasts of conditions that aim to isolate some mental process of interest. Here, we present a new version of a localizer for the fronto-temporal language-selective network. This localizer is similar to a commonly-used localizer based on the reading of sentences and nonword sequences (Fedorenko et al., 2010) but uses speeded presentation (200ms per word/nonword). Based on a direct comparison between the standard version (450ms per word/nonword) and the speeded versions of the language localizer in 24 participants, we show that a single run of the speeded localizer (3.5 min) is highly effective at identifying the language-selective areas: indeed, it is more effective than the standard localizer given that it leads to an increased response to the critical (sentence) condition and a decreased response to the control (nonwords) condition. This localizer may therefore become the version of choice for identifying the language network in neurotypical adults or special populations (as long as they are proficient readers), especially when time is of essence.},
	language = {en},
	urldate = {2024-08-12},
	author = {Tuckute, Greta and Lee, Elizabeth Jiachen and Sathe, Aalok and Fedorenko, Evelina},
	month = jul,
	year = {2024},
	file = {Tuckute et al. - 2024 - A 3.5-minute-long reading-based fMRI localizer for.pdf:C\:\\Users\\user\\Zotero\\storage\\DYECHEIG\\Tuckute et al. - 2024 - A 3.5-minute-long reading-based fMRI localizer for.pdf:application/pdf},
}

@misc{li_information-theoretic_2024,
	title = {An information-theoretic model of shallow and deep language comprehension},
	url = {http://arxiv.org/abs/2405.08223},
	abstract = {A large body of work in psycholinguistics has focused on the idea that online language comprehension can be shallow or ‘good enough’: given constraints on time or available computation, comprehenders may form interpretations of their input that are plausible but inaccurate. However, this idea has not yet been linked with formal theories of computation under resource constraints. Here we use information theory to formulate a model of language comprehension as an optimal tradeoff between accuracy and processing depth, formalized as bits of information extracted from the input, which increases with processing time. The model provides a measure of processing effort as the change in processing depth, which we link to EEG signals and reading times. We validate our theory against a large-scale dataset of garden path sentence reading times, and EEG experiments featuring N400, P600 and biphasic ERP effects. By quantifying the timecourse of language processing as it proceeds from shallow to deep, our model provides a unified framework to explain behavioral and neural signatures of language comprehension.},
	language = {en},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Li, Jiaxuan and Futrell, Richard},
	month = may,
	year = {2024},
	note = {arXiv:2405.08223 [cs, math]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Theory},
	annote = {Comment: 6 pages; accepted to COGSCI 2024},
	file = {Li e Futrell - 2024 - An information-theoretic model of shallow and deep.pdf:C\:\\Users\\user\\Zotero\\storage\\HXJXWHIG\\Li e Futrell - 2024 - An information-theoretic model of shallow and deep.pdf:application/pdf},
}

@article{smolensky_proper_1988,
	title = {On the proper treatment of connectionism},
	abstract = {A set of hypotheses i{\textasciitilde}' formulated for a connectionist approach to cognitive modeling. These hypotheses are shown to be incompatible with the hypotheses underlying traditional cognitive models. The connectionist models considered are massively parallel numerical computational systems that are a kind of continuous dynamical system. The numerical variables in the system correspond semantically to fine-grained features below the level of the concepts cons{\textless}.:iously used to describe the task domain. The level of analysis is intermediate between those of symbolic cognitive models and neural models. The explanations of behavior provided are like those traditional in the physical sciences, unlike the explanations provided by symbolic models.},
	language = {en},
	journal = {BEHAVIORAL AND BRAIN SCIENCES},
	author = {Smolensky, Paul},
	year = {1988},
	file = {Smolensky - 1988 - On the proper treatment of connectionism.pdf:C\:\\Users\\user\\Zotero\\storage\\FSQHZES7\\Smolensky - 1988 - On the proper treatment of connectionism.pdf:application/pdf},
}

@article{fox_large_2024,
	title = {Large {Language} {Models} and theoretical linguistics},
	volume = {50},
	issn = {0301-4428, 1613-4060},
	url = {https://www.degruyter.com/document/doi/10.1515/tl-2024-2005/html},
	doi = {10.1515/tl-2024-2005},
	abstract = {Some recent publications have made the suggestion that Large Language Models are not just successful engineering tools but also good theories of human linguistic cognition. This note reviews methodological and empirical reasons to reject this suggestion out of hand.},
	language = {en},
	number = {1-2},
	urldate = {2024-08-13},
	journal = {Theoretical Linguistics},
	author = {Fox, Danny and Katzir, Roni},
	month = jun,
	year = {2024},
	pages = {71--76},
	file = {Fox e Katzir - 2024 - Large Language Models and theoretical linguistics.pdf:C\:\\Users\\user\\Zotero\\storage\\TQJIUCIF\\Fox e Katzir - 2024 - Large Language Models and theoretical linguistics.pdf:application/pdf},
}

@article{aschenbrenner_situational_nodate,
	title = {Situational {Awareness}},
	language = {en},
	author = {Aschenbrenner, Leopold},
	file = {Aschenbrenner - Situational Awareness.pdf:C\:\\Users\\user\\Zotero\\storage\\GZUDEVNV\\Aschenbrenner - Situational Awareness.pdf:application/pdf},
}

@misc{widdows_quantum_2024,
	title = {Quantum {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2403.19758},
	abstract = {Language processing is at the heart of current developments in artificial intelligence, and quantum computers are becoming available at the same time. This has led to great interest in quantum natural language processing, and several early proposals and experiments. This paper surveys the state of this area, showing how NLP-related techniques have been used in quantum language processing. We examine the art of word embeddings and sequential models, proposing some avenues for future investigation and discussing the tradeoffs present in these directions. We also highlight some recent methods to compute attention in transformer models, and perform grammatical parsing. We also introduce a new quantum design for the basic task of text encoding (representing a string of characters in memory), which has not been addressed in detail before. Quantum theory has contributed toward quantifying uncertainty and explaining “What is intelligence?” In this context, we argue that “hallucinations” in modern artificial intelligence systems are a misunderstanding of the way facts are conceptualized: language can express many plausible hypotheses, of which only a few become actual.},
	language = {en},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Widdows, Dominic and Aboumrad, Willie and Kim, Dohun and Ray, Sayonee and Mei, Jonathan},
	month = apr,
	year = {2024},
	note = {arXiv:2403.19758 [quant-ph]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Quantum Physics},
	file = {Widdows et al. - 2024 - Quantum Natural Language Processing.pdf:C\:\\Users\\user\\Zotero\\storage\\ZZC72X24\\Widdows et al. - 2024 - Quantum Natural Language Processing.pdf:application/pdf},
}

@article{schuld_introduction_2015,
	title = {An introduction to quantum machine learning},
	volume = {56},
	issn = {0010-7514, 1366-5812},
	url = {http://arxiv.org/abs/1409.3097},
	doi = {10.1080/00107514.2014.964942},
	abstract = {Machine learning algorithms learn a desired input-output relation from examples in order to interpret new inputs. This is important for tasks such as image and speech recognition or strategy optimisation, with growing applications in the IT industry. In the last couple of years, researchers investigated if quantum computing can help to improve classical machine learning algorithms. Ideas range from running computationally costly algorithms or their subroutines eﬃciently on a quantum computer to the translation of stochastic methods into the language of quantum theory. This contribution gives a systematic overview of the emerging ﬁeld of quantum machine learning. It presents the approaches as well as technical details in an accessable way, and discusses the potential of a future theory of quantum learning.},
	language = {en},
	number = {2},
	urldate = {2024-08-16},
	journal = {Contemporary Physics},
	author = {Schuld, M. and Sinayskiy, I. and Petruccione, F.},
	month = apr,
	year = {2015},
	note = {arXiv:1409.3097 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {172--185},
	file = {Schuld et al. - 2015 - An introduction to quantum machine learning.pdf:C\:\\Users\\user\\Zotero\\storage\\WCTVIIH8\\Schuld et al. - 2015 - An introduction to quantum machine learning.pdf:application/pdf},
}

@misc{coecke_mathematical_2010,
	title = {Mathematical {Foundations} for a {Compositional} {Distributional} {Model} of {Meaning}},
	url = {http://arxiv.org/abs/1003.4394},
	abstract = {We propose a mathematical framework for a uniﬁcation of the distributional theory of meaning in terms of vector space models, and a compositional theory for grammatical types, for which we rely on the algebra of Pregroups, introduced by Lambek. This mathematical framework enables us to compute the meaning of a well-typed sentence from the meanings of its constituents. Concretely, the type reductions of Pregroups are ‘lifted’ to morphisms in a category, a procedure that transforms meanings of constituents into a meaning of the (well-typed) whole. Importantly, meanings of whole sentences live in a single space, independent of the grammatical structure of the sentence. Hence the inner-product can be used to compare meanings of arbitrary sentences, as it is for comparing the meanings of words in the distributional model. The mathematical structure we employ admits a purely diagrammatic calculus which exposes how the information ﬂows between the words in a sentence in order to make up the meaning of the whole sentence. A variation of our ‘categorical model’ which involves constraining the scalars of the vector spaces to the semiring of Booleans results in a Montague-style Boolean-valued semantics.},
	language = {en},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Coecke, Bob and Sadrzadeh, Mehrnoosh and Clark, Stephen},
	month = mar,
	year = {2010},
	note = {arXiv:1003.4394 [cs, math]},
	keywords = {Computer Science - Computation and Language, Computer Science - Logic in Computer Science, Mathematics - Category Theory},
	file = {Coecke et al. - 2010 - Mathematical Foundations for a Compositional Distr.pdf:C\:\\Users\\user\\Zotero\\storage\\N8PIA46C\\Coecke et al. - 2010 - Mathematical Foundations for a Compositional Distr.pdf:application/pdf},
}

@incollection{blackburn_mathematical_2003,
	address = {Berlin, Heidelberg},
	title = {A {Mathematical} {Model} for {Context} and {Word}-{Meaning}},
	volume = {2680},
	isbn = {978-3-540-40380-7},
	url = {http://link.springer.com/10.1007/3-540-44958-2_29},
	abstract = {Context is vital for deciding which of the possible senses of a word is being used in a particular situation, a task known as disambiguation. Motivated by a survey of disambiguation techniques in natural language processing, this paper presents a mathematical model describing the relationship between words, meanings and contexts, giving examples of how context-groups can be used to distinguish diﬀerent senses of ambiguous words. Many aspects of this model have interesting similarities with quantum theory.},
	language = {en},
	urldate = {2024-08-16},
	booktitle = {Modeling and {Using} {Context}},
	publisher = {Springer Berlin Heidelberg},
	author = {Widdows, Dominic},
	editor = {Blackburn, Patrick and Ghidini, Chiara and Turner, Roy M. and Giunchiglia, Fausto},
	year = {2003},
	doi = {10.1007/3-540-44958-2_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {369--382},
	file = {Widdows - 2003 - A Mathematical Model for Context and Word-Meaning.pdf:C\:\\Users\\user\\Zotero\\storage\\I9X5WJSW\\Widdows - 2003 - A Mathematical Model for Context and Word-Meaning.pdf:application/pdf},
}

@book{busemeyer_quantum_2012,
	address = {Cambridge},
	title = {Quantum models of cognition and decision},
	isbn = {978-1-107-01199-1},
	abstract = {"Much of our understanding of human thinking is based on probabilistic models. This innovative book by Jerome R. Busemeyer and Peter D. Bruza argues that, actually, the underlying mathematical structures from quantum theory provide a much better account of human thinking than traditional models. They introduce the foundations for modeling probabilistic-dynamic systems using two aspects of quantum theory. The first, 'contextuality', is a way to understand interference effects found with inferences and decisions under conditions of uncertainty. The second, 'quantum entanglement', allows cognitive phenomena to be modeled in a non-reductionist way. Employing these principles drawn from quantum theory allows us to view human cognition and decision in a totally new light. Introducing the basic principles in an easy-to-follow way, this book does not assume a physics background or a quantum brain and comes complete with a tutorial and fully worked out applications in important areas of cognition and decision"--},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Busemeyer, Jerome R. and Bruza, Peter David},
	year = {2012},
	keywords = {Cognition, Decision making, Mathematical models, PSYCHOLOGY / Cognitive Psychology, Quantum theory, Statistical decision},
	file = {Busemeyer e Bruza - 2012 - Quantum models of cognition and decision.pdf:C\:\\Users\\user\\Zotero\\storage\\6GRA6JXH\\Busemeyer e Bruza - 2012 - Quantum models of cognition and decision.pdf:application/pdf},
}

@misc{pang_iterative_2024,
	title = {Iterative {Reasoning} {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2404.19733},
	abstract = {Iterative preference optimization methods have recently been shown to perform well for general instruction tuning tasks, but typically make little improvement on reasoning tasks [Yuan et al., 2024, Chen et al., 2024]. In this work we develop an iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps. We train using a modified DPO loss [Rafailov et al., 2023] with an additional negative log-likelihood term, which we find to be crucial. We show reasoning improves across repeated iterations of this scheme. While only relying on examples in the training set, our approach results in increasing accuracy on GSM8K, MATH, and ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based models not relying on additionally sourced datasets. For example, we see a large improvement from 55.6\% to 81.6\% on GSM8K and an accuracy of 88.7\% with majority voting out of 32 samples.},
	language = {en},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Pang, Richard Yuanzhe and Yuan, Weizhe and Cho, Kyunghyun and He, He and Sukhbaatar, Sainbayar and Weston, Jason},
	month = jun,
	year = {2024},
	note = {arXiv:2404.19733 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {2404.pdf:C\:\\Users\\user\\Zotero\\storage\\2JJ7NVV2\\2404.pdf:application/pdf},
}

@misc{noauthor_information-theoretic_nodate,
	title = {Information-theoretic principles in incremental language production},
	url = {https://www.pnas.org/doi/epub/10.1073/pnas.2220593120},
	language = {en},
	urldate = {2024-08-17},
	doi = {10.1073/pnas.2220593120},
	file = {Full text:C\:\\Users\\user\\Zotero\\storage\\75LRYQSA\\Information-theoretic principles in incremental la.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\JPLE2FII\\pnas.html:text/html},
}

@misc{merullo_language_2024,
	title = {Language {Models} {Implement} {Simple} {Word2Vec}-style {Vector} {Arithmetic}},
	url = {http://arxiv.org/abs/2305.16130},
	abstract = {A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple vector arithmetic style mechanism to solve some relational tasks using regularities encoded in the hidden space of the model (e.g., Poland:Warsaw::China:Beijing). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, uppercasing, and past-tensing) a key part of the mechanism reduces to a simple additive update typically applied by the feedforward (FFN) networks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a growing body of work on the interpretability of LMs, and offer reason to be optimistic that, despite the massive and non-linear nature of the models, the strategies they ultimately use to solve tasks can sometimes reduce to familiar and even intuitive algorithms.},
	language = {en},
	urldate = {2024-08-17},
	publisher = {arXiv},
	author = {Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
	month = apr,
	year = {2024},
	note = {arXiv:2305.16130 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: NAACL},
	file = {Merullo et al. - 2024 - Language Models Implement Simple Word2Vec-style Ve.pdf:C\:\\Users\\user\\Zotero\\storage\\LTVJCQWV\\Merullo et al. - 2024 - Language Models Implement Simple Word2Vec-style Ve.pdf:application/pdf},
}

@inproceedings{dufter_static_2021,
	address = {Online},
	title = {Static {Embeddings} as {Efficient} {Knowledge} {Bases}?},
	url = {https://aclanthology.org/2021.naacl-main.186},
	doi = {10.18653/v1/2021.naacl-main.186},
	abstract = {Recent research investigates factual knowledge stored in large pretrained language models (PLMs). Instead of structural knowledge base (KB) queries, masked sentences such as “Paris is the capital of [MASK]” are used as probes. The good performance on this analysis task has been interpreted as PLMs becoming potential repositories of factual knowledge. In experiments across ten linguistically diverse languages, we study knowledge contained in static embeddings. We show that, when restricting the output space to a candidate set, simple nearest neighbor matching using static embeddings performs better than PLMs. E.g., static embeddings perform 1.6\% points better than BERT while just using 0.3\% of energy for training. One important factor in their good comparative performance is that static embeddings are standardly learned for a large vocabulary. In contrast, BERT exploits its more sophisticated, but expensive ability to compose meaningful representations from a much smaller subword vocabulary.},
	urldate = {2024-08-17},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Dufter, Philipp and Kassner, Nora and Schütze, Hinrich},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {2353--2363},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\UTSCWR4V\\Dufter et al. - 2021 - Static Embeddings as Efficient Knowledge Bases.pdf:application/pdf},
}

@inproceedings{jung_maieutic_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Maieutic {Prompting}: {Logically} {Consistent} {Reasoning} with {Recursive} {Explanations}},
	shorttitle = {Maieutic {Prompting}},
	url = {https://aclanthology.org/2022.emnlp-main.82},
	doi = {10.18653/v1/2022.emnlp-main.82},
	abstract = {Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20\% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.},
	urldate = {2024-08-19},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman, Faeze and Bhagavatula, Chandra and Le Bras, Ronan and Choi, Yejin},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {1266--1279},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\DZVHT38Y\\Jung et al. - 2022 - Maieutic Prompting Logically Consistent Reasoning.pdf:application/pdf},
}

@inproceedings{kang_impact_2023,
	address = {Singapore},
	title = {Impact of {Co}-occurrence on {Factual} {Knowledge} of {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.findings-emnlp.518},
	doi = {10.18653/v1/2023.findings-emnlp.518},
	abstract = {Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at https://github.com/CheongWoong/impact\_of\_cooccurrence.},
	urldate = {2024-08-19},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Kang, Cheongwoong and Choi, Jaesik},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {7721--7735},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\GFTQIEDH\\Kang e Choi - 2023 - Impact of Co-occurrence on Factual Knowledge of La.pdf:application/pdf},
}

@article{ouyang_training_nodate,
	title = {Training language models to follow instructions with human feedback},
	abstract = {Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	language = {en},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	file = {Ouyang et al. - Training language models to follow instructions wi.pdf:C\:\\Users\\user\\Zotero\\storage\\ZCP75LVX\\Ouyang et al. - Training language models to follow instructions wi.pdf:application/pdf},
}

@inproceedings{blevins_prompting_2023,
	address = {Toronto, Canada},
	title = {Prompting {Language} {Models} for {Linguistic} {Structure}},
	url = {https://aclanthology.org/2023.acl-long.367},
	doi = {10.18653/v1/2023.acl-long.367},
	abstract = {Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.},
	urldate = {2024-08-19},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Blevins, Terra and Gonen, Hila and Zettlemoyer, Luke},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {6649--6663},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\I4C38X7H\\Blevins et al. - 2023 - Prompting Language Models for Linguistic Structure.pdf:application/pdf},
}

@inproceedings{li_probing_2022,
	address = {Seattle, United States},
	title = {Probing via {Prompting}},
	url = {https://aclanthology.org/2022.naacl-main.84},
	doi = {10.18653/v1/2022.naacl-main.84},
	abstract = {Probing is a popular approach to understand what linguistic information is contained in the representations of pre-trained language models. However, the mechanism of selecting the probe model has recently been subject to intense debate, as it is not clear if the probes are merely extracting information or modelling the linguistic property themselves. To address this challenge, this paper introduces a novel model-free approach to probing via prompting, which formulates probing as a prompting task. We conduct experiments on five probing tasks and show that PP is comparable or better at extracting information than diagnostic probes while learning much less on its own. We further combine the probing via prompting approach with pruning to analyze where the model stores the linguistic information in its architecture. Finally, we apply the probing via prompting approach to examine the usefulness of a linguistic property for pre-training by removing the heads that are essential to it and evaluating the resulting model's performance on language modeling.},
	urldate = {2024-08-19},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Jiaoda and Cotterell, Ryan and Sachan, Mrinmaya},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {1144--1157},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\BC2DZ6CR\\Li et al. - 2022 - Probing via Prompting.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-08-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\FM3MD9K4\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@misc{misra_minicons_2022,
	title = {minicons: {Enabling} {Flexible} {Behavioral} and {Representational} {Analyses} of {Transformer} {Language} {Models}},
	shorttitle = {minicons},
	url = {http://arxiv.org/abs/2203.13112},
	abstract = {We present minicons, an open source library that provides a standard API for researchers interested in conducting behavioral and representational analyses of transformer-based language models (LMs). Speciﬁcally, minicons enables researchers to apply analysis methods at two levels: (1) at the prediction level—by providing functions to efﬁciently extract word/sentence level probabilities; and (2) at the representational level—by also facilitating efﬁcient extraction of word/phrase level vectors from one or more layers. In this paper, we describe the library and apply it to two motivating case studies: One focusing on the learning dynamics of the BERT architecture on relative grammatical judgments, and the other on benchmarking 23 different LMs on zero-shot abductive reasoning. minicons is available at https://github.com/ kanishkamisra/minicons.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Misra, Kanishka},
	month = mar,
	year = {2022},
	note = {arXiv:2203.13112 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To be submitted; Code to reproduce experiments can be found on https://github.com/kanishkamisra/minicons-experiments},
	file = {Misra - 2022 - minicons Enabling Flexible Behavioral and Represe.pdf:C\:\\Users\\user\\Zotero\\storage\\B2RRW3RH\\Misra - 2022 - minicons Enabling Flexible Behavioral and Represe.pdf:application/pdf},
}

@misc{borzunov_petals_2023-1,
	title = {Petals: {Collaborative} {Inference} and {Fine}-tuning of {Large} {Models}},
	shorttitle = {Petals},
	url = {http://arxiv.org/abs/2209.01188},
	abstract = {Many NLP tasks beneﬁt from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires highend hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM ofﬂoading or hosted APIs. However, these techniques have innate limitations: ofﬂoading is too slow for interactive inference, while APIs are not ﬂexible enough for research that requires access to weights, attention or logits. In this work, we propose PETALS1 — a system for inference and ﬁne-tuning of large models collaboratively by joining the resources of multiple parties. We demonstrate that this strategy outperforms ofﬂoading for very large models, running inference of BLOOM-176B on consumer GPUs with ≈ 1 step per second, which is enough for many interactive LLM applications. Unlike most inference APIs, PETALS also natively exposes hidden states of served models, allowing to train and share custom model extensions based on efﬁcient ﬁne-tuning methods.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and Ryabinin, Max and Belkada, Younes and Chumachenko, Artem and Samygin, Pavel and Raffel, Colin},
	month = mar,
	year = {2023},
	note = {arXiv:2209.01188 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: 10 pages, 4 figures. The version 2 updates the benchmarks and the description of the chat application. Source code and docs: https://petals.ml},
	file = {2209.pdf:C\:\\Users\\user\\Zotero\\storage\\3PL7AYZI\\2209.pdf:application/pdf},
}

@misc{yin_interpreting_2022-1,
	title = {Interpreting {Language} {Models} with {Contrastive} {Explanations}},
	url = {http://arxiv.org/abs/2202.10419},
	abstract = {Model interpretability methods are often used to explain NLP model decisions on tasks such as text classiﬁcation, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics. Existing explanation methods conﬂate evidence for all these features into a single explanation, which is less interpretable for human understanding.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Yin, Kayo and Neubig, Graham},
	month = may,
	year = {2022},
	note = {arXiv:2202.10419 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Yin e Neubig - 2022 - Interpreting Language Models with Contrastive Expl.pdf:C\:\\Users\\user\\Zotero\\storage\\L2JF4MI8\\Yin e Neubig - 2022 - Interpreting Language Models with Contrastive Expl.pdf:application/pdf},
}

@inproceedings{sarti_inseq_2023-1,
	title = {Inseq: {An} {Interpretability} {Toolkit} for {Sequence} {Generation} {Models}},
	shorttitle = {Inseq},
	url = {http://arxiv.org/abs/2302.13942},
	doi = {10.18653/v1/2023.acl-demo.40},
	abstract = {Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq1, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models’ internal information and feature importance scores for popular decoderonly and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.},
	language = {en},
	urldate = {2024-08-20},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 3: {System} {Demonstrations})},
	author = {Sarti, Gabriele and Feldhus, Nils and Sickert, Ludwig and van der Wal, Oskar and Nissim, Malvina and Bisazza, Arianna},
	year = {2023},
	note = {arXiv:2302.13942 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Human-Computer Interaction},
	pages = {421--435},
	annote = {Comment: ACL 2023 Demo Track. Library: https://github.com/inseq-team/inseq, Docs: https://inseq.readthedocs.io, v0.4},
	file = {Sarti et al. - 2023 - Inseq An Interpretability Toolkit for Sequence Ge.pdf:C\:\\Users\\user\\Zotero\\storage\\FG6X2JQS\\Sarti et al. - 2023 - Inseq An Interpretability Toolkit for Sequence Ge.pdf:application/pdf},
}

@misc{wu_pyvene_2024,
	title = {pyvene: {A} {Library} for {Understanding} and {Improving} {PyTorch} {Models} via {Interventions}},
	shorttitle = {pyvene},
	url = {http://arxiv.org/abs/2403.07809},
	abstract = {Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce pyvene, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. pyvene supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how pyvene provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Wu, Zhengxuan and Geiger, Atticus and Arora, Aryaman and Huang, Jing and Wang, Zheng and Goodman, Noah D. and Manning, Christopher D. and Potts, Christopher},
	month = mar,
	year = {2024},
	note = {arXiv:2403.07809 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 8 pages, 3 figures},
	file = {Wu et al. - 2024 - pyvene A Library for Understanding and Improving .pdf:C\:\\Users\\user\\Zotero\\storage\\NCW98RXJ\\Wu et al. - 2024 - pyvene A Library for Understanding and Improving .pdf:application/pdf},
}

@inproceedings{alammar_ecco_2021,
	address = {Online},
	title = {Ecco: {An} {Open} {Source} {Library} for the {Explainability} of {Transformer} {Language} {Models}},
	shorttitle = {Ecco},
	url = {https://aclanthology.org/2021.acl-demo.30},
	doi = {10.18653/v1/2021.acl-demo.30},
	abstract = {Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the transparency of Transformer-based language models, we present Ecco – an open-source library for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and interactively explore the inner mechanics of these models. This includes (1) gradient-based feature attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and examination tools for neuron activations in the under-explored Feed-Forward Neural Network sublayer of Transformer layers. (4) convenient examination of activation vectors via canonical correlation analysis (CCA), non-negative matrix factorization (NMF), and probing classifiers. We find that syntactic information can be retrieved from BERT's FFNN representations in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntactic information. Ecco is available at https://www.eccox.io/},
	urldate = {2024-08-20},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Alammar, J},
	editor = {Ji, Heng and Park, Jong C. and Xia, Rui},
	month = aug,
	year = {2021},
	pages = {249--257},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\2HFFLU3X\\Alammar - 2021 - Ecco An Open Source Library for the Explainabilit.pdf:application/pdf},
}

@inproceedings{sinha_language_2023,
	address = {Toronto, Canada},
	title = {Language model acceptability judgements are not always robust to context},
	url = {https://aclanthology.org/2023.acl-long.333},
	doi = {10.18653/v1/2023.acl-long.333},
	abstract = {Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Our best syntactic evaluation datasets, however, provide substantially less linguistic context than models receive during pretraining. This mismatch raises an important question: how robust are models' syntactic judgements across different contexts? In this paper, we vary the input contexts based on: length, the types of syntactic phenomena it contains, and whether or not there are grammatical violations. We find that model judgements are generally robust when placed in randomly sampled linguistic contexts, but are unstable when contexts match the test stimuli in syntactic structure. Among all tested models (GPT-2 and five variants of OPT), we find that model performance is affected when we provided contexts with matching syntactic structure: performance significantly improves when contexts are acceptable, and it significantly declines when they are unacceptable. This effect is amplified by the length of the context, except for unrelated inputs. We show that these changes in model performance are not explainable by acceptability-preserving syntactic perturbations. This sensitivity to highly specific syntactic features of the context can only be explained by the models' implicit in-context learning abilities.},
	urldate = {2024-08-20},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sinha, Koustuv and Gauthier, Jon and Mueller, Aaron and Misra, Kanishka and Fuentes, Keren and Levy, Roger and Williams, Adina},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {6043--6063},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\N4AH2WY7\\Sinha et al. - 2023 - Language model acceptability judgements are not al.pdf:application/pdf},
}

@article{paczynski_electrophysiological_2011,
	title = {Electrophysiological {Evidence} for {Use} of the {Animacy} {Hierarchy}, but not {Thematic} {Role} {Assignment}, {During} {Verb} {Argument} {Processing}},
	volume = {26},
	issn = {0169-0965},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3244078/},
	doi = {10.1080/01690965.2011.580143},
	abstract = {Animacy is known to play an important role in language processing and production, but debate remains as to how it exerts its effects: 1) through links to syntactic ordering, 2) through inherent differences between animate and inanimate entities in their salience/lexico-semantic accessibility, 3) through links to specific thematic roles. We contrasted these three accounts in two event related potential (ERP) experiments examining the processing of direct object arguments in simple English sentences. In Experiment 1, we found a larger N400 to animate than inanimate direct object arguments assigned the Patient role, ruling out the second account. In Experiment 2 we found no difference in the N400 evoked by animate direct object arguments assigned the Patient role (prototypically inanimate) and those assigned the Experiencer role (prototypically animate), ruling out the third account. We therefore suggest that animacy may impact processing through a direct link to syntactic linear ordering, at least on post-verbal arguments in English. We also examined processing on direct object arguments that violated the animacy-based selection restriction constraints of their preceding verbs. These violations evoked a robust P600, which was not modulated by thematic role assignment or reversibility, suggesting that the so-called semantic P600 is driven by overall propositional impossibility, rather than thematic role reanalysis.},
	number = {9},
	urldate = {2024-08-20},
	journal = {Language and cognitive processes},
	author = {Paczynski, Martin and Kuperberg, Gina R.},
	year = {2011},
	pmid = {22199415},
	pmcid = {PMC3244078},
	pages = {1402--1456},
	file = {PubMed Central Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\UMZYHL4I\\Paczynski e Kuperberg - 2011 - Electrophysiological Evidence for Use of the Anima.pdf:application/pdf},
}

@article{seyednozadi_functional_2021,
	title = {Functional {Role} of the {N400} and {P600} in {Language}-{Related} {ERP} {Studies} with {Respect} to {Semantic} {Anomalies}: {An} {Overview}},
	volume = {58},
	issn = {1300-0667},
	shorttitle = {Functional {Role} of the {N400} and {P600} in {Language}-{Related} {ERP} {Studies} with {Respect} to {Semantic} {Anomalies}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8419728/},
	doi = {10.29399/npa.27422},
	abstract = {In this study, the language-related ERP studies relevant to the functional role of the N400 and P600 in semantically anomalous sentences and the underlying reasons which may affect their functions were reviewed. Since their discovery, the N400 and P600 have been the most important language-related ERP components. The N400 has been mostly elicited as a result of processing sentences with lexical and semantic anomalies, but later on, in many studies instead of the expected lexical-semantic N400 effect, semantic anomalies elicited a P600 effect called semantic P600. However, the functional interpretation of these two ERP components has constantly been a matter of debate. Perhaps most notably, it is proposed that it is not just the N400 which is related to semantic anomalies but the P600 can also be reflected as a result of these kinds of anomalies. Reviewing the literature for explaining the functions of the two ERP components, the N400 and the P600, during the processing of semantic anomalies revealed that still there is a need for more research on language processing in order to make the researchers capable of describing the underlying factors influencing them, especially more focused investigations of the functional-anatomical and neurocomputational models may provide a clearer understanding of them. Moreover, any practical theory or model of the N400 and the P600 in language comprehension needs to consider the apparent inconsistencies in the elicitation pattern of the N400 and the P600 in order to successfully capture the full data spectrum.},
	number = {3},
	urldate = {2024-08-20},
	journal = {Archives of Neuropsychiatry},
	author = {SEYEDNOZADI, Zahra and PISHGHADAM, Reza and PISHGHADAM, Morteza},
	month = jan,
	year = {2021},
	pmid = {34526850},
	pmcid = {PMC8419728},
	pages = {249--252},
	file = {PubMed Central Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\3N9N97TJ\\SEYEDNOZADI et al. - 2021 - Functional Role of the N400 and P600 in Language-R.pdf:application/pdf},
}

@misc{milliere_language_2024,
	title = {Language {Models} as {Models} of {Language}},
	url = {http://arxiv.org/abs/2408.07144},
	abstract = {This chapter critically examines the potential contributions of modern language models to theoretical linguistics. Despite their focus on engineering goals, these models’ ability to acquire sophisticated linguistic knowledge from mere exposure to data warrants a careful reassessment of their relevance to linguistic theory. I review a growing body of empirical evidence suggesting that language models can learn hierarchical syntactic structure and exhibit sensitivity to various linguistic phenomena, even when trained on developmentally plausible amounts of data. While the competence/performance distinction has been invoked to dismiss the relevance of such models to linguistic theory, I argue that this assessment may be premature. By carefully controlling learning conditions and making use of causal intervention methods, experiments with language models can potentially constrain hypotheses about language acquisition and competence. I conclude that closer collaboration between theoretical linguists and computational researchers could yield valuable insights, particularly in advancing debates about linguistic nativism.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Millière, Raphaël},
	month = aug,
	year = {2024},
	note = {arXiv:2408.07144 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Forthcoming in Nefdt, R., Dupre, G., {\textbackslash}\& Stanton, K. (eds.), The Oxford Handbook of the Philosophy of Linguistics. Oxford University Press},
	file = {2408.pdf:C\:\\Users\\user\\Zotero\\storage\\WJE5MFUR\\2408.pdf:application/pdf},
}

@misc{milliere_philosophical_2024,
	title = {A {Philosophical} {Introduction} to {Language} {Models} -- {Part} {I}: {Continuity} {With} {Classic} {Debates}},
	shorttitle = {A {Philosophical} {Introduction} to {Language} {Models} -- {Part} {I}},
	url = {http://arxiv.org/abs/2401.03910},
	abstract = {Large language models like GPT-4 have achieved remarkable proficiency in a broad spectrum of language-based tasks, some of which are traditionally associated with hallmarks of human intelligence. This has prompted ongoing disagreements about the extent to which we can meaningfully ascribe any kind of linguistic or cognitive competence to language models. Such questions have deep philosophical roots, echoing longstanding debates about the status of artificial neural networks as cognitive models. This article–the first part of two companion papers–serves both as a primer on language models for philosophers, and as an opinionated survey of their significance in relation to classic debates in the philosophy cognitive science, artificial intelligence, and linguistics. We cover topics such as compositionality, language acquisition, semantic competence, grounding, world models, and the transmission of cultural knowledge. We argue that the success of language models challenges several long-held assumptions about artificial neural networks. However, we also highlight the need for further empirical investigation to better understand their internal mechanisms. This sets the stage for the companion paper (Part II), which turns to novel empirical methods for probing the inner workings of language models, and new philosophical questions prompted by their latest developments.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Millière, Raphaël and Buckner, Cameron},
	month = jan,
	year = {2024},
	note = {arXiv:2401.03910 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {2401.pdf:C\:\\Users\\user\\Zotero\\storage\\MNZS9CBP\\2401.pdf:application/pdf},
}

@misc{milliere_philosophical_2024-1,
	title = {A {Philosophical} {Introduction} to {Language} {Models} - {Part} {II}: {The} {Way} {Forward}},
	shorttitle = {A {Philosophical} {Introduction} to {Language} {Models} - {Part} {II}},
	url = {http://arxiv.org/abs/2405.03207},
	abstract = {In this paper, the second of two companion pieces, we explore novel philosophical questions raised by recent progress in large language models (LLMs) that go beyond the classical debates covered in the first part. We focus particularly on issues related to interpretability, examining evidence from causal intervention methods about the nature of LLMs’ internal representations and computations. We also discuss the implications of multimodal and modular extensions of LLMs, recent debates about whether such systems may meet minimal criteria for consciousness, and concerns about secrecy and reproducibility in LLM research. Finally, we discuss whether LLM-like systems may be relevant to modeling aspects of human cognition, if their architectural characteristics and learning scenario are adequately constrained.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Millière, Raphaël and Buckner, Cameron},
	month = may,
	year = {2024},
	note = {arXiv:2405.03207 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Millière e Buckner - 2024 - A Philosophical Introduction to Language Models - .pdf:C\:\\Users\\user\\Zotero\\storage\\V6NA42AX\\Millière e Buckner - 2024 - A Philosophical Introduction to Language Models - .pdf:application/pdf},
}

@misc{kartsaklis_lambeq_2021,
	title = {lambeq: {An} {Efficient} {High}-{Level} {Python} {Library} for {Quantum} {NLP}},
	shorttitle = {lambeq},
	url = {http://arxiv.org/abs/2110.04236},
	abstract = {We present lambeq, the ﬁrst high-level Python library for Quantum Natural Language Processing (QNLP). The open-source toolkit offers a detailed hierarchy of modules and classes implementing all stages of a pipeline for converting sentences to string diagrams, tensor networks, and quantum circuits ready to be used on a quantum computer. lambeq supports syntactic parsing, rewriting and simpliﬁcation of string diagrams, ansatz creation and manipulation, as well as a number of compositional models for preparing quantum-friendly representations of sentences, employing various degrees of syntax sensitivity. We present the generic architecture and describe the most important modules in detail, demonstrating the usage with illustrative examples. Further, we test the toolkit in practice by using it to perform a number of experiments on simple NLP tasks, implementing both classical and quantum pipelines.},
	language = {en},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Kartsaklis, Dimitri and Fan, Ian and Yeung, Richie and Pearson, Anna and Lorenz, Robin and Toumi, Alexis and de Felice, Giovanni and Meichanetzidis, Konstantinos and Clark, Stephen and Coecke, Bob},
	month = oct,
	year = {2021},
	note = {arXiv:2110.04236 [quant-ph]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Quantum Physics},
	file = {Kartsaklis et al. - 2021 - lambeq An Efficient High-Level Python Library for.pdf:C\:\\Users\\user\\Zotero\\storage\\4JNULAFJ\\Kartsaklis et al. - 2021 - lambeq An Efficient High-Level Python Library for.pdf:application/pdf},
}

@misc{orus_practical_2014,
	title = {A {Practical} {Introduction} to {Tensor} {Networks}: {Matrix} {Product} {States} and {Projected} {Entangled} {Pair} {States}},
	shorttitle = {A {Practical} {Introduction} to {Tensor} {Networks}},
	url = {http://arxiv.org/abs/1306.2164},
	doi = {10.1016/j.aop.2014.06.013},
	abstract = {This is a partly non-technical introduction to selected topics on tensor network methods, based on several lectures and introductory seminars given on the subject. It should be a good place for newcomers to get familiarized with some of the key ideas in the ﬁeld, specially regarding the numerics. After a very general introduction we motivate the concept of tensor network and provide several examples. We then move on to explain some basics about Matrix Product States (MPS) and Projected Entangled Pair States (PEPS). Selected details on some of the associated numerical methods for 1d and 2d quantum lattice systems are also discussed.},
	language = {en},
	urldate = {2024-08-21},
	author = {Orus, Roman},
	month = jun,
	year = {2014},
	note = {arXiv:1306.2164 [cond-mat, physics:hep-lat, physics:hep-th, physics:quant-ph]},
	keywords = {Quantum Physics, Condensed Matter - Strongly Correlated Electrons, High Energy Physics - Lattice, High Energy Physics - Theory},
	annote = {Comment: 51 pages, 45 figures, minor typos corrected. Accepted for publication in Annals of Physics},
	file = {Orus - 2014 - A Practical Introduction to Tensor Networks Matri.pdf:C\:\\Users\\user\\Zotero\\storage\\JTEIKDKV\\Orus - 2014 - A Practical Introduction to Tensor Networks Matri.pdf:application/pdf},
}

@misc{pestun_tensor_2017,
	title = {Tensor network language model},
	url = {http://arxiv.org/abs/1710.10248},
	abstract = {We propose a new statistical model suitable for machine learning of systems with long distance correlations such as natural languages. The model is based on directed acylic graph decorated by multi-linear tensor maps in the vertices and vector spaces in the edges, called tensor network. Such tensor networks have been previously employed for eﬀective numerical computation of the renormalization group ﬂow on the space of eﬀective quantum ﬁeld theories and lattice models of statistical mechanics. We provide explicit algebro-geometric analysis of the parameter moduli space for tree graphs, discuss model properties and applications such as statistical translation.},
	language = {en},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Pestun, Vasily and Vlassopoulos, Yiannis},
	month = oct,
	year = {2017},
	note = {arXiv:1710.10248 [cond-mat, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks},
	annote = {Comment: 21 pages},
	file = {Pestun e Vlassopoulos - 2017 - Tensor network language model.pdf:C\:\\Users\\user\\Zotero\\storage\\N8VD5KXF\\Pestun e Vlassopoulos - 2017 - Tensor network language model.pdf:application/pdf},
}

@article{guarasci_quantum_2022,
	title = {Quantum {Natural} {Language} {Processing}: {Challenges} and {Opportunities}},
	abstract = {The meeting between Natural Language Processing (NLP) and Quantum Computing has been very successful in recent years, leading to the development of several approaches of the so-called Quantum Natural Language Processing (QNLP). This is a hybrid ﬁeld in which the potential of quantum mechanics is exploited and applied to critical aspects of language processing, involving different NLP tasks. Approaches developed so far span from those that demonstrate the quantum advantage only at the theoretical level to the ones implementing algorithms on quantum hardware. This paper aims to list the approaches developed so far, categorizing them by type, i.e., theoretical work and those implemented on classical or quantum hardware; by task, i.e., general purpose such as syntax-semantic representation or speciﬁc NLP tasks, like sentiment analysis or question answering; and by the resource used in the evaluation phase, i.e., whether a benchmark dataset or a custom one has been used. The advantages offered by QNLP are discussed, both in terms of performance and methodology, and some considerations about the possible usage QNLP approaches in the place of state-of-the-art deep learning-based ones are given.},
	language = {en},
	author = {Guarasci, Raffaele and Pietro, Giuseppe De and Esposito, Massimo},
	year = {2022},
	file = {Guarasci et al. - 2022 - Quantum Natural Language Processing Challenges an.pdf:C\:\\Users\\user\\Zotero\\storage\\ABC6U3ZD\\Guarasci et al. - 2022 - Quantum Natural Language Processing Challenges an.pdf:application/pdf},
}

@inproceedings{lambek_type_1999,
	address = {Berlin, Heidelberg},
	title = {Type {Grammar} {Revisited}},
	isbn = {978-3-540-48975-7},
	doi = {10.1007/3-540-48975-4_1},
	abstract = {A protogroup is an ordered monoid in which each element a has both a left proto-inverse aℓ such that aℓa ≤ 1 and a right proto-inverse ar such that aar ≤ 1. We explore the assignment of elements of a free protogroup to English words as an aid for checking which strings of words are well-formed sentences, though ultimately we may have to relax the requirement of freeness. By a pregroup we mean a protogroup which also satisfies 1 ≤ aaℓ and 1 ≤ ara, rendering aℓ a left adjoint and ar a right adjoint of a. A pregroup is precisely a poset model of classical non-commutative linear logic in which the tensor product coincides with it dual. This last condition is crucial to our treatment of passives and Wh-questions, which exploits the fact that aℓℓ ≠ a in general. Free pregroups may be used to recognize the same sentences as free protogroups.},
	language = {en},
	booktitle = {Logical {Aspects} of {Computational} {Linguistics}},
	publisher = {Springer},
	author = {Lambek, J.},
	editor = {Lecomte, Alain and Lamarche, François and Perrier, Guy},
	year = {1999},
	keywords = {Linear Logic, Mass Noun, Noun Phrase, Relative Clause, Type Assignment},
	pages = {1--27},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\ICG2GBE3\\Lambek - 1999 - Type Grammar Revisited.pdf:application/pdf},
}

@misc{wiland_bear_2024,
	title = {{BEAR}: {A} {Unified} {Framework} for {Evaluating} {Relational} {Knowledge} in {Causal} and {Masked} {Language} {Models}},
	shorttitle = {{BEAR}},
	url = {http://arxiv.org/abs/2404.04113},
	abstract = {Knowledge probing assesses to which degree a language model (LM) has successfully learned relational knowledge during pre-training. Probing is an inexpensive way to compare LMs of different sizes and training configurations. However, previous approaches rely on the objective function used in pre-training LMs and are thus applicable only to masked or causal LMs. As a result, comparing different types of LMs becomes impossible. To address this, we propose an approach that uses an LM’s inherent ability to estimate the log-likelihood of any given textual statement. We carefully design an evaluation dataset of 7,731 instances (40,916 in a larger variant) from which we produce alternative statements for each relational fact, one of which is correct. We then evaluate whether an LM correctly assigns the highest log-likelihood to the correct statement. Our experimental evaluation of 22 common LMs shows that our proposed framework, BEAR, can effectively probe for knowledge across different LM types. We release the BEAR datasets and an open-source framework that implements the probing approach to the research community to facilitate the evaluation and development of LMs.},
	language = {en},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Wiland, Jacek and Ploner, Max and Akbik, Alan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.04113 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NAACL 2024},
	file = {Wiland et al. - 2024 - BEAR A Unified Framework for Evaluating Relationa.pdf:C\:\\Users\\user\\Zotero\\storage\\TJMW4JR4\\Wiland et al. - 2024 - BEAR A Unified Framework for Evaluating Relationa.pdf:application/pdf},
}

@misc{tull_formalising_2023,
	title = {Formalising and {Learning} a {Quantum} {Model} of {Concepts}},
	url = {http://arxiv.org/abs/2302.14822},
	abstract = {In this report we present a new modelling framework for concepts based on quantum theory, and demonstrate how the conceptual representations can be learned automatically from data. A contribution of the work is a thorough category-theoretic formalisation of our framework. We claim that the use of category theory, and in particular the use of string diagrams to describe quantum processes, helps elucidate some of the most important features of our quantum approach to concept modelling. Our approach builds upon Ga¨rdenfors’ classical framework of conceptual spaces, in which cognition is modelled geometrically through the use of convex spaces, which in turn factorise in terms of simpler spaces called domains. We show how concepts from the domains of shape, colour, size and position can be learned from images of simple shapes, where individual images are represented as quantum states and concepts as quantum eﬀects. Concepts are learned by a hybrid classical-quantum network trained to perform concept classiﬁcation, where the classical image processing is carried out by a convolutional neural network and the quantum representations are produced by a parameterised quantum circuit. We also use discarding to produce mixed eﬀects, which can then be used to learn concepts which only apply to a subset of the domains, and show how entanglement (together with discarding) can be used to capture interesting correlations across domains. Finally, we consider the question of whether our quantum models of concepts can be considered conceptual spaces in the Ga¨rdenfors sense.},
	language = {en},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Tull, Sean and Shaikh, Razin A. and Zemljic, Sara Sabrina and Clark, Stephen},
	month = feb,
	year = {2023},
	note = {arXiv:2302.14822 [quant-ph, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Quantum Physics, Quantitative Biology - Neurons and Cognition},
	file = {Tull et al. - 2023 - Formalising and Learning a Quantum Model of Concep.pdf:C\:\\Users\\user\\Zotero\\storage\\3XG6S4JJ\\Tull et al. - 2023 - Formalising and Learning a Quantum Model of Concep.pdf:application/pdf},
}

@article{tull_conceptual_2024,
	title = {From conceptual spaces to quantum concepts: formalising and learning structured conceptual models},
	volume = {6},
	issn = {2524-4914},
	shorttitle = {From conceptual spaces to quantum concepts},
	url = {https://doi.org/10.1007/s42484-023-00134-z},
	doi = {10.1007/s42484-023-00134-z},
	abstract = {In this article we present a new modelling framework for structured concepts using a category-theoretic generalisation of conceptual spaces, and show how the conceptual representations can be learned automatically from data, using two very different instantiations: one classical and one quantum. A contribution of the work is a thorough category-theoretic formalisation of our framework. We claim that the use of category theory, and in particular the use of string diagrams to describe quantum processes, helps elucidate some of the most important features of our approach. We build upon Gärdenfors’ classical framework of conceptual spaces, in which cognition is modelled geometrically through the use of convex spaces, which in turn factorise in terms of simpler spaces called domains. We show how concepts from the domains of shape, colour, size and position can be learned from images of simple shapes, where concepts are represented as Gaussians in the classical implementation, and quantum effects in the quantum one. In the classical case we develop a new model which is inspired by the \$\${\textbackslash}beta \$\$-VAE model of concepts, but is designed to be more closely connected with language, so that the names of concepts form part of the graphical model. In the quantum case, concepts are learned by a hybrid classical-quantum network trained to perform concept classification, where the classical image processing is carried out by a convolutional neural network and the quantum representations are produced by a parameterised quantum circuit. Finally, we consider the question of whether our quantum models of concepts can be considered conceptual spaces in the Gärdenfors sense.},
	language = {en},
	number = {1},
	urldate = {2024-08-26},
	journal = {Quantum Machine Intelligence},
	author = {Tull, Sean and Shaikh, Razin A. and Zemljič, Sara Sabrina and Clark, Stephen},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Category theory, Concept learning, Conceptual spaces, Quantum cognition},
	pages = {21},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\5ZU7CBCP\\Tull et al. - 2024 - From conceptual spaces to quantum concepts formal.pdf:application/pdf},
}

@misc{geiger_causal_2024,
	title = {Causal {Abstraction}: {A} {Theoretical} {Foundation} for {Mechanistic} {Interpretability}},
	shorttitle = {Causal {Abstraction}},
	url = {http://arxiv.org/abs/2301.04709},
	abstract = {Causal abstraction provides a theoretical foundation for mechanistic interpretability, the ﬁeld concerned with providing intelligible algorithms that are faithful simpliﬁcations of the known, but opaque low-level details of black box AI models. Our contributions are (1) generalizing the theory of causal abstraction from mechanism replacement (i.e., hard and soft interventions) to arbitrary mechanism transformation (i.e., functionals from old mechanisms to new mechanisms), (2) providing a ﬂexible, yet precise formalization for the core concepts of modular features, polysemantic neurons, and graded faithfulness, and (3) unifying a variety of mechanistic interpretability methodologies in the common language of causal abstraction, namely activation and path patching, causal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure, sparse autoencoders, diﬀerential binary masking, distributed alignment search, and activation steering.},
	language = {en},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Geiger, Atticus and Ibeling, Duligur and Zur, Amir and Chaudhary, Maheep and Chauhan, Sonakshi and Huang, Jing and Arora, Aryaman and Wu, Zhengxuan and Goodman, Noah and Potts, Christopher and Icard, Thomas},
	month = aug,
	year = {2024},
	note = {arXiv:2301.04709 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {2301.pdf:C\:\\Users\\user\\Zotero\\storage\\GWAQFV79\\2301.pdf:application/pdf},
}

@misc{mueller_quest_2024,
	title = {The {Quest} for the {Right} {Mediator}: {A} {History}, {Survey}, and {Theoretical} {Grounding} of {Causal} {Interpretability}},
	shorttitle = {The {Quest} for the {Right} {Mediator}},
	url = {http://arxiv.org/abs/2408.01416},
	abstract = {Interpretability provides a toolset for understanding how and why neural networks behave in certain ways. However, there is little unity in the field: most studies employ ad-hoc evaluations and do not share theoretical foundations, making it difficult to measure progress and compare the pros and cons of different techniques. Furthermore, while mechanistic understanding is frequently discussed, the basic causal units underlying these mechanisms are often not explicitly defined. In this paper, we propose a perspective on interpretability research grounded in causal mediation analysis. Specifically, we describe the history and current state of interpretability taxonomized according to the types of causal units (mediators) employed, as well as methods used to search over mediators. We discuss the pros and cons of each mediator, providing insights as to when particular kinds of mediators and search methods are most appropriate depending on the goals of a given study. We argue that this framing yields a more cohesive narrative of the field, as well as actionable insights for future work. Specifically, we recommend a focus on discovering new mediators with better trade-offs between human-interpretability and compute-efficiency, and which can uncover more sophisticated abstractions from neural networks than the primarily linear mediators employed in current work. We also argue for more standardized evaluations that enable principled comparisons across mediator types, such that we can better understand when particular causal units are better suited to particular use cases.},
	language = {en},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Mueller, Aaron and Brinkmann, Jannik and Li, Millicent and Marks, Samuel and Pal, Koyena and Prakash, Nikhil and Rager, Can and Sankaranarayanan, Aruna and Sharma, Arnab Sen and Sun, Jiuding and Todd, Eric and Bau, David and Belinkov, Yonatan},
	month = aug,
	year = {2024},
	note = {arXiv:2408.01416 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Mueller et al. - 2024 - The Quest for the Right Mediator A History, Surve.pdf:C\:\\Users\\user\\Zotero\\storage\\6A5R9HUQ\\Mueller et al. - 2024 - The Quest for the Right Mediator A History, Surve.pdf:application/pdf},
}

@misc{wang_improving_2024,
	title = {Improving {Text} {Embeddings} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.00368},
	abstract = {In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pretraining with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.},
	language = {en},
	urldate = {2024-08-27},
	publisher = {arXiv},
	author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
	month = may,
	year = {2024},
	note = {arXiv:2401.00368 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: Accepted by ACL 2024},
	file = {Wang et al. - 2024 - Improving Text Embeddings with Large Language Mode.pdf:C\:\\Users\\user\\Zotero\\storage\\HNUI74QS\\Wang et al. - 2024 - Improving Text Embeddings with Large Language Mode.pdf:application/pdf},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-08-27},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\Q26MCTVL\\Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\XHWRI8NF\\2201.html:text/html},
}

@misc{xiong_benchmarking_2024,
	title = {Benchmarking {Retrieval}-{Augmented} {Generation} for {Medicine}},
	url = {http://arxiv.org/abs/2402.13178},
	abstract = {While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrievalaugmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MEDRAG toolkit introduced in this work. Overall, MEDRAG improves the accuracy of six different LLMs by up to 18\% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the “lostin-the-middle” effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.},
	language = {en},
	urldate = {2024-08-27},
	publisher = {arXiv},
	author = {Xiong, Guangzhi and Jin, Qiao and Lu, Zhiyong and Zhang, Aidong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.13178 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: Homepage: https://teddy-xionggz.github.io/benchmark-medical-rag/},
	file = {Xiong et al. - 2024 - Benchmarking Retrieval-Augmented Generation for Me.pdf:C\:\\Users\\user\\Zotero\\storage\\PJXNM4A7\\Xiong et al. - 2024 - Benchmarking Retrieval-Augmented Generation for Me.pdf:application/pdf},
}

@misc{cui_ultrafeedback_2024,
	title = {{UltraFeedback}: {Boosting} {Language} {Models} with {Scaled} {AI} {Feedback}},
	shorttitle = {{UltraFeedback}},
	url = {http://arxiv.org/abs/2310.01377},
	abstract = {Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality AI feedback automatically for a scalable alternative. Specifically, we identify scale and diversity as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present ULTRAFEEDBACK, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon ULTRAFEEDBACK, we align a LLaMA-based model by best-of-n sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research.},
	language = {en},
	urldate = {2024-08-27},
	publisher = {arXiv},
	author = {Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and He, Bingxiang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Xie, Ruobing and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
	month = jul,
	year = {2024},
	note = {arXiv:2310.01377 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: ICML 2024 camera ready},
	file = {Cui et al. - 2024 - UltraFeedback Boosting Language Models with Scale.pdf:C\:\\Users\\user\\Zotero\\storage\\8E4X5NST\\Cui et al. - 2024 - UltraFeedback Boosting Language Models with Scale.pdf:application/pdf},
}

@article{warglien_semantics_2013,
	title = {Semantics, conceptual spaces, and the meeting of minds},
	volume = {190},
	copyright = {Springer Science+Business Media Dordrecht 2013},
	issn = {00397857},
	url = {https://www.proquest.com/docview/1372303790/abstract/941C2505D79A449DPQ/1},
	doi = {10.1007/s11229-011-9963-z},
	abstract = {We present an account of semantics that is not construed as a mapping of language to the world but rather as a mapping between individual meaning spaces. The meanings of linguistic entities are established via a "meeting of minds." The concepts in the minds of communicating individuals are modeled as convex regions in conceptual spaces. We outline a mathematical framework, based on fixpoints in continuous mappings between conceptual spaces, that can be used to model such a semantics. If concepts are convex, it will in general be possible for interactors to agree on joint meaning even if they start out from different representational spaces. Language is discrete, while mental representations tend to be continuous--posing a seeming paradox. We show that the convexity assumption allows us to address this problem. Using examples, we further show that our approach helps explain the semantic processes involved in the composition of expressions.[PUBLICATION ABSTRACT]},
	language = {English},
	number = {12},
	urldate = {2024-08-28},
	journal = {Synthese},
	author = {Warglien, Massimo and Gärdenfors, Peter},
	month = aug,
	year = {2013},
	note = {Num Pages: 2165-2193
Place: Dordrecht, Netherlands
Publisher: Springer Nature B.V.},
	keywords = {Cognition \& reasoning, Conceptual semantics, Conceptual space, Convex, Linguistics, Meaning, Mental representation, Mind, Semantics, Space},
	pages = {2165--2193},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\9495H79S\\Warglien e Gärdenfors - 2013 - Semantics, conceptual spaces, and the meeting of m.pdf:application/pdf},
}

@article{rama_fiorini_representing_2014,
	title = {Representing part–whole relations in conceptual spaces},
	volume = {15},
	copyright = {http://www.springer.com/tdm},
	issn = {1612-4782, 1612-4790},
	url = {http://link.springer.com/10.1007/s10339-013-0585-x},
	doi = {10.1007/s10339-013-0585-x},
	language = {en},
	number = {2},
	urldate = {2024-08-28},
	journal = {Cognitive Processing},
	author = {Rama Fiorini, Sandro and Gärdenfors, Peter and Abel, Mara},
	month = may,
	year = {2014},
	pages = {127--142},
	file = {Rama Fiorini et al. - 2014 - Representing part–whole relations in conceptual sp.pdf:C\:\\Users\\user\\Zotero\\storage\\TW2XZYI6\\Rama Fiorini et al. - 2014 - Representing part–whole relations in conceptual sp.pdf:application/pdf},
}

@article{gerstl_midwinters_1995,
	title = {Midwinters, end games, and body parts: a classification of part-whole relations},
	volume = {43},
	issn = {1071-5819},
	shorttitle = {Midwinters, end games, and body parts},
	url = {https://www.sciencedirect.com/science/article/pii/S1071581985710798},
	doi = {10.1006/ijhc.1995.1079},
	abstract = {This paper deals with the conceptual part-whole relation as it occurs in language processing, visual perception, and general problem solving. One important long-term goal is to develop a naive or common sense theory of the mereological domain, that is the domain of parts and wholes and their relations. In this paper, we work towards such a theory by presenting a classification of part-whole relations that is suitable for different cognitive tasks and give proposals for the representation and processing of these relations. In order to be independent of specific tasks like language understanding or the recognition of objects, we use structural properties to develop our classification. The paper starts with a brief overview of the mereological research in different disciplines and two examples of the role of part-whole relations in linguistics (possessive constructions) and knowledge processing (reasoning about objects). In the second section, we discuss two important approaches to mereological problems: the "Classical Extensional Mereology" as presented by Simons and the meronymic system of part-whole relations proposed by Winston, Chaffin and Hermann. Our own work is described in the third and last section. First, we discuss different kinds of wholes according to their inherent compositional structure; complexes, collections, and masses. Then partitions induced by or independent of the compositional structure of a whole are described, accompanied by proposals for their processing.},
	number = {5},
	urldate = {2024-08-28},
	journal = {International Journal of Human-Computer Studies},
	author = {Gerstl, Peter and Pribbenow, Simone},
	month = nov,
	year = {1995},
	pages = {865--889},
	file = {Gerstl e Pribbenow - 1995 - Midwinters, end games, and body parts a classific.pdf:C\:\\Users\\user\\Zotero\\storage\\WVB3EFQG\\Gerstl e Pribbenow - 1995 - Midwinters, end games, and body parts a classific.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\user\\Zotero\\storage\\JLALYT7U\\S1071581985710798.html:text/html},
}

@misc{bolt_interacting_2017,
	title = {Interacting {Conceptual} {Spaces} {I} : {Grammatical} {Composition} of {Concepts}},
	shorttitle = {Interacting {Conceptual} {Spaces} {I}},
	url = {http://arxiv.org/abs/1703.08314},
	abstract = {The categorical compositional approach to meaning has been successfully applied in natural language processing, outperforming other models in mainstream empirical language processing tasks. We show how this approach can be generalized to conceptual space models of cognition. In order to do this, ﬁrst we introduce the category of convex relations as a new setting for categorical compositional semantics, emphasizing the convex structure important to conceptual space applications. We then show how to construct conceptual spaces for various types such as nouns, adjectives and verbs. Finally we show by means of examples how concepts can be systematically combined to establish the meanings of composite phrases from the meanings of their constituent parts. This provides the mathematical underpinnings of a new compositional approach to cognition.},
	language = {en},
	urldate = {2024-09-01},
	publisher = {arXiv},
	author = {Bolt, Joe and Coecke, Bob and Genovese, Fabrizio and Lewis, Martha and Marsden, Dan and Piedeleu, Robin},
	month = sep,
	year = {2017},
	note = {arXiv:1703.08314 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Logic in Computer Science},
	file = {Bolt et al. - 2017 - Interacting Conceptual Spaces I  Grammatical Comp.pdf:C\:\\Users\\user\\Zotero\\storage\\A2P6ENNU\\Bolt et al. - 2017 - Interacting Conceptual Spaces I  Grammatical Comp.pdf:application/pdf},
}

@article{halford_relational_2010,
	title = {Relational knowledge: the foundation of higher cognition},
	volume = {14},
	issn = {1364-6613},
	shorttitle = {Relational knowledge},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661310002020},
	doi = {10.1016/j.tics.2010.08.005},
	abstract = {Accumulating evidence on the nature, function and acquisition of relational knowledge indicates a crucial role of such knowledge in higher cognitive processes. In this review, we specify the essential properties of relational knowledge, together with the role it plays in reasoning, categorisation, planning, quantification and language. Furthermore, we discuss the processes involved in its acquisition and how these processes have been implemented in contemporary neural network models. We present evidence demonstrating that relational knowledge integrates heuristic and analytic cognition, is important for symbolic processes and the creation of novelty, activates specific regions of the prefrontal cortex, and is the most recently evolved and slowest-developing cognitive process. Arguably, relational knowledge represents the core of higher cognition.},
	number = {11},
	urldate = {2024-09-02},
	journal = {Trends in Cognitive Sciences},
	author = {Halford, Graeme S. and Wilson, William H. and Phillips, Steven},
	month = nov,
	year = {2010},
	pages = {497--505},
	file = {ScienceDirect Snapshot:C\:\\Users\\user\\Zotero\\storage\\QUDP3A7N\\S1364661310002020.html:text/html},
}

@misc{go_aligning_2023,
	title = {Aligning {Language} {Models} with {Preferences} through f-divergence {Minimization}},
	url = {http://arxiv.org/abs/2302.08215},
	doi = {10.48550/arXiv.2302.08215},
	abstract = {Aligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, f-DPG, which allows the use of any f-divergence to approximate any target distribution that can be evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally optimal objective but that different divergences present different alignment and diversity trade-offs. We show that Jensen-Shannon divergence strikes a good balance between these objectives, and frequently outperforms forward KL divergence by a wide margin, leading to significant improvements over prior work. These distinguishing characteristics between divergences persist as the model size increases, highlighting the importance of selecting appropriate divergence objectives.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Go, Dongyoung and Korbak, Tomasz and Kruszewski, Germán and Rozen, Jos and Ryu, Nahyeon and Dymetman, Marc},
	month = jun,
	year = {2023},
	note = {arXiv:2302.08215 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\X545S6K3\\Go et al. - 2023 - Aligning Language Models with Preferences through .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\UH9SIPKU\\2302.html:text/html},
}

@article{dennett_styles_1982,
	title = {Styles of {Mental} {Representation}},
	volume = {83},
	issn = {0066-7374},
	url = {https://www.jstor.org/stable/4545000},
	urldate = {2024-09-06},
	journal = {Proceedings of the Aristotelian Society},
	author = {Dennett, Daniel C.},
	year = {1982},
	note = {Publisher: [Aristotelian Society, Wiley]},
	pages = {213--226},
	file = {JSTOR Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\W8W34H9E\\Dennett - 1982 - Styles of Mental Representation.pdf:application/pdf},
}

@article{montrul_heritage_2023,
	title = {Heritage {Languages}: {Language} {Acquired}, {Language} {Lost}, {Language} {Regained}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2333-9683, 2333-9691},
	shorttitle = {Heritage {Languages}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-linguistics-030521-050236},
	doi = {10.1146/annurev-linguistics-030521-050236},
	abstract = {A heritage language is a sociopolitically minority and/or minoritized language acquired as the first or one of the first languages in a bilingual or multilingual context. Heritage languages are typically acquired under conditions of reduced exposure and are often used less than the majority language during late childhood and adolescence. Heritage languages show structural differences and changes at all levels of linguistic analysis from baseline grammars that arise from the complex interaction between the nature and quantity of input and the age of bilinguals. Although many situations give rise to heritage languages, this article focuses on immigrants and their children and reviews foundational studies of the linguistic properties of heritage languages; studies of age effects that have shed light on critical differences between first, second, and heritage language acquisition; and recent studies of heritage language relearning and reactivation. The implications of the study of heritage languages for bilingualism and society and for the language and cognitive sciences are discussed.},
	language = {en},
	number = {1},
	urldate = {2024-09-11},
	journal = {Annual Review of Linguistics},
	author = {Montrul, Silvina},
	month = jan,
	year = {2023},
	pages = {399--418},
	file = {Montrul - 2023 - Heritage Languages Language Acquired, Language Lo.pdf:C\:\\Users\\user\\Zotero\\storage\\FJ37S5EH\\Montrul - 2023 - Heritage Languages Language Acquired, Language Lo.pdf:application/pdf},
}

@inproceedings{alam_llms_2024,
	address = {St. Julian's, Malta},
	title = {{LLMs} for {Low} {Resource} {Languages} in {Multilingual}, {Multimodal} and {Dialectal} {Settings}},
	url = {https://aclanthology.org/2024.eacl-tutorials.5},
	abstract = {The recent breakthroughs in Artificial Intelligence (AI) can be attributed to the remarkable performance of Large Language Models (LLMs) across a spectrum of research areas (e.g., machine translation, question-answering, automatic speech recognition, text-to-speech generation) and application domains (e.g., business, law, healthcare, education, and psychology). The success of these LLMs largely de- pends on specific training techniques, most notably instruction tuning, RLHF, and subsequent prompting to achieve the desired output. As the development of such LLMs continues to increase in both closed and open settings, evaluation has become crucial for understanding their generalization capabilities across different tasks, modalities, languages, and dialects. This evaluation process is tightly coupled with prompting, which plays a key role in obtain- ing better outputs. There has been attempts to evaluate such models focusing on diverse tasks, languages, and dialects, which suggests that the capabilities of LLMs are still limited to medium-to-low-resource languages due to the lack of representative datasets. The tutorial offers an overview of this emerging research area. We explore the capabilities of LLMs in terms of their performance, zero- and few-shot settings, fine-tuning, instructions tuning, and close vs. open models with a special emphasis on low-resource settings. In addition to LLMs for standard NLP tasks, we will focus on speech and multimodality.},
	urldate = {2024-09-11},
	booktitle = {Proceedings of the 18th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Tutorial} {Abstracts}},
	publisher = {Association for Computational Linguistics},
	author = {Alam, Firoj and Chowdhury, Shammur Absar and Boughorbel, Sabri and Hasanain, Maram},
	editor = {Mesgar, Mohsen and Loáiciga, Sharid},
	month = mar,
	year = {2024},
	pages = {27--33},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\KD8NJJNH\\Alam et al. - 2024 - LLMs for Low Resource Languages in Multilingual, M.pdf:application/pdf},
}

@article{liu_visual_nodate,
	title = {Visual {Instruction} {Tuning}},
	abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal ﬁeld. We present the ﬁrst attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for generalpurpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When ﬁne-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available.},
	language = {en},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	file = {Liu et al. - Visual Instruction Tuning.pdf:C\:\\Users\\user\\Zotero\\storage\\NZQ4D9XW\\Liu et al. - Visual Instruction Tuning.pdf:application/pdf},
}

@article{liu_visual_2023,
	title = {Visual {Instruction} {Tuning}},
	volume = {36},
	url = {https://papers.nips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html},
	language = {en},
	urldate = {2024-09-12},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	month = dec,
	year = {2023},
	pages = {34892--34916},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\A8F9RYQC\\Liu et al. - 2023 - Visual Instruction Tuning.pdf:application/pdf},
}

@misc{liu_improved_2024,
	title = {Improved {Baselines} with {Visual} {Instruction} {Tuning}},
	url = {http://arxiv.org/abs/2310.03744},
	doi = {10.48550/arXiv.2310.03744},
	abstract = {Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in {\textasciitilde}1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.},
	urldate = {2024-09-13},
	publisher = {arXiv},
	author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
	month = may,
	year = {2024},
	note = {arXiv:2310.03744 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\user\\Zotero\\storage\\S4DRMNDA\\Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\user\\Zotero\\storage\\FAF22IPW\\2310.html:text/html},
}

@misc{coecke_mathematics_2020,
	title = {The {Mathematics} of {Text} {Structure}},
	url = {http://arxiv.org/abs/1904.03478},
	abstract = {In previous work we gave a mathematical foundation, referred to as DisCoCat, for how words interact in a sentence in order to produce the meaning of that sentence. To do so, we exploited the perfect structural match of grammar and categories of meaning spaces.},
	language = {en},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Coecke, Bob},
	month = feb,
	year = {2020},
	note = {arXiv:1904.03478 [quant-ph]},
	keywords = {Computer Science - Computation and Language, Quantum Physics, Mathematics - Category Theory},
	annote = {Comment: 37 pages, many pictures},
	file = {Coecke - 2020 - The Mathematics of Text Structure.pdf:C\:\\Users\\user\\Zotero\\storage\\5YUYNZL4\\Coecke - 2020 - The Mathematics of Text Structure.pdf:application/pdf},
}

@book{coecke_picturing_2017,
	edition = {1},
	title = {Picturing {Quantum} {Processes}: {A} {First} {Course} in {Quantum} {Theory} and {Diagrammatic} {Reasoning}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-1-107-10422-8 978-1-316-21931-7},
	shorttitle = {Picturing {Quantum} {Processes}},
	url = {https://www.cambridge.org/core/product/identifier/9781316219317/type/book},
	abstract = {The unique features of the quantum world are explained in this book through the language of diagrams, setting out an innovative visual method for presenting complex theories. Requiring only basic mathematical literacy, this book employs a unique formalism that builds an intuitive understanding of quantum features while eliminating the need for complex calculations. This entirely diagrammatic presentation of quantum theory represents the culmination of ten years of research, uniting classical techniques in linear algebra and Hilbert spaces with cutting-edge developments in quantum computation and foundations. Written in an entertaining and user-friendly style and including more than one hundred exercises, this book is an ideal first course in quantum theory, foundations, and computation for students from undergraduate to PhD level, as well as an opportunity for researchers from a broad range of fields, from physics to biology, linguistics, and cognitive science, to discover a new set of tools for studying processes and interaction.},
	language = {en},
	urldate = {2024-09-19},
	publisher = {Cambridge University Press},
	author = {Coecke, Bob and Kissinger, Aleks},
	month = mar,
	year = {2017},
	doi = {10.1017/9781316219317},
	file = {Coecke e Kissinger - 2017 - Picturing Quantum Processes A First Course in Qua.pdf:C\:\\Users\\user\\Zotero\\storage\\GI8U3FCX\\Coecke e Kissinger - 2017 - Picturing Quantum Processes A First Course in Qua.pdf:application/pdf},
}

@misc{wang-mascianica_distilling_2023,
	title = {Distilling {Text} into {Circuits}},
	url = {http://arxiv.org/abs/2301.10595},
	abstract = {This paper concerns the structure of meanings within natural language. In [13] a framework for natural language named DisCoCirc was sketched that has the following features: (1) it is both compositional and distributional (a.k.a. vectorial); (2) it applies to general text, not just sentences; (3) it captures linguistic ‘connections’ between meanings (cf. grammar); (4) word meanings get updated as text progresses; (5) sentence types reﬂect the words that have their meaning updated; (6) language ambiguity is naturally accommodated. In this paper, we realise DisCoCirc for a substantial fragment of English. We moreover show that when passing to DisCoCirc’s text circuits, some ‘grammatical bureaucracy’ is eliminated. That is, DisCoCirc displays a signiﬁcant degree of: (7) inter- and intra-language independence. By inter-language independence we mean for example independence from word-order conventions that diﬀer across languages, and by intra-language independence we mean for example independence from choices like using many short sentences versus few long sentences. This inter-language independence means our text circuits should carry over to other languages, unlike the language-speciﬁc typings of categorial grammars. Hence, text circuits are a lean structure for the ‘actual substance of text’, that is, the inner-workings of meanings within text across several layers of expressiveness (cf. words, sentences, text), and may capture that what is truly universal beneath grammar.},
	language = {en},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wang-Mascianica, Vincent and Liu, Jonathon and Coecke, Bob},
	month = jan,
	year = {2023},
	note = {arXiv:2301.10595 [cs, math]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Mathematics - Category Theory},
	annote = {Comment: 53 pages, lots of figures},
	file = {Wang-Mascianica et al. - 2023 - Distilling Text into Circuits.pdf:C\:\\Users\\user\\Zotero\\storage\\PRSCSH47\\Wang-Mascianica et al. - 2023 - Distilling Text into Circuits.pdf:application/pdf},
}

@misc{floridi_why_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Why the {AI} {Hype} is another {Tech} {Bubble}},
	url = {https://papers.ssrn.com/abstract=4960826},
	abstract = {This article argues that the current hype surrounding artificial intelligence (AI) exhibits characteristics of a tech bubble, based on parallels with five previous technological bubbles: the Dot-Com Bubble, the Telecom Bubble, the Chinese Tech Bubble, the Cryptocurrency Boom, and the Tech Stock Bubble. The AI hype cycle shares with them some essential features, including the presence of potentially disruptive technology, speculation outpacing reality, the emergence of new valuation paradigms, significant retail investor participation, and a lack of adequate regulation. The article also highlights other specific similarities, such as the proliferation of AI startups, inflated valuations, and the ethical concerns associated with the technology. While acknowledging AI's transformative potential, the article calls for pragmatic caution, evidence-based planning, and critical thinking in approaching the current hype. It concludes by offering some recommendations to minimise the negative impact of the impending bubble burst, emphasising the importance of focusing on sustainable business models and real-world applications, maintaining a balanced perspective on AI's potential and limitations, and supporting the development of effective regulatory frameworks to guide the technology's design, development, and deployment.},
	language = {en},
	urldate = {2024-09-20},
	author = {Floridi, Luciano},
	month = sep,
	year = {2024},
	keywords = {Artificial Intelligence, AI bubble, AI Hype, AI Winter, Tech Bubble},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\8CWTQK3A\\Floridi - 2024 - Why the AI Hype is another Tech Bubble.pdf:application/pdf},
}

@misc{hou_clerc_2024,
	title = {{CLERC}: {A} {Dataset} for {Legal} {Case} {Retrieval} and {Retrieval}-{Augmented} {Analysis} {Generation}},
	shorttitle = {{CLERC}},
	url = {http://arxiv.org/abs/2406.17186},
	abstract = {Legal professionals need to write analyses that rely on citations to relevant precedents, i.e., previous case decisions. Intelligence systems assisting legal professionals in writing such documents provide great benefits but are challenging to design. Such systems need to help locate, summarize, and reason over salient precedents in order to be useful. To enable systems for such tasks, we work with legal professionals to transform a large open-source legal corpus into a dataset1 supporting two important backbone tasks: information retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC (Case Law Evaluation and Retrieval Corpus), is constructed for training and evaluating models on their ability to (1) find corresponding citations for a given piece of legal analysis and to (2) compile the text of these citations (as well as previous context) into a cogent analysis that supports a reasoning goal. We benchmark state-of-the-art models on CLERC, showing that current approaches still struggle: GPT-4o generates analyses with the highest ROUGE F-scores but hallucinates the most, while zero-shot IR models only achieve 48.3\% recall@1000.},
	language = {en},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Hou, Abe Bohan and Weller, Orion and Qin, Guanghui and Yang, Eugene and Lawrie, Dawn and Holzenberger, Nils and Blair-Stanek, Andrew and Van Durme, Benjamin},
	month = jun,
	year = {2024},
	note = {arXiv:2406.17186 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Hou et al. - 2024 - CLERC A Dataset for Legal Case Retrieval and Retr.pdf:C\:\\Users\\user\\Zotero\\storage\\CFTGAIJ3\\Hou et al. - 2024 - CLERC A Dataset for Legal Case Retrieval and Retr.pdf:application/pdf},
}

@misc{cheng_adapting_2024,
	title = {Adapting {Large} {Language} {Models} to {Domains} via {Reading} {Comprehension}},
	url = {http://arxiv.org/abs/2309.09530},
	abstract = {We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension—practice after reading improves the ability to answer questions based on the learned knowledge—we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model’s performance even on general benchmarks, showing the potential to develop a general model across even more domains. Our model, code, and data are available at https://github.com/microsoft/LMOps.},
	language = {en},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Cheng, Daixuan and Huang, Shaohan and Wei, Furu},
	month = jul,
	year = {2024},
	note = {arXiv:2309.09530 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ICLR 2024 Conference},
	file = {Cheng et al. - 2024 - Adapting Large Language Models to Domains via Read.pdf:C\:\\Users\\user\\Zotero\\storage\\FFFKCVGP\\Cheng et al. - 2024 - Adapting Large Language Models to Domains via Read.pdf:application/pdf},
}

@misc{kwon_efficient_2023,
	title = {Efficient {Memory} {Management} for {Large} {Language} {Model} {Serving} with {PagedAttention}},
	url = {http://arxiv.org/abs/2309.06180},
	abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4\${\textbackslash}times\$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm},
	language = {en},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
	month = sep,
	year = {2023},
	note = {arXiv:2309.06180 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: SOSP 2023},
	file = {Kwon et al. - 2023 - Efficient Memory Management for Large Language Mod.pdf:C\:\\Users\\user\\Zotero\\storage\\WR2E276H\\Kwon et al. - 2023 - Efficient Memory Management for Large Language Mod.pdf:application/pdf},
}

@misc{edge_local_2024,
	title = {From {Local} to {Global}: {A} {Graph} {RAG} {Approach} to {Query}-{Focused} {Summarization}},
	shorttitle = {From {Local} to {Global}},
	url = {http://arxiv.org/abs/2404.16130},
	abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as “What are the main themes in the dataset?”, since this is inherently a queryfocused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na¨ıve RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.},
	language = {en},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16130 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, H.3.3, I.2.7},
	file = {Edge et al. - 2024 - From Local to Global A Graph RAG Approach to Quer.pdf:C\:\\Users\\user\\Zotero\\storage\\K63JVAKL\\Edge et al. - 2024 - From Local to Global A Graph RAG Approach to Quer.pdf:application/pdf},
}

@article{dupre_what_2021,
	title = {({What}) {Can} {Deep} {Learning} {Contribute} to {Theoretical} {Linguistics}?},
	volume = {31},
	issn = {1572-8641},
	url = {https://doi.org/10.1007/s11023-021-09571-w},
	doi = {10.1007/s11023-021-09571-w},
	abstract = {Deep learning (DL) techniques have revolutionised artificial systems’ performance on myriad tasks, from playing Go to medical diagnosis. Recent developments have extended such successes to natural language processing, an area once deemed beyond such systems’ reach. Despite their different goals (technological development vs. theoretical insight), these successes have suggested that such systems may be pertinent to theoretical linguistics. The competence/performance distinction presents a fundamental barrier to such inferences. While DL systems are trained on linguistic performance, linguistic theories are aimed at competence. Such a barrier has traditionally been sidestepped by assuming a fairly close correspondence: performance as competence plus noise. I argue this assumption is unmotivated. Competence and performance can differ arbitrarily. Thus, we should not expect DL models to illuminate linguistic theory.},
	language = {en},
	number = {4},
	urldate = {2024-10-01},
	journal = {Minds and Machines},
	author = {Dupre, Gabe},
	month = dec,
	year = {2021},
	keywords = {Artificial Intelligence, Competence and performance, Machine learning, Philosophy of artificial intelligence, Philosophy of linguistics},
	pages = {617--635},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\MWDHA7EL\\Dupre - 2021 - (What) Can Deep Learning Contribute to Theoretical.pdf:application/pdf},
}

@article{dupre_correction_2022,
	title = {Correction {To}: ({What}) {Can} {Deep} {Learning} {Contribute} to {Theoretical} {Linguistics}?},
	volume = {32},
	shorttitle = {Correction {To}},
	doi = {10.1007/s11023-022-09600-2},
	number = {1},
	journal = {Minds and Machines},
	author = {Dupre, Gabe},
	year = {2022},
	note = {Publisher: Springer Verlag},
	pages = {11--11},
	file = {Full text:C\:\\Users\\user\\Zotero\\storage\\X33CEJPJ\\Dupre - 2022 - Correction To (What) Can Deep Learning Contribute.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\FBVYZ3D8\\DUPCTW.html:text/html},
}

@misc{noauthor_what_nodate,
	title = {What {Can} {Language} {Models} {Tell} {Us} {About} {Human} {Cognition}?},
	url = {https://journals.sagepub.com/doi/epub/10.1177/09637214241242746},
	language = {en},
	urldate = {2024-10-01},
	doi = {10.1177/09637214241242746},
	file = {Full text:C\:\\Users\\user\\Zotero\\storage\\CAWQRJ5Y\\What Can Language Models Tell Us About Human Cogni.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\WTCFGKMY\\09637214241242746.html:text/html},
}

@incollection{konopka_theory_2017,
	title = {Theory, data, and the epistemology of syntax},
	isbn = {978-3-11-051821-4},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110518214-016/html},
	abstract = {Syntactic theory has tended to vacillate between implausible methodological extremes. Some linguists hold that our theories are accountable solely for the corpus of attested utterances; others assume our subject matter is unobservable intuitive feelings about sentences. Both extremes should be rejected. The subject matter of syntax is neither past utterance production nor the functioning of inaccessible mental machinery; it is normative — a system of tacitly grasped constraints defining correctness of structure. There are interesting parallels between syntactic and moral systems, modulo the key difference that linguistic systems are diverse whereas morality is universal. The appropriate epistemology for justifying formulations of normative systems is familiar in philosophy: it is known as the method of reflective equilibrium.},
	language = {en},
	urldate = {2024-10-01},
	booktitle = {Grammatische {Variation}},
	publisher = {De Gruyter},
	author = {Pullum, Geoffrey K.},
	editor = {Konopka, Marek and Wöllstein, Angelika},
	month = mar,
	year = {2017},
	doi = {10.1515/9783110518214-016},
	pages = {283--298},
	file = {Pullum - 2017 - Theory, data, and the epistemology of syntax.pdf:C\:\\Users\\user\\Zotero\\storage\\ZWV5EEYP\\Pullum - 2017 - Theory, data, and the epistemology of syntax.pdf:application/pdf},
}

@inproceedings{mueller_how_2023,
	address = {Toronto, Canada},
	title = {How to {Plant} {Trees} in {Language} {Models}: {Data} and {Architectural} {Effects} on the {Emergence} of {Syntactic} {Inductive} {Biases}},
	shorttitle = {How to {Plant} {Trees} in {Language} {Models}},
	url = {https://aclanthology.org/2023.acl-long.629},
	doi = {10.18653/v1/2023.acl-long.629},
	abstract = {Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic features—as opposed to incorrect linear features—when performing tasks after fine-tuning. We test what aspects of pre-training are important for endowing encoder-decoder Transformers with an inductive bias that favors hierarchical syntactic generalizations. We focus on architectural features (depth, width, and number of parameters), as well as the genre and size of the pre-training corpus, diagnosing inductive biases using two syntactic transformation tasks: question formation and passivization, both in English. We find that the number of parameters alone does not explain hierarchical generalization: model depth plays greater role than model width. We also find that pre-training on simpler language, such as child-directed speech, induces a hierarchical bias using an order-of-magnitude less data than pre-training on more typical datasets based on web text or Wikipedia; this suggests that in cognitively plausible language acquisition settings, neural language models may be more data-efficient than previously thought.},
	urldate = {2024-10-02},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Mueller, Aaron and Linzen, Tal},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {11237--11252},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\HX9JRW33\\Mueller e Linzen - 2023 - How to Plant Trees in Language Models Data and Ar.pdf:application/pdf},
}

@inproceedings{chrupala_putting_2023,
	address = {Toronto, Canada},
	title = {Putting {Natural} in {Natural} {Language} {Processing}},
	url = {https://aclanthology.org/2023.findings-acl.495},
	doi = {10.18653/v1/2023.findings-acl.495},
	abstract = {Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processing community which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to a fortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of these two fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processing could lead to better integration with the rest of language science and could lead to systems which are more data-efficient and more human-like, and which can communicate beyond the textual modality.},
	urldate = {2024-10-02},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Chrupała, Grzegorz},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {7820--7827},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\ZLEXL6G7\\Chrupała - 2023 - Putting Natural in Natural Language Processing.pdf:application/pdf},
}

@article{ambridge_large_2024,
	title = {Large language models are better than theoretical linguists at theoretical linguistics},
	volume = {50},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1613-4060},
	url = {https://www.degruyter.com/document/doi/10.1515/tl-2024-2002/html?lang=en},
	doi = {10.1515/tl-2024-2002},
	abstract = {Large language models are better than theoretical linguists at theoretical linguistics, at least in the domain of verb argument structure; explaining why (for example), we can say both The ball rolled and Someone rolled the ball , but not both The man laughed and * Someone laughed the man . Verbal accounts of this phenomenon either do not make precise quantitative predictions at all, or do so only with the help of ancillary assumptions and by-hand data processing. Large language models, on the other hand (taking text-davinci-002 as an example), predict human acceptability ratings for these types of sentences with correlations of around r = 0.9, and themselves constitute theories of language acquisition and representation; theories that instantiate exemplar-, input- and construction-based approaches, though only very loosely. Indeed, large language models succeed where these verbal (i.e., non-computational) linguistic theories fail, precisely because the latter insist – in the service of intuitive interpretability – on simple yet empirically inadequate (over)generalizations.},
	language = {en},
	number = {1-2},
	urldate = {2024-10-02},
	journal = {Theoretical Linguistics},
	author = {Ambridge, Ben and Blything, Liam},
	month = jun,
	year = {2024},
	note = {Publisher: De Gruyter Mouton},
	keywords = {large language models, causatives, grammaticality judgments},
	pages = {33--48},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\VI5WP5N4\\Ambridge e Blything - 2024 - Large language models are better than theoretical .pdf:application/pdf},
}

@article{lappin_assessing_2024,
	title = {Assessing the {Strengths} and {Weaknesses} of {Large} {Language} {Models}},
	volume = {33},
	issn = {0925-8531, 1572-9583},
	url = {https://link.springer.com/10.1007/s10849-023-09409-x},
	doi = {10.1007/s10849-023-09409-x},
	abstract = {The transformers that drive chatbots and other AI systems constitute large language models (LLMs). These are currently the focus of a lively discussion in both the scientiﬁc literature and the popular media. This discussion ranges from hyperbolic claims that attribute general intelligence and sentience to LLMs, to the skeptical view that these devices are no more than “stochastic parrots”. I present an overview of some of the weak arguments that have been presented against LLMs, and I consider several of the more compelling criticisms of these devices. The former signiﬁcantly underestimate the capacity of transformers to achieve subtle inductive inferences required for high levels of performance on complex, cognitively signiﬁcant tasks. In some instances, these arguments misconstrue the nature of deep learning. The latter criticisms identify signiﬁcant limitations in the way in which transformers learn and represent patterns in data. They also point out important diﬀerences between the procedures through which deep neural networks and humans acquire knowledge of natural language. It is necessary to look carefully at both sets of arguments in order to achieve a balanced assessment of the potential and the limitations of LLMs.},
	language = {en},
	number = {1},
	urldate = {2024-10-02},
	journal = {Journal of Logic, Language and Information},
	author = {Lappin, Shalom},
	month = mar,
	year = {2024},
	pages = {9--20},
	file = {Lappin - 2024 - Assessing the Strengths and Weaknesses of Large La.pdf:C\:\\Users\\user\\Zotero\\storage\\TYJNW9P7\\Lappin - 2024 - Assessing the Strengths and Weaknesses of Large La.pdf:application/pdf},
}

@article{storey_understanding_1993,
	title = {Understanding semantic relationships},
	volume = {2},
	copyright = {http://www.springer.com/tdm},
	issn = {1066-8888, 0949-877X},
	url = {http://link.springer.com/10.1007/BF01263048},
	doi = {10.1007/BF01263048},
	abstract = {To develop sophisticated database management systems, there is a need to incorporate more understanding of the real world in the information that is stored in a database. Semantic data models have been developed to try to capture some of the meaning, as well as the structure, of data using abstractions such as inclusion, aggregation, and association. Besides these well-known relationships, a number of additional semantic relationships have been identified by researchers in other disciplines such as linguistics, logic, and cognitive psychology. This article explores some of the lesser-recognized semantic relationships and discusses both how they could be captured, either manually or by using an automated tool, and their impact on database design. To demonstrate the feasibility of this research, a prototype system for analyzing semantic relationships, called the Semantic Relationship Analyzer, is presented.},
	language = {en},
	number = {4},
	urldate = {2024-10-02},
	journal = {The VLDB Journal},
	author = {Storey, Vede C.},
	month = oct,
	year = {1993},
	pages = {455--488},
	file = {Storey - 1993 - Understanding semantic relationships.pdf:C\:\\Users\\user\\Zotero\\storage\\4TERJZPU\\Storey - 1993 - Understanding semantic relationships.pdf:application/pdf},
}

@inproceedings{salazar_masked_2020,
	address = {Online},
	title = {Masked {Language} {Model} {Scoring}},
	url = {https://aclanthology.org/2020.acl-main.240},
	doi = {10.18653/v1/2020.acl-main.240},
	abstract = {Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model's WER by 30\% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL's unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.},
	urldate = {2024-10-02},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Salazar, Julian and Liang, Davis and Nguyen, Toan Q. and Kirchhoff, Katrin},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {2699--2712},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\NIL47CD2\\Salazar et al. - 2020 - Masked Language Model Scoring.pdf:application/pdf},
}

@inproceedings{bender_climbing_2020,
	address = {Online},
	title = {Climbing towards {NLU}: {On} {Meaning}, {Form}, and {Understanding} in the {Age} of {Data}},
	shorttitle = {Climbing towards {NLU}},
	url = {https://aclanthology.org/2020.acl-main.463},
	doi = {10.18653/v1/2020.acl-main.463},
	abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We've Been and Where We're Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
	urldate = {2024-10-03},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M. and Koller, Alexander},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {5185--5198},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\4BHJXDR4\\Bender e Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf:application/pdf},
}

@misc{papadimitriou_injecting_2023,
	title = {Injecting structural hints: {Using} language models to study inductive biases in language learning},
	shorttitle = {Injecting structural hints},
	url = {http://arxiv.org/abs/2304.13060},
	abstract = {Both humans and large language models are able to learn language without explicit structural supervision. What inductive biases make this learning possible? We address this fundamental cognitive question by leveraging transformer language models: we inject inductive bias into language models by pretraining on formally-structured data, and then evaluate the biased learners’ ability to learn typologicallydiverse natural languages. Our experimental setup creates a testbed for hypotheses about inductive bias in human language learning. We investigate the effect of injecting models with three types of inductive bias: 1) recursive, hierarchical processing, 2) crossing token-token relationships that can’t be modeled by contextfree grammars, and 3) a Zipfian power-law vocabulary distribution. We show that noncontext-free relationships form the best inductive biases. Our study leverages the capabilities of transformer models to run controlled language learning experiments that are not possible to run on humans, and surfaces hypotheses about the structures that facilitate language learning in both humans and machines.},
	language = {en},
	urldate = {2024-10-04},
	publisher = {arXiv},
	author = {Papadimitriou, Isabel and Jurafsky, Dan},
	month = oct,
	year = {2023},
	note = {arXiv:2304.13060 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Findings of EMNLP 2023},
	file = {Papadimitriou e Jurafsky - 2023 - Injecting structural hints Using language models .pdf:C\:\\Users\\user\\Zotero\\storage\\LX9Z7W2X\\Papadimitriou e Jurafsky - 2023 - Injecting structural hints Using language models .pdf:application/pdf},
}

@misc{noauthor_lexical_nodate,
	title = {Lexical semantic content, not syntactic structure, is the main contributor to {ANN}-brain similarity of {fMRI} responses in the language network {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/2023.05.05.539646v1.full},
	urldate = {2024-10-04},
	file = {Lexical semantic content, not syntactic structure, is the main contributor to ANN-brain similarity of fMRI responses in the language network | bioRxiv:C\:\\Users\\user\\Zotero\\storage\\Z4USDT7Y\\2023.05.05.539646v1.html:text/html},
}

@misc{kauf_lexical_2023,
	title = {Lexical semantic content, not syntactic structure, is the main contributor to {ANN}-brain similarity of {fMRI} responses in the language network},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.05.05.539646v1},
	doi = {10.1101/2023.05.05.539646},
	abstract = {Representations from artificial neural network (ANN) language models have been shown to predict human brain activity in the language network. To understand what aspects of linguistic stimuli contribute to ANN-to-brain similarity, we used an fMRI dataset of responses to n=627 naturalistic English sentences (Pereira et al., 2018) and systematically manipulated the stimuli for which ANN representations were extracted. In particular, we i) perturbed sentences’ word order, ii) removed different subsets of words, or iii) replaced sentences with other sentences of varying semantic similarity. We found that the lexical semantic content of the sentence (largely carried by content words) rather than the sentence’s syntactic form (conveyed via word order or function words) is primarily responsible for the ANN-to-brain similarity. In follow-up analyses, we found that perturbation manipulations that adversely affect brain predictivity also lead to more divergent representations in the ANN’s embedding space and decrease the ANN’s ability to predict upcoming tokens in those stimuli. Further, results are robust to whether the mapping model is trained on intact or perturbed stimuli, and whether the ANN sentence representations are conditioned on the same linguistic context that humans saw. The critical result—that lexical- semantic content is the main contributor to the similarity between ANN representations and neural ones—aligns with the idea that the goal of the human language system is to extract meaning from linguistic strings. Finally, this work highlights the strength of systematic experimental manipulations for evaluating how close we are to accurate and generalizable models of the human language network.},
	language = {en},
	urldate = {2024-10-04},
	publisher = {bioRxiv},
	author = {Kauf, Carina and Tuckute, Greta and Levy, Roger and Andreas, Jacob and Fedorenko, Evelina},
	month = may,
	year = {2023},
	note = {Pages: 2023.05.05.539646
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\CMCEM6SS\\Kauf et al. - 2023 - Lexical semantic content, not syntactic structure,.pdf:application/pdf},
}

@misc{noauthor_lexical_nodate-1,
	title = {Lexical semantic content, not syntactic structure, is the main contributor to {ANN}-brain similarity of {fMRI} responses in the language network {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/2023.05.05.539646v1.full},
	urldate = {2024-10-04},
	file = {Full text:C\:\\Users\\user\\Zotero\\storage\\WY9TI6ZQ\\Lexical semantic content, not syntactic structure,.pdf:application/pdf;Lexical semantic content, not syntactic structure, is the main contributor to ANN-brain similarity of fMRI responses in the language network | bioRxiv:C\:\\Users\\user\\Zotero\\storage\\DTNI8GZ6\\2023.05.05.539646v1.html:text/html},
}

@article{chomsky_certain_1959,
	title = {On certain formal properties of grammars},
	volume = {2},
	issn = {0019-9958},
	url = {https://www.sciencedirect.com/science/article/pii/S0019995859903626},
	doi = {10.1016/S0019-9958(59)90362-6},
	abstract = {A grammar can be regarded as a device that enumerates the sentences of a language. We study a sequence of restrictions that limit grammars first to Turing machines, then to two types of system from which a phrase structure description of the generated language can be drawn, and finally to finite state Markov sources (finite automata). These restrictions are shown to be increasingly heavy in the sense that the languages that can be generated by grammars meeting a given restriction constitute a proper subset of those that can be generated by grammars meeting the preceding restriction. Various formulations of phrase structure description are considered, and the source of their excess generative power over finite state sources is investigated in greater detail.},
	number = {2},
	urldate = {2024-10-04},
	journal = {Information and Control},
	author = {Chomsky, Noam},
	month = jun,
	year = {1959},
	pages = {137--167},
	file = {Chomsky - 1959 - On certain formal properties of grammars.pdf:C\:\\Users\\user\\Zotero\\storage\\CI4WS2W2\\Chomsky - 1959 - On certain formal properties of grammars.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\user\\Zotero\\storage\\HJBFKV4H\\S0019995859903626.html:text/html},
}

@inproceedings{linzen_how_2020,
	address = {Online},
	title = {How {Can} {We} {Accelerate} {Progress} {Towards} {Human}-like {Linguistic} {Generalization}?},
	url = {https://aclanthology.org/2020.acl-main.465},
	doi = {10.18653/v1/2020.acl-main.465},
	abstract = {This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.},
	urldate = {2024-10-07},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Linzen, Tal},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {5210--5217},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\TDNHRQBW\\Linzen - 2020 - How Can We Accelerate Progress Towards Human-like .pdf:application/pdf},
}

@misc{collier_reality_2023,
	title = {On {Reality} and the {Limits} of {Language} {Data}: {Aligning} {LLMs} with {Human} {Norms}},
	shorttitle = {On {Reality} and the {Limits} of {Language} {Data}},
	url = {http://arxiv.org/abs/2208.11981},
	abstract = {Recent advancements in Large Language Models (LLMs) harness linguistic associations in vast natural language data for practical applications. However, their ability to understand the physical world using only language data remains a question. After reviewing existing protocols, we explore this question using a novel and tightly controlled reasoning test (ART) and compare human norms against versions of GPT-3. Our ﬁndings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness. GPT-3 offers evidence for verbal reasoning on a par with human subjects for several relations including Synonymy, Antonymy, and Default inheritance, Without reinforcement learning from human judgements, it appears GPT-3 performs at the lower end of the reference interval for Has-part and Contained-in. Weaknesses were observed also in affordance characteristics through Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs with symbolic world grounding is a promising direction to address associative learning.},
	language = {en},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Collier, Nigel H. and Liu, Fangyu and Shareghi, Ehsan},
	month = may,
	year = {2023},
	note = {arXiv:2208.11981 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 9 pages; data available, see https://sites.google.com/site/nhcollier/projects/art},
	file = {Collier et al. - 2023 - On Reality and the Limits of Language Data Aligni.pdf:C\:\\Users\\user\\Zotero\\storage\\2EGFSAS3\\Collier et al. - 2023 - On Reality and the Limits of Language Data Aligni.pdf:application/pdf},
}

@misc{hernandez_linearity_2024,
	title = {Linearity of {Relation} {Decoding} in {Transformer} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.09124},
	abstract = {Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in LMs.},
	language = {en},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Hernandez, Evan and Sharma, Arnab Sen and Haklay, Tal and Meng, Kevin and Wattenberg, Martin and Andreas, Jacob and Belinkov, Yonatan and Bau, David},
	month = feb,
	year = {2024},
	note = {arXiv:2308.09124 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Hernandez et al. - 2024 - Linearity of Relation Decoding in Transformer Lang.pdf:C\:\\Users\\user\\Zotero\\storage\\4G6PMDJQ\\Hernandez et al. - 2024 - Linearity of Relation Decoding in Transformer Lang.pdf:application/pdf},
}

@article{ziems_can_2024,
	title = {Can {Large} {Language} {Models} {Transform} {Computational} {Social} {Science}?},
	volume = {50},
	issn = {0891-2017, 1530-9312},
	url = {https://direct.mit.edu/coli/article/50/1/237/118498/Can-Large-Language-Models-Transform-Computational},
	doi = {10.1162/coli_a_00502},
	abstract = {Abstract
            Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.},
	language = {en},
	number = {1},
	urldate = {2024-10-08},
	journal = {Computational Linguistics},
	author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
	month = mar,
	year = {2024},
	pages = {237--291},
	file = {Ziems et al. - 2024 - Can Large Language Models Transform Computational .pdf:C\:\\Users\\user\\Zotero\\storage\\WJEWJ6Q6\\Ziems et al. - 2024 - Can Large Language Models Transform Computational .pdf:application/pdf},
}

@misc{wang_symbolic_2024,
	title = {Symbolic {Working} {Memory} {Enhances} {Language} {Models} for {Complex} {Rule} {Application}},
	url = {http://arxiv.org/abs/2408.13654},
	abstract = {Large Language Models (LLMs) have shown remarkable reasoning performance but struggle with multi-step deductive reasoning involving a series of rule application steps, especially when rules are presented non-sequentially. Our preliminary analysis shows that while LLMs excel in single-step rule application, their performance drops significantly in multi-step scenarios due to the challenge in rule grounding. It requires anchoring the applicable rule and supporting facts at each step, amidst multiple input rules, facts, and inferred facts. To address this, we propose augmenting LLMs with external working memory and introduce a neurosymbolic framework for rule application. The memory stores facts and rules in both natural language and symbolic forms, enabling precise tracking. Utilizing this memory, our framework iteratively performs symbolic rule grounding and LLM-based rule implementation. The former matches predicates and variables of symbolic rules and facts to ground applicable rules at each step. Experiments indicate our framework's effectiveness in rule application and its robustness across various steps and settings{\textasciitilde}{\textbackslash}footnote\{Code and data are available at {\textbackslash}url\{https://github.com/SiyuanWangw/RuleApplication\}.\}.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Wang, Siyuan and Wei, Zhongyu and Choi, Yejin and Ren, Xiang},
	month = aug,
	year = {2024},
	note = {arXiv:2408.13654 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Wang et al. - 2024 - Symbolic Working Memory Enhances Language Models f.pdf:C\:\\Users\\user\\Zotero\\storage\\KTEEF7P9\\Wang et al. - 2024 - Symbolic Working Memory Enhances Language Models f.pdf:application/pdf},
}

@misc{dziri_faith_2023,
	title = {Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}},
	shorttitle = {Faith and {Fate}},
	url = {http://arxiv.org/abs/2305.18654},
	abstract = {Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks—multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations’ performance can rapidly decay with increased task complexity.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Welleck, Sean and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
	month = oct,
	year = {2023},
	note = {arXiv:2305.18654 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 10 pages + appendix (40 pages)},
	file = {Dziri et al. - 2023 - Faith and Fate Limits of Transformers on Composit.pdf:C\:\\Users\\user\\Zotero\\storage\\MN85N95S\\Dziri et al. - 2023 - Faith and Fate Limits of Transformers on Composit.pdf:application/pdf},
}

@misc{li_counterfactual_2022,
	title = {Counterfactual reasoning: {Do} language models need world knowledge for causal understanding?},
	shorttitle = {Counterfactual reasoning},
	url = {http://arxiv.org/abs/2212.03278},
	abstract = {Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difﬁcult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We ﬁnd that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, we also ﬁnd that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we ﬁnd that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Li, Jiaxuan and Yu, Lang and Ettinger, Allyson},
	month = dec,
	year = {2022},
	note = {arXiv:2212.03278 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 5 pages, accepted to nCSI workshop at NeurIPS 2022},
	file = {Li et al. - 2022 - Counterfactual reasoning Do language models need .pdf:C\:\\Users\\user\\Zotero\\storage\\F3HH9952\\Li et al. - 2022 - Counterfactual reasoning Do language models need .pdf:application/pdf},
}

@misc{feng_were_2024,
	title = {Were {RNNs} {All} {We} {Needed}?},
	url = {http://arxiv.org/abs/2410.01201},
	abstract = {The scalability limitations of Transformers regarding sequence length have renewed interest in recurrent sequence models that are parallelizable during training. As a result, many novel recurrent architectures, such as S4, Mamba, and Aaren, have been proposed that achieve comparable performance. In this work, we revisit traditional recurrent neural networks (RNNs) from over a decade ago: LSTMs (1997) and GRUs (2014). While these models were slow due to requiring to backpropagate through time (BPTT), we show that by removing their hidden state dependencies from their input, forget, and update gates, LSTMs and GRUs no longer need to BPTT and can be efficiently trained in parallel. Building on this, we introduce minimal versions (minLSTMs and minGRUs) that (1) use significantly fewer parameters than their traditional counterparts and (2) are fully parallelizable during training (175× faster for a sequence of length 512). Lastly, we show that these stripped-down versions of decade-old RNNs match the empirical performance of recent sequence models.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Feng, Leo and Tung, Frederick and Ahmed, Mohamed Osama and Bengio, Yoshua and Hajimirsadegh, Hossein},
	month = oct,
	year = {2024},
	note = {arXiv:2410.01201 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Feng et al. - 2024 - Were RNNs All We Needed.pdf:C\:\\Users\\user\\Zotero\\storage\\ITHQZL2Y\\Feng et al. - 2024 - Were RNNs All We Needed.pdf:application/pdf},
}

@inproceedings{asif_legal_2024,
	address = {Cham},
	title = {Legal {Definition} {Annotation} in {EU} {Legislation} {Using} {Symbolic} {AI}},
	isbn = {978-3-031-68211-7},
	doi = {10.1007/978-3-031-68211-7_4},
	abstract = {The definition is an integral component of the legislation. It is essential to the adequacy and legitimacy of legislation. In legislation, legal definitions are used for clarity, consistency, and legitimate certainty, but also for creating new legal concepts (e.g., personal data) or crimes (e.g. stalking, mobbing). With the advancement in society with technological innovation, the significance of accurate and precise definitions in legislation is indeed more articulated. In order to avoid ambiguity and to ensure, as far as possible, a strict interpretation of Law, Legal Texts (LT) usually define the specific lexical terms used within their discourse by means of normative rules. Due to the continuous increase of LT and a large number of domain-specific rules, extracting these definitions from the LT would be costly and time-consuming if it’s done humanely. Definition extraction is widely used in Legal domains to perform legal and compliance analysis. In this paper, we detect and annotate the legal definitions using Symbolic Artificial Intelligence (AI) based on Natural Language Processing (NLP) and fostering LegalXML annotation. The goal is to qualify a very valuable part of legislation for supporting further AI applications also in the judiciary domain. The detection and annotation of definitions are performed on the delimiting type of definitions. The dataset consists of EU Legislation in the span of time from 2010 to 2021 in Akoma Ntoso (AKN) file format. The resultant 15082 AKN files are annotated. (Akoma Ntoso OASIS LegalDocML XML Standard. http://docs.oasis-open.org/legaldocml/akn-core/v1.0/akn-core-v1.0-part1-vocabulary.html)},
	language = {en},
	booktitle = {Electronic {Government} and the {Information} {Systems} {Perspective}},
	publisher = {Springer Nature Switzerland},
	author = {Asif, Muhammad and Palmirani, Monica},
	editor = {Kö, Andrea and Kotsis, Gabriele and Tjoa, A. Min and Khalil, Ismail},
	year = {2024},
	keywords = {Definition Annotation, Definition Extraction, Symbolic AI},
	pages = {34--39},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\V3CMNRLL\\Asif e Palmirani - 2024 - Legal Definition Annotation in EU Legislation Usin.pdf:application/pdf},
}

@book{ashley_artificial_2017,
	address = {Cambridge},
	title = {Artificial {Intelligence} and {Legal} {Analytics}: {New} {Tools} for {Law} {Practice} in the {Digital} {Age}},
	isbn = {978-1-107-17150-3},
	shorttitle = {Artificial {Intelligence} and {Legal} {Analytics}},
	url = {https://www.cambridge.org/core/books/artificial-intelligence-and-legal-analytics/E7D705EEF392501A1DB180645917E7E0},
	abstract = {The field of artificial intelligence (AI) and the law is on the cusp of a revolution that began with text analytic programs like IBM's Watson and Debater and the open-source information management architectures on which they are based. Today, new legal applications are beginning to appear and this book - designed to explain computational processes to non-programmers - describes how they will change the practice of law, specifically by connecting computational models of legal reasoning directly with legal text, generating arguments for and against particular outcomes, predicting outcomes and explaining these predictions with reasons that legal professionals will be able to evaluate for themselves. These legal applications will support conceptual legal information retrieval and allow cognitive computing, enabling a collaboration between humans and computers in which each does what it can do best. Anyone interested in how AI is changing the practice of law should read this illuminating work.},
	urldate = {2024-10-10},
	publisher = {Cambridge University Press},
	author = {Ashley, Kevin D.},
	year = {2017},
	doi = {10.1017/9781316761380},
	file = {Ashley - 2017 - Artificial Intelligence and Legal Analytics New T.pdf:C\:\\Users\\user\\Zotero\\storage\\6UDDGP3T\\Ashley - 2017 - Artificial Intelligence and Legal Analytics New T.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\9JIJK4VN\\E7D705EEF392501A1DB180645917E7E0.html:text/html},
}

@book{ko_electronic_2024,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Electronic {Government} and the {Information} {Systems} {Perspective}: 13th {International} {Conference}, {EGOVIS} 2024, {Naples}, {Italy}, {August} 26–28, 2024, {Proceedings}},
	volume = {14913},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-68210-0 978-3-031-68211-7},
	shorttitle = {Electronic {Government} and the {Information} {Systems} {Perspective}},
	url = {https://link.springer.com/10.1007/978-3-031-68211-7},
	language = {en},
	urldate = {2024-10-10},
	publisher = {Springer Nature Switzerland},
	editor = {Kö, Andrea and Kotsis, Gabriele and Tjoa, A Min and Khalil, Ismail},
	year = {2024},
	doi = {10.1007/978-3-031-68211-7},
	file = {Kö et al. - 2024 - Electronic Government and the Information Systems .pdf:C\:\\Users\\user\\Zotero\\storage\\N4K8L6EV\\Kö et al. - 2024 - Electronic Government and the Information Systems .pdf:application/pdf},
}

@incollection{francesconi_transfer_2022,
	title = {Transfer {Learning} for {Deontic} {Rule} {Classification}: {The} {Case} {Study} of the {GDPR}},
	copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
	isbn = {978-1-64368-364-5 978-1-64368-365-2},
	shorttitle = {Transfer {Learning} for {Deontic} {Rule} {Classification}},
	url = {https://ebooks.iospress.nl/doi/10.3233/FAIA220467},
	abstract = {This work focuses on the automatic classiﬁcation of deontic sentences. It presents a novel Machine Learning approach which combines the power of Transfer Learning with the information provided by two famous LegalXML formats. In particular, different BERT-like neural architectures have been ﬁne-tuned on the downstream task of classifying rules from the European General Data Protection Regulation (GDPR) encoded in Akoma Ntoso and LegalRuleML. This work shows that ﬁne-tuned language models can leverage the information provided in LegalXML documents to achieve automatic classiﬁcation of deontic sentences and rules.},
	language = {en},
	urldate = {2024-10-10},
	booktitle = {Frontiers in {Artificial} {Intelligence} and {Applications}},
	publisher = {IOS Press},
	author = {Liga, Davide and Palmirani, Monica},
	editor = {Francesconi, Enrico and Borges, Georg and Sorge, Christoph},
	month = dec,
	year = {2022},
	doi = {10.3233/FAIA220467},
	file = {Liga e Palmirani - 2022 - Transfer Learning for Deontic Rule Classification.pdf:C\:\\Users\\user\\Zotero\\storage\\883MAWBY\\Liga e Palmirani - 2022 - Transfer Learning for Deontic Rule Classification.pdf:application/pdf},
}

@inproceedings{sovrano_dataset_2021,
	address = {São Paulo Brazil},
	title = {A dataset for evaluating legal question answering on private international law},
	isbn = {978-1-4503-8526-8},
	url = {https://dl.acm.org/doi/10.1145/3462757.3466094},
	doi = {10.1145/3462757.3466094},
	language = {en},
	urldate = {2024-10-10},
	booktitle = {Proceedings of the {Eighteenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {ACM},
	author = {Sovrano, Francesco and Palmirani, Monica and Distefano, Biagio and Sapienza, Salvatore and Vitali, Fabio},
	month = jun,
	year = {2021},
	pages = {230--234},
}

@inproceedings{gauthier_syntaxgym_2020,
	address = {Online},
	title = {{SyntaxGym}: {An} {Online} {Platform} for {Targeted} {Evaluation} of {Language} {Models}},
	shorttitle = {{SyntaxGym}},
	url = {https://aclanthology.org/2020.acl-demos.10},
	doi = {10.18653/v1/2020.acl-demos.10},
	abstract = {Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models. However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models. We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design. This paper releases two tools of independent value for the computational linguistics community: 1. A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2. Two command-line tools, `syntaxgym` and `lm-zoo`, which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine.},
	urldate = {2024-10-10},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
	editor = {Celikyilmaz, Asli and Wen, Tsung-Hsien},
	month = jul,
	year = {2020},
	pages = {70--76},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\UJWQXR6G\\Gauthier et al. - 2020 - SyntaxGym An Online Platform for Targeted Evaluat.pdf:application/pdf},
}

@misc{warstadt_what_2022,
	title = {What {Artificial} {Neural} {Networks} {Can} {Tell} {Us} {About} {Human} {Language} {Acquisition}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv220807998W},
	doi = {10.48550/arXiv.2208.07998},
	abstract = {Rapid progress in machine learning for natural language processing has the potential to transform debates about how humans learn language. However, the learning environments and biases of current artificial learners and humans diverge in ways that weaken the impact of the evidence obtained from learning simulations. For example, today's most effective neural language models are trained on roughly one thousand times the amount of linguistic data available to a typical child. To increase the relevance of learnability results from computational models, we need to train model learners without significant advantages over humans. If an appropriate model successfully acquires some target linguistic knowledge, it can provide a proof of concept that the target is learnable in a hypothesized human learning scenario. Plausible model learners will enable us to carry out experimental manipulations to make causal inferences about variables in the learning environment, and to rigorously test poverty-of-the-stimulus-style claims arguing for innate linguistic knowledge in humans on the basis of speculations about learnability. Comparable experiments will never be possible with human subjects due to practical and ethical considerations, making model learners an indispensable resource. So far, attempts to deprive current models of unfair advantages obtain sub-human results for key grammatical behaviors such as acceptability judgments. But before we can justifiably conclude that language learning requires more prior domain-specific knowledge than current models possess, we must first explore non-linguistic inputs in the form of multimodal stimuli and multi-agent interaction as ways to make our learners more efficient at learning from limited linguistic input.},
	urldate = {2024-10-10},
	author = {Warstadt, Alex and Bowman, Samuel R.},
	month = aug,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv220807998W},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\GWPZUCIL\\Warstadt e Bowman - 2022 - What Artificial Neural Networks Can Tell Us About .pdf:application/pdf},
}

@inproceedings{michael_what_2023,
	address = {Toronto, Canada},
	title = {What {Do} {NLP} {Researchers} {Believe}? {Results} of the {NLP} {Community} {Metasurvey}},
	shorttitle = {What {Do} {NLP} {Researchers} {Believe}?},
	url = {https://aclanthology.org/2023.acl-long.903},
	doi = {10.18653/v1/2023.acl-long.903},
	abstract = {We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed meta-questions, asking respondents to predict the distribution of survey responses. This allows us to uncover false sociological beliefs where the community's predictions don't match reality. Among other results, we find that the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.},
	urldate = {2024-10-10},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Michael, Julian and Holtzman, Ari and Parrish, Alicia and Mueller, Aaron and Wang, Alex and Chen, Angelica and Madaan, Divyam and Nangia, Nikita and Pang, Richard Yuanzhe and Phang, Jason and Bowman, Samuel R.},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {16334--16368},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\2DC3E2WB\\Michael et al. - 2023 - What Do NLP Researchers Believe Results of the NL.pdf:application/pdf},
}

@inproceedings{bowman_dangers_2022,
	address = {Dublin, Ireland},
	title = {The {Dangers} of {Underclaiming}: {Reasons} for {Caution} {When} {Reporting} {How} {NLP} {Systems} {Fail}},
	shorttitle = {The {Dangers} of {Underclaiming}},
	url = {https://aclanthology.org/2022.acl-long.516},
	doi = {10.18653/v1/2022.acl-long.516},
	abstract = {Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field's successes, often in response to the field's widespread hype. Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening. It also limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.},
	urldate = {2024-10-10},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bowman, Samuel},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {7484--7499},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\HT6XVNVW\\Bowman - 2022 - The Dangers of Underclaiming Reasons for Caution .pdf:application/pdf},
}

@inproceedings{sugawara_what_2022,
	address = {Dublin, Ireland},
	title = {What {Makes} {Reading} {Comprehension} {Questions} {Difficult}?},
	url = {https://aclanthology.org/2022.acl-long.479},
	doi = {10.18653/v1/2022.acl-long.479},
	abstract = {For a natural language understanding benchmark to be useful in research, it has to consist of examples that are diverse and difﬁcult enough to discriminate among current and near-future state-of-the-art systems. However, we do not yet know how best to select text sources to collect a variety of challenging examples. In this study, we crowdsource multiple-choice reading comprehension questions for passages taken from seven qualitatively distinct sources, analyzing what attributes of passages contribute to the difﬁculty and question types of the collected examples. To our surprise, we ﬁnd that passage source, length, and readability measures do not signiﬁcantly affect question difﬁculty. Through our manual annotation of seven reasoning types, we observe several trends between passage sources and reasoning types, e.g., logical reasoning is more often required in questions written for technical passages. These results suggest that when creating a new benchmark dataset, selecting a diverse set of passages can help ensure a diverse range of question types, but that passage difﬁculty need not be a priority.},
	language = {en},
	urldate = {2024-10-10},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sugawara, Saku and Nangia, Nikita and Warstadt, Alex and Bowman, Samuel},
	year = {2022},
	pages = {6951--6971},
	file = {Sugawara et al. - 2022 - What Makes Reading Comprehension Questions Difficu.pdf:C\:\\Users\\user\\Zotero\\storage\\URD5AZLA\\Sugawara et al. - 2022 - What Makes Reading Comprehension Questions Difficu.pdf:application/pdf},
}

@inproceedings{gauthier_syntaxgym_2020-1,
	address = {Online},
	title = {{SyntaxGym}: {An} {Online} {Platform} for {Targeted} {Evaluation} of {Language} {Models}},
	shorttitle = {{SyntaxGym}},
	url = {https://aclanthology.org/2020.acl-demos.10},
	doi = {10.18653/v1/2020.acl-demos.10},
	abstract = {Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models. However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models. We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design. This paper releases two tools of independent value for the computational linguistics community: 1. A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2. Two command-line tools, `syntaxgym` and `lm-zoo`, which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine.},
	urldate = {2024-10-10},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
	editor = {Celikyilmaz, Asli and Wen, Tsung-Hsien},
	month = jul,
	year = {2020},
	pages = {70--76},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\XPXJMK6F\\Gauthier et al. - 2020 - SyntaxGym An Online Platform for Targeted Evaluat.pdf:application/pdf},
}

@inproceedings{marvin_targeted_2018,
	address = {Brussels, Belgium},
	title = {Targeted {Syntactic} {Evaluation} of {Language} {Models}},
	url = {http://aclweb.org/anthology/D18-1151},
	doi = {10.18653/v1/D18-1151},
	abstract = {We present a dataset for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reﬂexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM’s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.},
	language = {en},
	urldate = {2024-10-10},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Marvin, Rebecca and Linzen, Tal},
	year = {2018},
	pages = {1192--1202},
	file = {Marvin e Linzen - 2018 - Targeted Syntactic Evaluation of Language Models.pdf:C\:\\Users\\user\\Zotero\\storage\\26LC3B8I\\Marvin e Linzen - 2018 - Targeted Syntactic Evaluation of Language Models.pdf:application/pdf},
}

@misc{jiang_origins_2024-1,
	title = {On the {Origins} of {Linear} {Representations} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2403.03867},
	abstract = {Recent works have argued that high-level semantic concepts are encoded “linearly” in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.},
	language = {en},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon and Veitch, Victor},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03867 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2403.pdf:C\:\\Users\\user\\Zotero\\storage\\TPVHM6ST\\2403.pdf:application/pdf},
}

@article{charniak_bayesian_nodate,
	title = {Bayesian {Networks} without {Tears}},
	language = {en},
	author = {Charniak, Eugene},
	file = {Charniak - Bayesian Networks without Tears.pdf:C\:\\Users\\user\\Zotero\\storage\\PA98CP4Z\\Charniak - Bayesian Networks without Tears.pdf:application/pdf},
}

@misc{feng_were_2024-1,
	title = {Were {RNNs} {All} {We} {Needed}?},
	url = {https://arxiv.org/abs/2410.01201v2},
	abstract = {The scalability limitations of Transformers regarding sequence length have renewed interest in recurrent sequence models that are parallelizable during training. As a result, many novel recurrent architectures, such as S4, Mamba, and Aaren, have been proposed that achieve comparable performance. In this work, we revisit traditional recurrent neural networks (RNNs) from over a decade ago: LSTMs (1997) and GRUs (2014). While these models were slow due to requiring to backpropagate through time (BPTT), we show that by removing their hidden state dependencies from their input, forget, and update gates, LSTMs and GRUs no longer need to BPTT and can be efficiently trained in parallel. Building on this, we introduce minimal versions (minLSTMs and minGRUs) that (1) use significantly fewer parameters than their traditional counterparts and (2) are fully parallelizable during training (175x faster for a sequence of length 512). Lastly, we show that these stripped-down versions of decade-old RNNs match the empirical performance of recent sequence models.},
	language = {en},
	urldate = {2024-10-10},
	journal = {arXiv.org},
	author = {Feng, Leo and Tung, Frederick and Ahmed, Mohamed Osama and Bengio, Yoshua and Hajimirsadegh, Hossein},
	month = oct,
	year = {2024},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\MCAQQLKP\\Feng et al. - 2024 - Were RNNs All We Needed.pdf:application/pdf},
}

@inproceedings{simhi_interpreting_2023,
	address = {Singapore},
	title = {Interpreting {Embedding} {Spaces} by {Conceptualization}},
	url = {https://aclanthology.org/2023.emnlp-main.106},
	doi = {10.18653/v1/2023.emnlp-main.106},
	abstract = {One of the main methods for computational interpretation of a text is mapping it into a vector in some embedding space. Such vectors can then be used for a variety of textual processing tasks. Recently, most embedding spaces are a product of training large language models (LLMs). One major drawback of this type of representation is their incomprehensibility to humans. Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model. In this paper, we present a novel method of understanding embeddings by transforming a latent embedding space into a comprehensible conceptual space. We present an algorithm for deriving a conceptual space with dynamic on-demand granularity. We devise a new evaluation method, using either human rater or LLM-based raters, to show that the conceptualized vectors indeed represent the semantics of the original latent ones. We show the use of our method for various tasks, including comparing the semantics of alternative models and tracing the layers of the LLM. The code is available online https://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.},
	urldate = {2024-10-10},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Simhi, Adi and Markovitch, Shaul},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {1704--1719},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\ZYEFX7GQ\\Simhi e Markovitch - 2023 - Interpreting Embedding Spaces by Conceptualization.pdf:application/pdf},
}

@misc{lai_large_2023,
	title = {Large {Language} {Models} in {Law}: {A} {Survey}},
	shorttitle = {Large {Language} {Models} in {Law}},
	url = {http://arxiv.org/abs/2312.03718},
	abstract = {The advent of artificial intelligence (AI) has significantly impacted the traditional judicial industry. Moreover, recently, with the development of AI-generated content (AIGC), AI and law have found applications in various domains, including image recognition, automatic text generation, and interactive chat. With the rapid emergence and growing popularity of large models, it is evident that AI will drive transformation in the traditional judicial industry. However, the application of legal large language models (LLMs) is still in its nascent stage. Several challenges need to be addressed. In this paper, we aim to provide a comprehensive survey of legal LLMs. We not only conduct an extensive survey of LLMs, but also expose their applications in the judicial system. We first provide an overview of AI technologies in the legal field and showcase the recent research in LLMs. Then, we discuss the practical implementation presented by legal LLMs, such as providing legal advice to users and assisting judges during trials. In addition, we explore the limitations of legal LLMs, including data, algorithms, and judicial practice. Finally, we summarize practical recommendations and propose future development directions to address these challenges.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Lai, Jinqi and Gan, Wensheng and Wu, Jiayang and Qi, Zhenlian and Yu, Philip S.},
	month = nov,
	year = {2023},
	note = {arXiv:2312.03718},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\user\\Zotero\\storage\\WRTFHYPV\\Lai et al. - 2023 - Large Language Models in Law A Survey.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\FDQANJ2U\\2312.html:text/html},
}

@incollection{prakken_legal_2015,
	title = {Legal {Reasoning}: {Computational} {Models}},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	isbn = {978-0-08-097087-5},
	shorttitle = {Legal {Reasoning}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080970868861619},
	abstract = {This article reviews computational models of legal reasoning as they are being developed in the ﬁeld of artiﬁcial intelligence (AI) and law. A theoretical aim of these models is to understand legal reasoning by simulating it in a computer program, while a practical aim is to study how advanced information technology can aid legal practice. First logical deduction is discussed as a necessary but insuﬃcient component of realistic models of legal reasoning. Then models of defeasible legal reasoning and legal argumentation are discussed, which focus on the generation and comparison of reasons or arguments for and against legal claims. Computational models of legal interpretation emphasize the interplay between rules and cases and the role of principles, purposes and values. Computational models of legal proof account for the uncertainty in legal proof in three alternative ways: with Bayesian probability theory, with argumentation and with narrative. Finally, procedural models of legal reasoning are based on the idea that the quality of a legal decision partly depends on how it was reached.},
	language = {en},
	urldate = {2024-10-14},
	booktitle = {International {Encyclopedia} of the {Social} \& {Behavioral} {Sciences}},
	publisher = {Elsevier},
	author = {Prakken, Henry},
	year = {2015},
	doi = {10.1016/B978-0-08-097086-8.86161-9},
	pages = {784--791},
	file = {Prakken - 2015 - Legal Reasoning Computational Models.pdf:C\:\\Users\\user\\Zotero\\storage\\T7ZFHP2T\\Prakken - 2015 - Legal Reasoning Computational Models.pdf:application/pdf},
}

@article{bresciani_constitutional_nodate,
	title = {Constitutional {Opportunities} and {Risks} of {AI} in the law-making process},
	abstract = {En]: This paper explores the constitutional opportunities and risks linked to the use of AI in the legislative process, adopting an interdisciplinary approach. The analysis distinguishes between AI assisting, enhancing, and decision-making for legislators, highlighting their different constitutional implications. Opportunities and risks for each use are discussed with examples, emphasizing the need for constitutional safeguards. The Authors argue that assistive AI, with proper safeguards, poses fewer issues than commonly believed, while AI augmenting MPs abilities or making decisions may compromise fundamental principles of constitutional law and due legislative process.},
	language = {en},
	number = {2},
	author = {Bresciani, Pier Francesco and Palmirani, Monica},
	file = {Bresciani e Palmirani - Constitutional Opportunities and Risks of AI in th.pdf:C\:\\Users\\user\\Zotero\\storage\\H2R82V5X\\Bresciani e Palmirani - Constitutional Opportunities and Risks of AI in th.pdf:application/pdf},
}

@inproceedings{linzen_issues_2016,
	address = {Berlin, Germany},
	title = {Issues in evaluating semantic spaces using word analogies},
	url = {https://aclanthology.org/W16-2503},
	doi = {10.18653/v1/W16-2503},
	urldate = {2024-10-15},
	booktitle = {Proceedings of the 1st {Workshop} on {Evaluating} {Vector}-{Space} {Representations} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Linzen, Tal},
	month = aug,
	year = {2016},
	pages = {13--18},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\WHVG2Q7M\\Linzen - 2016 - Issues in evaluating semantic spaces using word an.pdf:application/pdf},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	urldate = {2024-10-15},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\user\\Zotero\\storage\\HPLMISEZ\\Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\VXFQFT3R\\2206.html:text/html},
}

@inproceedings{teif_partonomy_2006,
	address = {Bologna, Italy},
	title = {Partonomy and taxonomy in object-oriented thinking: junior high school students' perceptions of object-oriented basic concepts},
	isbn = {978-1-59593-603-5},
	shorttitle = {Partonomy and taxonomy in object-oriented thinking},
	url = {http://portal.acm.org/citation.cfm?doid=1189215.1189170},
	doi = {10.1145/1189215.1189170},
	language = {en},
	urldate = {2024-10-15},
	booktitle = {Working group reports on {ITiCSE} on {Innovation} and technology in computer science education  - {ITiCSE}-{WGR} '06},
	publisher = {ACM Press},
	author = {Teif, Mariana and Hazzan, Orit},
	year = {2006},
	pages = {55},
	file = {Teif e Hazzan - 2006 - Partonomy and taxonomy in object-oriented thinking.pdf:C\:\\Users\\user\\Zotero\\storage\\PAMSH8AC\\Teif e Hazzan - 2006 - Partonomy and taxonomy in object-oriented thinking.pdf:application/pdf},
}

@article{tversky_cognitive_2015,
	title = {The {Cognitive} {Design} of {Tools} of {Thought}},
	volume = {6},
	issn = {1878-5158, 1878-5166},
	url = {http://link.springer.com/10.1007/s13164-014-0214-3},
	doi = {10.1007/s13164-014-0214-3},
	abstract = {When thought overwhelms the mind, the mind puts it into the world, notably in diagrams and gestures.Both use space and arrays of elements, depictive and nondepictive, to convey ideas, concrete and abstract,clear and sketchy. The arrays and the non-depictive elements like boxes and arrows serve to showrelationships and organizations, thematic, categorical, and more. on paper, in the air, in the diagrammedworld. Human actions organize space to convey abstractions: spraction.},
	language = {en},
	number = {1},
	urldate = {2024-10-15},
	journal = {Review of Philosophy and Psychology},
	author = {Tversky, Barbara},
	month = mar,
	year = {2015},
	pages = {99--116},
	file = {Tversky - 2015 - The Cognitive Design of Tools of Thought.pdf:C\:\\Users\\user\\Zotero\\storage\\HNIHH9YH\\Tversky - 2015 - The Cognitive Design of Tools of Thought.pdf:application/pdf},
}

@article{tversky_parts_nodate,
	title = {Parts, {Partonomies}, and {Taxonomies}},
	language = {en},
	author = {Tversky, Barbara},
	file = {Tversky - Parts, Partonomies, and Taxonomies.pdf:C\:\\Users\\user\\Zotero\\storage\\WBIXM2WX\\Tversky - Parts, Partonomies, and Taxonomies.pdf:application/pdf},
}

@article{rosch_basic_1976,
	title = {Basic objects in natural categories},
	volume = {8},
	issn = {0010-0285},
	url = {https://www.sciencedirect.com/science/article/pii/001002857690013X},
	doi = {10.1016/0010-0285(76)90013-X},
	abstract = {Categorizations which humans make of the concrete world are not arbitrary but highly determined. In taxonomies of concrete objects, there is one level of abstraction at which the most basic category cuts are made. Basic categories are those which carry the most information, possess the highest category cue validity, and are, thus, the most differentiated from one another. The four experiments of Part I define basic objects by demonstrating that in taxonomies of common concrete nouns in English based on class inclusion, basic objects are the most inclusive categories whose members: (a) possess significant numbers of attributes in common, (b) have motor programs which are similar to one another, (c) have similar shapes, and (d) can be identified from averaged shapes of members of the class. The eight experiments of Part II explore implications of the structure of categories. Basic objects are shown to be the most inclusive categories for which a concrete image of the category as a whole can be formed, to be the first categorizations made during perception of the environment, to be the earliest categories sorted and earliest named by children, and to be the categories most codable, most coded, and most necessary in language.},
	number = {3},
	urldate = {2024-10-15},
	journal = {Cognitive Psychology},
	author = {Rosch, Eleanor and Mervis, Carolyn B and Gray, Wayne D and Johnson, David M and Boyes-Braem, Penny},
	month = jul,
	year = {1976},
	pages = {382--439},
	file = {ScienceDirect Snapshot:C\:\\Users\\user\\Zotero\\storage\\BCSBYQMZ\\001002857690013X.html:text/html;Versione inviata:C\:\\Users\\user\\Zotero\\storage\\XSW55RKJ\\Rosch et al. - 1976 - Basic objects in natural categories.pdf:application/pdf},
}

@article{rayside_aristotelian_nodate,
	title = {An {Aristotelian} {Understanding} of {Object}-{Oriented} {Programming}},
	abstract = {The folklore of the object-oriented programming community at times maintains that object-oriented programming has drawn inspiration from philosophy, speciﬁcally that of Aristotle. We investigate this relation, ﬁrst of all, in the hope of attaining a better understanding of object-oriented programming and, secondly, to explain aspects of Aristotelian logic to the computer science research community (since it diﬀers from ﬁrst order predicate calculus in a number of important ways). In both respects we endeavour to contribute to the theory of objects, albeit in a more philosophical than mathematical fashion.},
	language = {en},
	author = {Rayside, Derek and Campbell, Gerard T},
	file = {Rayside e Campbell - An Aristotelian Understanding of Object-Oriented P.pdf:C\:\\Users\\user\\Zotero\\storage\\DG83X95M\\Rayside e Campbell - An Aristotelian Understanding of Object-Oriented P.pdf:application/pdf},
}

@misc{noauthor_230406623_nodate,
	title = {[2304.06623] {Exploring} the {State} of the {Art} in {Legal} {QA} {Systems}},
	url = {https://arxiv.org/abs/2304.06623},
	urldate = {2024-10-15},
}

@incollection{kamps_legal_2023,
	address = {Cham},
	title = {Legal {IR} and {NLP}: {The} {History}, {Challenges}, and {State}-of-the-{Art}},
	volume = {13982},
	isbn = {978-3-031-28240-9 978-3-031-28241-6},
	shorttitle = {Legal {IR} and {NLP}},
	url = {https://link.springer.com/10.1007/978-3-031-28241-6_34},
	abstract = {Artificial Intelligence (AI), Machine Learning (ML), Information Retrieval (IR) and Natural Language Processing (NLP) are transforming the way legal professionals and law firms approach their work. The significant potential for the application of AI to Law, for instance, by creating computational solutions for legal tasks, has intrigued researchers for decades. This appeal has only been amplified with the advent of Deep Learning (DL). It is worth noting that working with legal text is far more challenging than in many other subdomains of IR/NLP, mainly due to factors like lengthy documents, complex language and lack of large-scale datasets.},
	language = {en},
	urldate = {2024-10-15},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Nature Switzerland},
	author = {Ganguly, Debasis and Conrad, Jack G. and Ghosh, Kripabandhu and Ghosh, Saptarshi and Goyal, Pawan and Bhattacharya, Paheli and Nigam, Shubham Kumar and Paul, Shounak},
	editor = {Kamps, Jaap and Goeuriot, Lorraine and Crestani, Fabio and Maistro, Maria and Joho, Hideo and Davis, Brian and Gurrin, Cathal and Kruschwitz, Udo and Caputo, Annalina},
	year = {2023},
	doi = {10.1007/978-3-031-28241-6_34},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {331--340},
	file = {Ganguly et al. - 2023 - Legal IR and NLP The History, Challenges, and Sta.pdf:C\:\\Users\\user\\Zotero\\storage\\ANAT84RV\\Ganguly et al. - 2023 - Legal IR and NLP The History, Challenges, and Sta.pdf:application/pdf},
}

@misc{dias_state_2022,
	title = {State of the {Art} in {Artificial} {Intelligence} applied to the {Legal} {Domain}},
	url = {http://arxiv.org/abs/2204.07047},
	abstract = {While Artificial Intelligence applied to the legal domain is a topic with origins in the last century, recent advances in Artificial Intelligence are posed to revolutionize it. This work presents an overview and contextualizes the main advances on the field of Natural Language Processing and how these advances have been used to further the state of the art in legal text analysis.},
	urldate = {2024-10-15},
	publisher = {arXiv},
	author = {Dias, João and Santos, Pedro A. and Cordeiro, Nuno and Antunes, Ana and Martins, Bruno and Baptista, Jorge and Gonçalves, Carlos},
	month = mar,
	year = {2022},
	note = {arXiv:2204.07047},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Snapshot:C\:\\Users\\user\\Zotero\\storage\\2HLEQU3X\\2204.html:text/html},
}

@article{bex_ai_2024,
	title = {{AI}, {Law} and beyond. {A} transdisciplinary ecosystem for the future of {AI} \& {Law}},
	issn = {1572-8382},
	url = {https://doi.org/10.1007/s10506-024-09404-y},
	doi = {10.1007/s10506-024-09404-y},
	abstract = {We live in exciting times for AI and Law: technical developments are moving at a breakneck pace, and at the same time, the call for more robust AI governance and regulation grows stronger. How should we as an AI \& Law community navigate these dramatic developments and claims? In this Presidential Address, I present my ideas for a way forward: researching, developing and evaluating real AI systems for the legal field with researchers from AI, Law and beyond. I will demonstrate how we at the Netherlands National Police Lab AI are developing responsible AI by combining insights from different disciplines, and how this connects to the future of our field.},
	language = {en},
	urldate = {2024-10-15},
	journal = {Artificial Intelligence and Law},
	author = {Bex, Floris J.},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\2LIB6LDX\\Bex - 2024 - AI, Law and beyond. A transdisciplinary ecosystem .pdf:application/pdf},
}

@article{deroy_applicability_2024,
	title = {Applicability of large language models and generative models for legal case judgement summarization},
	issn = {1572-8382},
	url = {https://doi.org/10.1007/s10506-024-09411-z},
	doi = {10.1007/s10506-024-09411-z},
	abstract = {Automatic summarization of legal case judgements, which are known to be long and complex, has traditionally been tried via extractive summarization models. In recent years, generative models including abstractive summarization models and Large language models (LLMs) have gained huge popularity. In this paper, we explore the applicability of such models for legal case judgement summarization. We applied various domain-specific abstractive summarization models and general-domain LLMs as well as extractive summarization models over two sets of legal case judgements – from the United Kingdom (UK) Supreme Court and the Indian Supreme Court – and evaluated the quality of the generated summaries. We also perform experiments on a third dataset of legal documents of a different type – Government reports from the United States. Results show that abstractive summarization models and LLMs generally perform better than the extractive methods as per traditional metrics for evaluating summary quality. However, detailed investigation shows the presence of inconsistencies and hallucinations in the outputs of the generative models, and we explore ways to reduce the hallucinations and inconsistencies in the summaries. Overall, the investigation suggests that further improvements are needed to enhance the reliability of abstractive models and LLMs for legal case judgement summarization. At present, a human-in-the-loop technique is more suitable for performing manual checks to identify inconsistencies in the generated summaries.},
	language = {en},
	urldate = {2024-10-15},
	journal = {Artificial Intelligence and Law},
	author = {Deroy, Aniket and Ghosh, Kripabandhu and Ghosh, Saptarshi},
	month = jul,
	year = {2024},
	keywords = {Artificial Intelligence, Abstractive summarization, Hallucinations, Large language models, Legal judgement summarization, Prompting},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\6M3G4W5Q\\Deroy et al. - 2024 - Applicability of large language models and generat.pdf:application/pdf},
}

@inproceedings{niklaus_multilegalpile_2024,
	address = {Bangkok, Thailand},
	title = {{MultiLegalPile}: {A} {689GB} {Multilingual} {Legal} {Corpus}},
	shorttitle = {{MultiLegalPile}},
	url = {https://aclanthology.org/2024.acl-long.805},
	doi = {10.18653/v1/2024.acl-long.805},
	abstract = {Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, few datasets are available for specialized critical domains such as law and the available ones are often small and only in English. To fill this gap, we curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. MultiLegalPile includes diverse legal data sources and allows for pretraining NLP models under fair use, with most of the dataset licensed very permissively. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, trained models, and all code under the most open licenses possible.},
	urldate = {2024-10-15},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Niklaus, Joel and Matoshi, Veton and Stürmer, Matthias and Chalkidis, Ilias and Ho, Daniel},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {15077--15094},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\ZZKFAFDX\\Niklaus et al. - 2024 - MultiLegalPile A 689GB Multilingual Legal Corpus.pdf:application/pdf},
}

@inproceedings{chalkidis_lexglue_2022,
	address = {Dublin, Ireland},
	title = {{LexGLUE}: {A} {Benchmark} {Dataset} for {Legal} {Language} {Understanding} in {English}},
	shorttitle = {{LexGLUE}},
	url = {https://aclanthology.org/2022.acl-long.297},
	doi = {10.18653/v1/2022.acl-long.297},
	abstract = {Laws and their interpretations, legal arguments and agreements are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks.},
	urldate = {2024-10-15},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel and Aletras, Nikolaos},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {4310--4330},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\2BID38KU\\Chalkidis et al. - 2022 - LexGLUE A Benchmark Dataset for Legal Language Un.pdf:application/pdf},
}

@misc{scardapane_alices_2024,
	title = {Alice's {Adventures} in a {Differentiable} {Wonderland} -- {Volume} {I}, {A} {Tour} of the {Land}},
	url = {http://arxiv.org/abs/2404.17625},
	doi = {10.48550/arXiv.2404.17625},
	abstract = {Neural networks surround us, in the form of large language models, speech transcription systems, molecular discovery algorithms, robotics, and much more. Stripped of anything else, neural networks are compositions of differentiable primitives, and studying them means learning how to program and how to interact with these models, a particular example of what is called differentiable programming. This primer is an introduction to this fascinating field imagined for someone, like Alice, who has just ventured into this strange differentiable wonderland. I overview the basics of optimizing a function via automatic differentiation, and a selection of the most common designs for handling sequences, graphs, texts, and audios. The focus is on a intuitive, self-contained introduction to the most important design techniques, including convolutional, attentional, and recurrent blocks, hoping to bridge the gap between theory and code (PyTorch and JAX) and leaving the reader capable of understanding some of the most advanced models out there, such as large language models (LLMs) and multimodal architectures.},
	urldate = {2024-10-16},
	publisher = {arXiv},
	author = {Scardapane, Simone},
	month = jul,
	year = {2024},
	note = {arXiv:2404.17625},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\user\\Zotero\\storage\\3DCLLTTV\\Scardapane - 2024 - Alice's Adventures in a Differentiable Wonderland .pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\8N7QNPV9\\2404.html:text/html},
}

@misc{pfeiffer_modular_2024,
	title = {Modular {Deep} {Learning}},
	url = {http://arxiv.org/abs/2302.11529},
	abstract = {Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.},
	urldate = {2024-10-16},
	publisher = {arXiv},
	author = {Pfeiffer, Jonas and Ruder, Sebastian and Vulić, Ivan and Ponti, Edoardo Maria},
	month = jan,
	year = {2024},
	note = {arXiv:2302.11529},
	keywords = {Computer Science - Machine Learning},
	file = {Full text:C\:\\Users\\user\\Zotero\\storage\\MV7DFKIR\\Pfeiffer et al. - 2024 - Modular Deep Learning.pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\7NTA7KDH\\2302.html:text/html},
}

@misc{verdini_how_2024,
	title = {How to {Connect} {Speech} {Foundation} {Models} and {Large} {Language} {Models}? {What} {Matters} and {What} {Does} {Not}},
	shorttitle = {How to {Connect} {Speech} {Foundation} {Models} and {Large} {Language} {Models}?},
	url = {http://arxiv.org/abs/2409.17044},
	abstract = {The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In speech-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a Speech Foundational Model (SFM) into the LLM embedding space through an adapter module. However, no work has yet investigated how much the downstream-task performance depends on each component (SFM, adapter, LLM) nor whether the best design of the adapter depends on the chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on two widespread S2T tasks, namely Automatic Speech Recognition and Speech Translation. Our results demonstrate that the SFM plays a pivotal role in downstream performance, while the adapter choice has moderate impact and depends on the SFM and LLM.},
	urldate = {2024-10-16},
	publisher = {arXiv},
	author = {Verdini, Francesco and Melucci, Pierfrancesco and Perna, Stefano and Cariaggi, Francesco and Gaido, Marco and Papi, Sara and Mazurek, Szymon and Kasztelnik, Marek and Bentivogli, Luisa and Bratières, Sébastien and Merialdo, Paolo and Scardapane, Simone},
	month = sep,
	year = {2024},
	note = {arXiv:2409.17044},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\user\\Zotero\\storage\\EZQXAIAN\\Verdini et al. - 2024 - How to Connect Speech Foundation Models and Large .pdf:application/pdf;Snapshot:C\:\\Users\\user\\Zotero\\storage\\28FMW8TL\\2409.html:text/html},
}

@article{halford_relational_2010-1,
	title = {Relational knowledge: the foundation of higher cognition},
	volume = {14},
	issn = {1364-6613},
	shorttitle = {Relational knowledge},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661310002020},
	doi = {10.1016/j.tics.2010.08.005},
	abstract = {Accumulating evidence on the nature, function and acquisition of relational knowledge indicates a crucial role of such knowledge in higher cognitive processes. In this review, we specify the essential properties of relational knowledge, together with the role it plays in reasoning, categorisation, planning, quantification and language. Furthermore, we discuss the processes involved in its acquisition and how these processes have been implemented in contemporary neural network models. We present evidence demonstrating that relational knowledge integrates heuristic and analytic cognition, is important for symbolic processes and the creation of novelty, activates specific regions of the prefrontal cortex, and is the most recently evolved and slowest-developing cognitive process. Arguably, relational knowledge represents the core of higher cognition.},
	number = {11},
	urldate = {2024-10-16},
	journal = {Trends in Cognitive Sciences},
	author = {Halford, Graeme S. and Wilson, William H. and Phillips, Steven},
	month = nov,
	year = {2010},
	pages = {497--505},
	file = {ScienceDirect Snapshot:C\:\\Users\\user\\Zotero\\storage\\9Q2YL84Z\\S1364661310002020.html:text/html},
}

@article{rambelli_neural_2024,
	title = {Neural {Generative} {Models} and the {Parallel} {Architecture} of {Language}: {A} {Critical} {Review} and {Outlook}},
	issn = {1756-8757, 1756-8765},
	shorttitle = {Neural {Generative} {Models} and the {Parallel} {Architecture} of {Language}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/tops.12733},
	doi = {10.1111/tops.12733},
	abstract = {Abstract
            According to the parallel architecture, syntactic and semantic information processing are two separate streams that interact selectively during language comprehension. While considerable effort is put into psycho‐ and neurolinguistics to understand the interchange of processing mechanisms in human comprehension, the nature of this interaction in recent neural Large Language Models remains elusive. In this article, we revisit influential linguistic and behavioral experiments and evaluate the ability of a large language model, GPT‐3, to perform these tasks. The model can solve semantic tasks autonomously from syntactic realization in a manner that resembles human behavior. However, the outcomes present a complex and variegated picture, leaving open the question of how Language Models could learn structured conceptual representations.
          , 
            LLMs like GPT‐3 show human‐like behavior in solving some semantic tasks autonomously from syntax, exhibiting alignment with the parallel architecture. Yet LLMs struggle with tasks like interpreting elliptical gaps or scrambled sentences, indicating a need to further explore their ability to understand structured conceptual representations.},
	language = {en},
	urldate = {2024-10-16},
	journal = {Topics in Cognitive Science},
	author = {Rambelli, Giulia and Chersoni, Emmanuele and Testa, Davide and Blache, Philippe and Lenci, Alessandro},
	month = apr,
	year = {2024},
	pages = {tops.12733},
	file = {Rambelli et al. - 2024 - Neural Generative Models and the Parallel Architec.pdf:C\:\\Users\\user\\Zotero\\storage\\59V7WEMH\\Rambelli et al. - 2024 - Neural Generative Models and the Parallel Architec.pdf:application/pdf},
}
